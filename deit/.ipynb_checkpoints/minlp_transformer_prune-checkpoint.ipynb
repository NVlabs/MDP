{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_allow_trim(dict):\n",
    "    res = False\n",
    "    if \"allow_trim\" in dict.keys():\n",
    "        if dict[\"allow_trim\"]:\n",
    "            res = True\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_criteria(list_criteria_per_layer, layers_group, group_size=1):\n",
    "    '''\n",
    "    Function combine criteria per neuron into groups of size group_size.\n",
    "    Output is a list of groups organized by layers. Length of output is a number of layers.\n",
    "    The criterion for the group is computed as an average of member's criteria.\n",
    "    Input:\n",
    "    list_criteria_per_layer - list of criteria per neuron organized per layer\n",
    "    group_size - number of neurons per group\n",
    "    layers_group - layers can form a group, e.g. residual connection, they will be pruned together\n",
    "\n",
    "    Output:\n",
    "    groups - groups organized per layer. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    groups_unique - groups organized per UNIQUE layers only. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    '''\n",
    "    assert len(list_criteria_per_layer) == len(layers_group)\n",
    "    groups = list()\n",
    "\n",
    "    for layer_indx, layer_criteria in enumerate(list_criteria_per_layer):\n",
    "        layer = layer_criteria\n",
    "\n",
    "        if layer_indx == 0:\n",
    "            group_size = 16\n",
    "        else:\n",
    "            if layer_indx%4 == 1:\n",
    "                group_size = 1\n",
    "            elif layer_indx%4 == 2:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 3:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 0:\n",
    "                group_size = 32\n",
    "\n",
    "        if layers_group[layer_indx] != -1:\n",
    "\n",
    "            #if layer/parameter is a part of the group\n",
    "            #then we aggregate importance across the group\n",
    "            #this procedure is repeated for each layer/parameter in the group\n",
    "\n",
    "            all_criteria = np.asarray([lc for li, lc in enumerate(list_criteria_per_layer) if layers_group[layer_indx]==layers_group[li]])\n",
    "\n",
    "            all_criteria = all_criteria.sum(0)\n",
    "            layer = all_criteria\n",
    "\n",
    "        groups_in_layer = list()\n",
    "        indeces = np.argsort(layer)\n",
    "        for group_id in range(int(np.ceil(len(layer)/group_size))):\n",
    "            current_group = slice(group_id*group_size, min((group_id+1)*group_size, len(layer)))\n",
    "            values = [layer[ind] for ind in indeces[current_group]]\n",
    "            group = [indeces[current_group], sum(values)]\n",
    "\n",
    "            groups_in_layer.append(group)\n",
    "        groups.append(groups_in_layer)\n",
    "\n",
    "    if all( [l==-1 for l in layers_group]):\n",
    "        #group with index -1 means no group\n",
    "        unique_groups = groups\n",
    "    else:\n",
    "        unique_groups = list()\n",
    "        groups_exists = list()\n",
    "        for gi, g in enumerate(groups):\n",
    "            if (layers_group[gi] == -1) or (layers_group[gi] not in groups_exists):\n",
    "                unique_groups.append(g)\n",
    "                groups_exists.append(layers_group[gi])\n",
    "\n",
    "    # if torch.distributed.get_rank() == 0:\n",
    "    #     import pdb;pdb.set_trace()\n",
    "\n",
    "    return groups, unique_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency():\n",
    "    def write_to_debug(what_write_name, what_write_value):\n",
    "        # Aux function to store information in the text file\n",
    "        with open(prune_engine.log_debug, 'a') as f:\n",
    "            f.write(\"{} {}\\n\".format(what_write_name,what_write_value))\n",
    "\n",
    "    def nothing(what_write_name, what_write_value):\n",
    "        pass\n",
    "\n",
    "    #store the mask for future needs\n",
    "    old_mask = copy.deepcopy(prune_engine.pruning_gates)\n",
    "\n",
    "    \n",
    "    write_to_debug = nothing\n",
    "\n",
    "    # compute loss since the last pruning and decide if to prune:\n",
    "    if prune_engine.util_loss_tracker_num > 0:\n",
    "        # validation_error = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        validation_loss = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        # validation_error_long = validation_error\n",
    "        acc = prune_engine.util_acc_tracker / prune_engine.util_loss_tracker_num\n",
    "    else:\n",
    "        print(\"compute loss and run prune_engine.util_add_loss(loss.item()) before running this\")\n",
    "        validation_error = 0.0\n",
    "        acc = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "    prune_engine.util_training_loss = validation_loss\n",
    "    prune_engine.util_training_acc = acc\n",
    "\n",
    "    # reset training loss tracker\n",
    "    prune_engine.util_loss_tracker = 0.0\n",
    "    prune_engine.util_acc_tracker = 0.0\n",
    "    prune_engine.util_loss_tracker_num = 0\n",
    "\n",
    "\n",
    "    if (validation_loss > prune_engine.pruning_threshold) and (prune_engine.pruning_threshold != -1.0):\n",
    "        ## if error is big then skip pruning\n",
    "        print(\"skipping pruning because current loss is: \", validation_loss, \"while limit is set to\", prune_engine.pruning_threshold)\n",
    "        if prune_engine.method != 4:\n",
    "            prune_engine.res_pruning = -1\n",
    "            return -1\n",
    "\n",
    "    if prune_engine.maximum_pruning_iterations <= prune_engine.pruning_iterations_done:\n",
    "        # if reached max number of pruning iterations -> exit\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "    prune_engine.full_list_of_criteria = list()\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "        if prune_engine.iterations_done > 0:\n",
    "            # momentum turned to be useless and even reduces performance\n",
    "            contribution = prune_engine.prune_network_accomulate[\"by_layer\"][layer] / prune_engine.iterations_done\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if len(prune_engine.prune_network_accomulate[\"averaged\"][layer])==0 or not prune_engine.use_momentum or (prune_engine.method in [4, 40, 50, 25]):\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = contribution\n",
    "            else:\n",
    "                # use momentum to accumulate criteria over several pruning iterations:\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = prune_engine.momentum_coeff*prune_engine.prune_network_accomulate[\"averaged\"][layer]+(1.0- prune_engine.momentum_coeff)*contribution\n",
    "\n",
    "            current_layer = prune_engine.prune_network_accomulate[\"averaged\"][layer]\n",
    "            if not (prune_engine.method in [1, 4, 40, 15, 50, 25]):\n",
    "                current_layer = current_layer.cpu().numpy()\n",
    "\n",
    "            if prune_engine.l2_normalization_per_layer:\n",
    "                eps = 1e-8\n",
    "                current_layer = current_layer / (np.linalg.norm(current_layer) + eps)\n",
    "\n",
    "            prune_engine.prune_network_accomulate[\"averaged_cpu\"][layer] = current_layer\n",
    "        else:\n",
    "            print(\"First do some add_criteria iterations\")\n",
    "            return -1\n",
    "\n",
    "        for unit in range(len(prune_engine.parameters[layer])):\n",
    "            criterion_now = current_layer[unit]\n",
    "\n",
    "            # make sure that pruned neurons have 0 criteria\n",
    "            if not prune_engine.push_down:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now * prune_engine.pruning_gates[layer][unit]\n",
    "            else:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "            if prune_engine.method == 50:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "    # count number of neurons\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "    prune_engine.neuron_units = neuron_units\n",
    "    prune_engine.all_neuron_units = all_neuron_units\n",
    "\n",
    "    # store criteria_result into file\n",
    "    if not prune_engine.pruning_silent:\n",
    "\n",
    "        if not torch.distributed.is_initialized() or torch.distributed.get_rank()==0:\n",
    "            import pickle\n",
    "            store_criteria = prune_engine.prune_network_accomulate[\"averaged_cpu\"]\n",
    "            pickle.dump(store_criteria, open(prune_engine.folder_to_write_debug + \"criteria_%04d.pickle\"%prune_engine.pruning_iterations_done, \"wb\"))\n",
    "            if prune_engine.pruning_iterations_done == 0:\n",
    "                pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d.pickle\"%prune_engine.method, \"wb\"))\n",
    "            pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d_final.pickle\"%prune_engine.method, \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "    if not prune_engine.fixed_criteria:\n",
    "        prune_engine.iterations_done = 0\n",
    "\n",
    "    prune_network_criteria_updated = prune_engine.prune_network_criteria\n",
    "\n",
    "    # Compute current model statistic\n",
    "    model_dim = list()\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "        if layer == 0:\n",
    "            model_dim.append(np.nonzero(prune_engine.pruning_gates[layer])[0].size)\n",
    "        else:\n",
    "            if layer%4 == 1:\n",
    "                head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 2:\n",
    "                qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 3:\n",
    "                v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 0:\n",
    "                mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                model_dim.append({'head':head,'QK':qk,'V':v,'MLP':mlp})\n",
    "    \n",
    "    # create groups per layer\n",
    "    # comment later\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "#     return groups, unique_groups, prune_network_criteria_updated\n",
    "\n",
    "    # Compute latency and adjust importance\n",
    "    if prune_engine.latency_regularization:\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer==0 and not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                latency_improve = 0.\n",
    "                for blk in range(12):\n",
    "                    qk_head = model_dim[blk+1]['head']\n",
    "                    qk = model_dim[blk+1]['QK']\n",
    "                    v = model_dim[blk+1]['V']\n",
    "                    mlp = model_dim[blk+1]['MLP']\n",
    "                    latency_improve += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb-1,qk_head,qk,v,mlp)\n",
    "                pc = np.array(prune_network_criteria_updated[layer])\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "            elif not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                qk_head = model_dim[(layer-1)//4+1]['head']\n",
    "                qk = model_dim[(layer-1)//4+1]['QK']\n",
    "                v = model_dim[(layer-1)//4+1]['V']\n",
    "                mlp = model_dim[(layer-1)//4+1]['MLP']\n",
    "                latency_improve = 0.\n",
    "                if layer%4 == 1 and qk_head>2:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head-1,qk,v,mlp)\n",
    "                elif layer%4 == 2 and qk>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk-1,v,mlp)\n",
    "                elif layer%4 == 3 and v>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v-1,mlp)\n",
    "                elif layer%4 == 0 and mlp>16:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v,mlp-1)\n",
    "\n",
    "                pc = np.array(prune_network_criteria_updated[layer])#+1\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "\n",
    "\n",
    "    # create groups per layer\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "\n",
    "    # get an array of all criteria from groups\n",
    "    all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)\n",
    "    # from IPython import embed; embed()\n",
    "\n",
    "    prune_neurons_now = (prune_engine.pruning_iterations_done * prune_engine.prune_per_iteration)//prune_engine.group_size - 1\n",
    "    if prune_engine.push_down:\n",
    "        removed_gates = sum([(a==0.0).sum() for a in prune_engine.pruning_gates])\n",
    "        prune_additionally = prune_engine.prune_per_iteration\n",
    "        prune_neurons_now = (removed_gates + prune_additionally) // prune_engine.group_size - 1\n",
    "\n",
    "    if prune_engine.prune_neurons_max != -1:\n",
    "        prune_neurons_now = max(0,min(len(all_criteria)-1, min(prune_neurons_now, prune_engine.prune_neurons_max//prune_engine.group_size - 1)))\n",
    "\n",
    "    if prune_engine.push_down:\n",
    "        prune_engine.reset_gates_to_1()\n",
    "\n",
    "    # adaptively estimate threshold given a number of neurons to be removed\n",
    "    print(prune_neurons_now)\n",
    "    prune_neurons_now = 100\n",
    "    threshold_now = np.sort(all_criteria)[prune_neurons_now]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if np.isnan(threshold_now):\n",
    "        print(\"skipping\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "    prune_engine.pruning_iterations_done += 1\n",
    "\n",
    "\n",
    "    if prune_engine.pruning_iterations_done < prune_engine.start_pruning_after_n_iterations:\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if prune_engine.prune_per_iteration == 0:\n",
    "            continue\n",
    "\n",
    "        total_groups_in_layer = len(groups[layer])\n",
    "        zeroed_groups = 0\n",
    "\n",
    "        for group in groups[layer]:\n",
    "            if group[1] <= threshold_now:\n",
    "                print(layer, group)\n",
    "                #add skip if all groups are set to zero in the current layer:\n",
    "                if (zeroed_groups >= total_groups_in_layer-1) and prune_engine.leave_at_least_one_group:\n",
    "                    print(\"PRUNING: skipping the group because others are zero\")\n",
    "                    continue\n",
    "\n",
    "                zeroed_groups += 1\n",
    "                for unit in group[0]:\n",
    "                    # do actual pruning\n",
    "                    if prune_engine.leave_at_least_one_group and (prune_engine.pruning_gates[layer].sum()<=1):\n",
    "                        print(\"PRUNING: skipping setting the last neuron to zero\")\n",
    "                        continue\n",
    "\n",
    "                    prune_engine.pruning_gates[layer][unit] *= 0.0\n",
    "\n",
    "\n",
    "                    if not prune_engine.push_down:\n",
    "                        for param in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "\n",
    "                            if check_allow_trim(param):\n",
    "                                in_the_range = unit + param[\"shift\"] < param[\"parameter\"].data.shape[param[\"dim\"]]\n",
    "                                if (not in_the_range) or not(unit + param[\"shift\"] >= 0):\n",
    "                                    continue\n",
    "\n",
    "                            if param[\"dim\"] == 0:\n",
    "                                param[\"parameter\"].data[unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 1:\n",
    "                                param[\"parameter\"].data[:, unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 2:\n",
    "                                param[\"parameter\"].data[:, :, unit + param[\"shift\"]] *= 0.0\n",
    "\n",
    "        write_to_debug(\"pruned_perc:\", [np.nonzero(1.0-prune_engine.pruning_gates[layer])[0].size, len(prune_engine.pruning_gates[layer])])\n",
    "\n",
    "    # count number of neurons\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        model_dim = np.zeros((1,49))\n",
    "        latency = 0.\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer == 0:\n",
    "                model_dim[0,0] = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            else:\n",
    "                if layer%4 == 1:\n",
    "                    qk_head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+1] = qk_head\n",
    "                elif layer%4 == 2:\n",
    "                    qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+13] = qk\n",
    "                elif layer%4 == 3:\n",
    "                    v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+25] = v\n",
    "                elif layer%4 == 0:\n",
    "                    mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+36] = mlp\n",
    "                    latency += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)\n",
    "\n",
    "        prune_engine.current_latency = latency\n",
    "\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "\n",
    "    prune_engine.pruned_neurons = all_neuron_units-neuron_units\n",
    "\n",
    "    if prune_engine.method == 25:\n",
    "        prune_engine.method_25_first_done = True\n",
    "\n",
    "    prune_engine.threshold_now = threshold_now\n",
    "    try:\n",
    "        prune_engine.min_criteria_value = (all_criteria[all_criteria > 0.0]).min()\n",
    "        prune_engine.max_criteria_value = (all_criteria[all_criteria > 0.0]).max()\n",
    "        prune_engine.median_criteria_value = np.median(all_criteria[all_criteria > 0.0])\n",
    "\n",
    "        prune_engine.min_max_crit_stats =list()\n",
    "        for layer_id, layer in enumerate(unique_groups):\n",
    "            criterias_group = np.asarray([group[1] for group in layer])\n",
    "            min_c = criterias_group[criterias_group>0.0].min()\n",
    "            max_c = criterias_group[criterias_group>0.0].max()\n",
    "            mean_c = criterias_group[criterias_group>0.0].mean()\n",
    "            prune_engine.min_max_crit_stats.append({\"min\": min_c, \"max\": max_c, \"mean_c\": mean_c})\n",
    "\n",
    "    except:\n",
    "        prune_engine.min_criteria_value = 0.0\n",
    "        prune_engine.max_criteria_value = 0.0\n",
    "        prune_engine.median_criteria_value = 0.0\n",
    "\n",
    "    #get overlap\n",
    "    prune_engine.overlap_score = prune_engine.compute_mask_overlap(old_mask, prune_engine.pruning_gates)\n",
    "\n",
    "    # set result to successful\n",
    "    prune_engine.res_pruning = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb, (head, qk, v, mlp) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 [array([291, 471, 552,  81, 332, 343, 655, 445, 639, 253,  68, 577, 539,\n",
      "       487, 524, 551]), -4.471082928077408e-06]\n",
      "0 [array([637, 277, 517, 212, 617, 131, 233, 631, 404, 556, 502, 615, 174,\n",
      "       114, 419, 451]), -4.313450832256649e-06]\n",
      "0 [array([215, 722, 383, 579, 109, 371, 316,  60,  27, 598,  22, 505, 519,\n",
      "       143,  90, 417]), -4.255369768742412e-06]\n",
      "0 [array([119, 508, 499, 162, 232, 580, 726, 406, 155, 134, 398, 270,  63,\n",
      "       140, 482, 664]), -4.197164747495208e-06]\n",
      "0 [array([501, 547, 112, 738, 695, 274, 113, 348, 128, 154, 701, 293,  51,\n",
      "       136, 731, 659]), -4.129881999233476e-06]\n",
      "0 [array([523, 164, 534, 276, 461,  54, 393, 742, 729, 159, 301,  92, 642,\n",
      "       666, 509, 531]), -4.077673248133351e-06]\n",
      "0 [array([418, 720, 628, 635, 267, 458, 349, 296, 424, 585, 657, 300, 747,\n",
      "         9, 252, 756]), -4.0340965787493134e-06]\n",
      "0 [array([240, 133, 258, 663, 299, 334, 260, 150, 571, 715, 413, 241, 621,\n",
      "        80, 236, 163]), -3.99246691370081e-06]\n",
      "0 [array([572, 120, 680,  61, 466, 228, 275, 245, 124, 202, 152, 725,  83,\n",
      "       171, 739,  84]), -3.947320154480849e-06]\n",
      "0 [array([138, 616, 199, 623, 606, 436, 542, 712, 662, 108, 350, 697, 185,\n",
      "       750, 322, 604]), -3.8977354270457455e-06]\n",
      "0 [array([536, 339, 749,  42, 333,  74, 763, 643, 586,  36, 226, 548, 230,\n",
      "       490, 239, 702]), -3.8429890862516915e-06]\n",
      "0 [array([153,  47, 194, 402, 691, 656, 660, 200, 372, 211, 734, 255, 439,\n",
      "       497, 699,  97]), -3.8058618072227547e-06]\n",
      "0 [array([467, 641, 603, 654, 347, 249, 751, 462,  37, 511, 405, 503, 423,\n",
      "       692, 450, 455]), -3.7715418896482335e-06]\n",
      "0 [array([173, 619, 411, 355, 195, 289,   8, 592, 495,  53, 448, 543, 122,\n",
      "       578, 690, 167]), -3.7285127644963724e-06]\n",
      "0 [array([304, 609, 684, 627, 281, 709, 597, 581, 500, 718,  67, 493, 156,\n",
      "       318, 447, 473]), -3.6898110648735384e-06]\n",
      "0 [array([208,  45, 574, 297,  19, 634, 254,  26, 147,  25, 589,  78, 510,\n",
      "       157, 385, 331]), -3.6392989299542934e-06]\n",
      "0 [array([ 87, 370,  34, 670, 646, 629, 209, 758, 594, 188,  93, 694, 175,\n",
      "       146, 151, 429]), -3.591784631680639e-06]\n",
      "0 [array([689, 588, 719, 569, 104, 246, 360, 427, 361, 464, 395, 311,  65,\n",
      "        96, 149, 178]), -3.540969244227199e-06]\n",
      "0 [array([421, 106, 472,  55, 516, 187, 123, 518, 324, 484, 687, 329, 645,\n",
      "       541, 647, 582]), -3.4994420572047604e-06]\n",
      "0 [array([477, 313, 137, 166, 285, 191, 261,   0, 352, 533, 735, 177, 431,\n",
      "       363,  98, 231]), -3.467936862307397e-06]\n",
      "0 [array([314, 456, 563, 346,  40, 512,  48, 529, 374, 387,  79,  18, 319,\n",
      "       380, 459, 358]), -3.4290671264329832e-06]\n",
      "0 [array([764, 650, 205, 238,  15, 600, 494, 515, 443, 728, 148, 452, 686,\n",
      "       129, 345, 416]), -3.4009306928055595e-06]\n",
      "0 [array([676, 190, 307, 470, 207, 622, 698, 658, 721,  12, 115,  20,   3,\n",
      "       601, 292,  57]), -3.346273152089907e-06]\n",
      "0 [array([760, 632, 724, 182, 620, 457, 446, 244, 683, 479, 492, 575,  86,\n",
      "       608, 422, 336]), -3.3029121750587366e-06]\n",
      "0 [array([568, 362, 630, 271, 409, 555, 282,  69, 403, 682, 243, 468, 216,\n",
      "       325, 644, 696]), -3.2513255005710563e-06]\n",
      "0 [array([221, 506, 229, 481, 265, 573, 399, 288, 176, 716, 280, 356, 557,\n",
      "       491,  72, 103]), -3.208494886848712e-06]\n",
      "0 [array([651, 435, 375, 391, 704,  35, 344,  99, 287, 223,   7, 354,  39,\n",
      "       736, 105, 107]), -3.1484955711107433e-06]\n",
      "0 [array([537, 135, 327,  23, 607, 488,  10, 583, 700, 741, 706, 514, 626,\n",
      "       192, 591, 550]), -3.086582174034902e-06]\n",
      "0 [array([303, 611, 730,  21, 525, 125, 217, 283, 381, 553, 340, 668, 210,\n",
      "       394, 117, 220]), -3.0359521956029313e-06]\n",
      "0 [array([320, 130, 198, 610, 489,  41, 183, 388, 353, 225, 440, 483, 520,\n",
      "       745, 373, 640]), -2.9784693877559222e-06]\n",
      "0 [array([179, 566, 746, 305, 545, 365,  43, 203, 757, 101, 111, 593, 486,\n",
      "       560, 213, 612]), -2.8901356030246463e-06]\n",
      "0 [array([425,  46, 227, 624, 441, 306, 449, 677,  95,  31, 544, 145, 341,\n",
      "       180, 234, 386]), -2.806014647489974e-06]\n",
      "0 [array([160, 268, 605,  89, 561, 102, 337, 535,  85, 596, 369, 256, 294,\n",
      "       672, 197, 351]), -2.7438593674560254e-06]\n",
      "0 [array([504,  94, 279, 189, 754, 272, 142, 478, 317, 116, 132, 242, 193,\n",
      "       717,  32, 100]), -2.635411588698844e-06]\n",
      "0 [array([613, 707, 453, 158, 602,  28, 476, 118, 463, 384, 141, 432, 521,\n",
      "       618, 214,  62]), -2.5511793337500423e-06]\n",
      "0 [array([ 52,  76, 315, 711, 169, 218, 204, 396, 559, 554, 678, 250, 330,\n",
      "       652, 748, 237]), -2.4831462736756295e-06]\n",
      "0 [array([567, 614, 723,  58, 219, 357, 284, 703,  24, 752, 638, 165, 480,\n",
      "       475,  33, 507]), -2.3989341412971045e-06]\n",
      "0 [array([ 73, 526, 743, 161, 442, 530, 633,  13, 121, 172, 235, 321, 753,\n",
      "       653, 673, 290]), -2.33085800812205e-06]\n",
      "0 [array([469, 433, 407,  66, 599, 765, 766, 648, 222,   2, 528, 420,   1,\n",
      "       266, 127, 693]), -2.1683557049527737e-06]\n",
      "0 [array([310, 181, 576, 428,   4, 732, 390, 309,  14, 430, 273, 367, 144,\n",
      "       759, 587, 392]), -1.988612451782501e-06]\n",
      "0 [array([264, 584, 224, 286, 669,  38,  11, 308,  70, 295, 298, 595, 761,\n",
      "       434, 465, 426]), -1.868003819367914e-06]\n",
      "0 [array([  6, 438, 762, 342, 460, 636,  59, 649, 767, 527,  50, 485, 454,\n",
      "       674, 247, 681]), -1.619863878772776e-06]\n",
      "0 [array([259, 414, 667, 379, 733, 338, 522, 408, 251, 269,  71, 410,  82,\n",
      "       257, 671, 538]), -1.2696936677230043e-06]\n",
      "0 [array([376, 364, 665, 513, 737, 570, 685, 335, 625, 248, 170, 139, 184,\n",
      "       382, 444, 397]), -9.215716245307705e-07]\n",
      "1 [array([3]), -1.2048242813212582e-06]\n",
      "1 [array([7]), -1.0049578432547932e-06]\n",
      "1 [array([4]), -8.423420948323255e-07]\n",
      "5 [array([0]), -1.2264144563024146e-06]\n",
      "5 [array([3]), -1.210085836068707e-06]\n",
      "5 [array([9]), -1.1649007359627248e-06]\n",
      "5 [array([10]), -1.1562341254223938e-06]\n",
      "5 [array([1]), -1.1540719438774758e-06]\n",
      "5 [array([8]), -1.1147229682440738e-06]\n",
      "9 [array([0]), -1.158782280886804e-06]\n",
      "9 [array([11]), -1.0593414910620922e-06]\n",
      "9 [array([3]), -1.0553796752977505e-06]\n",
      "9 [array([5]), -9.906412313588007e-07]\n",
      "9 [array([7]), -9.834987420921024e-07]\n",
      "9 [array([10]), -9.674969786222718e-07]\n",
      "9 [array([1]), -7.11981206526012e-07]\n",
      "9 [array([9]), -7.04313029321689e-07]\n",
      "9 [array([8]), -6.603847762865698e-07]\n",
      "13 [array([9]), -1.1348119156385437e-06]\n",
      "13 [array([3]), -9.110936130885747e-07]\n",
      "13 [array([5]), -8.411378101603404e-07]\n",
      "13 [array([6]), -7.812707477417789e-07]\n",
      "17 [array([1]), -9.063040150370725e-07]\n",
      "17 [array([0]), -7.961552822356097e-07]\n",
      "17 [array([10]), -7.813008463320656e-07]\n",
      "17 [array([6]), -7.490971737145955e-07]\n",
      "17 [array([3]), -6.966214894722143e-07]\n",
      "21 [array([4]), -9.768881082739753e-07]\n",
      "21 [array([5]), -9.306759719341571e-07]\n",
      "21 [array([9]), -7.966040041840969e-07]\n",
      "25 [array([0]), -9.031930599879487e-07]\n",
      "29 [array([11]), -1.06353951965351e-06]\n",
      "29 [array([0]), -8.654153217169878e-07]\n",
      "29 [array([6]), -8.406134580430589e-07]\n",
      "29 [array([4]), -8.008144246408264e-07]\n",
      "29 [array([8]), -7.520833290376103e-07]\n",
      "29 [array([3]), -6.885139126400972e-07]\n",
      "33 [array([11]), -9.90920446232245e-07]\n",
      "33 [array([6]), -9.36493185311825e-07]\n",
      "33 [array([5]), -8.552536226792071e-07]\n",
      "33 [array([3]), -8.510968624098823e-07]\n",
      "33 [array([1]), -7.248721570552661e-07]\n",
      "33 [array([2]), -6.547282876757308e-07]\n",
      "37 [array([4]), -1.0643599265070733e-06]\n",
      "37 [array([10]), -9.616993478024014e-07]\n",
      "37 [array([8]), -9.398954628826104e-07]\n",
      "37 [array([11]), -8.061533006487807e-07]\n",
      "37 [array([3]), -7.903581914282185e-07]\n",
      "37 [array([0]), -7.579455908247255e-07]\n",
      "37 [array([7]), -7.150928152744527e-07]\n",
      "37 [array([1]), -6.771695010077892e-07]\n",
      "41 [array([10]), -9.622025825895761e-07]\n",
      "41 [array([0]), -8.957017510814111e-07]\n",
      "41 [array([4]), -7.968462424135723e-07]\n",
      "41 [array([6]), -6.976249463453641e-07]\n",
      "41 [array([9]), -6.555288135435475e-07]\n",
      "45 [array([3]), -8.470148512363593e-07]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/workspace/alex/NViT/nvit/pre_compute_saliency.pkl\", 'rb') as f:\n",
    "    prune_engine = pickle.load(f)\n",
    "    prune_engine.group_criteria = group_criteria\n",
    "# model_dim = compute_saliency()\n",
    "# groups, unique_groups, prune_network_criteria_updated = compute_saliency()\n",
    "compute_saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([3]), 5.1065452311149784e-08],\n",
       " [array([7]), 2.5093189037761476e-07],\n",
       " [array([4]), 4.135476388000825e-07],\n",
       " [array([0]), 8.859171316544234e-07],\n",
       " [array([1]), 9.903394584398484e-07],\n",
       " [array([6]), 1.0704957276175264e-06],\n",
       " [array([5]), 1.2170478385087335e-06],\n",
       " [array([8]), 1.4203769751475193e-06],\n",
       " [array([9]), 1.5064233593875542e-06],\n",
       " [array([10]), 1.6574052779105841e-06],\n",
       " [array([2]), 2.4562550606788136e-06],\n",
       " [array([11]), 4.141616955166683e-06]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_importance = np.min(all_criteria)\n",
    "offset = np.abs(min_importance) + 1e-8\n",
    "IMPORTANCE_SCALE = 1e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 256\n",
    "NUM_TOKENS = 198\n",
    "WARMUP = 20\n",
    "TOTAL = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f\"mlp_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    mlp_lut = pickle.load(f)\n",
    "    mlp_lut = mlp_lut[1:, 1:]\n",
    "save_name = f\"qk_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    qk_lut = pickle.load(f)\n",
    "    qk_lut = qk_lut[1:, 1:, 1:]\n",
    "save_name = f\"vandproj_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    vandproj_lut = pickle.load(f)\n",
    "    vandproj_lut = vandproj_lut[1:, 1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pyomo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pyomo.environ import *\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "# all_variable_specs = {\n",
    "#     \"EMB\": [768, 16, 768//16+1],\n",
    "#     \"HEAD\": [12, 1, 12//1+1],\n",
    "#     \"QK\": [64, 2, 64//2+1],\n",
    "#     \"V\": [64, 2, 64//2+1],\n",
    "#     \"MLP\": [3072, 32, 3072//32+1],\n",
    "# }\n",
    "\n",
    "EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "all_variable_specs = {\n",
    "    \"EMB\": [768, 16, 768//16],\n",
    "    \"HEAD\": [12, 1, 12//1],\n",
    "    \"QK\": [64, 2, 64//2],\n",
    "    \"V\": [64, 2, 64//2],\n",
    "    \"MLP\": [3072, 32, 3072//32],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "importance_dict = {}\n",
    "NAMES = [\"HEAD\", \"QK\", \"V\", \"MLP\"]\n",
    "for layer_idx, layer_importance in enumerate(unique_groups):\n",
    "    block_idx = (layer_idx - 1) // 4\n",
    "    layer_name = \"EMB\" if layer_idx == 0 else f\"{block_idx}_{NAMES[(layer_idx-1) % 4]}\"\n",
    "    running_total_importance = 0\n",
    "    running_total_indices = []\n",
    "    importance_dict[layer_name] = {\"importance\":[], \"indices\":[]}\n",
    "    for x in reversed(layer_importance):\n",
    "        group_indices, group_importance = x\n",
    "#         transformed_group_importance = (group_importance + offset) * IMPORTANCE_SCALE\n",
    "        transformed_group_importance = int((group_importance + offset) * IMPORTANCE_SCALE)\n",
    "        \n",
    "        running_total_importance += transformed_group_importance\n",
    "        running_total_indices += list(group_indices)\n",
    "        \n",
    "        importance_dict[layer_name][\"importance\"].append(running_total_importance)\n",
    "        importance_dict[layer_name][\"indices\"].append(deepcopy(running_total_indices))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14766,\n",
       " 16387,\n",
       " 17135,\n",
       " 17618,\n",
       " 18014,\n",
       " 18375,\n",
       " 18701,\n",
       " 19002,\n",
       " 19291,\n",
       " 19562,\n",
       " 19817,\n",
       " 20065,\n",
       " 20304,\n",
       " 20537,\n",
       " 20761,\n",
       " 20974,\n",
       " 21181,\n",
       " 21380,\n",
       " 21570,\n",
       " 21754,\n",
       " 21933,\n",
       " 22106,\n",
       " 22273,\n",
       " 22436,\n",
       " 22593,\n",
       " 22746,\n",
       " 22894,\n",
       " 23039,\n",
       " 23180,\n",
       " 23318,\n",
       " 23452,\n",
       " 23581,\n",
       " 23705,\n",
       " 23824,\n",
       " 23939,\n",
       " 24050,\n",
       " 24157,\n",
       " 24260,\n",
       " 24358,\n",
       " 24451,\n",
       " 24539,\n",
       " 24623,\n",
       " 24703,\n",
       " 24778,\n",
       " 24846,\n",
       " 24908,\n",
       " 24964,\n",
       " 25005]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict[\"EMB\"][\"importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23496,\n",
       " 33647,\n",
       " 41757,\n",
       " 48711,\n",
       " 54934,\n",
       " 60541,\n",
       " 65629,\n",
       " 70174,\n",
       " 74246,\n",
       " 77942,\n",
       " 81303,\n",
       " 84298,\n",
       " 86731,\n",
       " 88684,\n",
       " 90310,\n",
       " 91722,\n",
       " 92903,\n",
       " 93939,\n",
       " 94877,\n",
       " 95731,\n",
       " 96497,\n",
       " 97193,\n",
       " 97839,\n",
       " 98439,\n",
       " 98989,\n",
       " 99505,\n",
       " 99985,\n",
       " 100420,\n",
       " 100818,\n",
       " 101190,\n",
       " 101539,\n",
       " 101865,\n",
       " 102161,\n",
       " 102435,\n",
       " 102687,\n",
       " 102917,\n",
       " 103132,\n",
       " 103332,\n",
       " 103518,\n",
       " 103690,\n",
       " 103850,\n",
       " 103998,\n",
       " 104135,\n",
       " 104259,\n",
       " 104374,\n",
       " 104478,\n",
       " 104574,\n",
       " 104663,\n",
       " 104747,\n",
       " 104827,\n",
       " 104902,\n",
       " 104973,\n",
       " 105040,\n",
       " 105104,\n",
       " 105164,\n",
       " 105222,\n",
       " 105277,\n",
       " 105330,\n",
       " 105381,\n",
       " 105430,\n",
       " 105477,\n",
       " 105522,\n",
       " 105565,\n",
       " 105606,\n",
       " 105645,\n",
       " 105683,\n",
       " 105720,\n",
       " 105756,\n",
       " 105790,\n",
       " 105823,\n",
       " 105856,\n",
       " 105888,\n",
       " 105919,\n",
       " 105949,\n",
       " 105978,\n",
       " 106005,\n",
       " 106032,\n",
       " 106058,\n",
       " 106083,\n",
       " 106107,\n",
       " 106130,\n",
       " 106152,\n",
       " 106174,\n",
       " 106195,\n",
       " 106215,\n",
       " 106234,\n",
       " 106252,\n",
       " 106269,\n",
       " 106285,\n",
       " 106300,\n",
       " 106314,\n",
       " 106327,\n",
       " 106339,\n",
       " 106350,\n",
       " 106360,\n",
       " 106367]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict[\"0_MLP\"][\"importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EMB', '0_HEAD', '0_QK', '0_V', '0_MLP', '1_HEAD', '1_QK', '1_V', '1_MLP', '2_HEAD', '2_QK', '2_V', '2_MLP', '3_HEAD', '3_QK', '3_V', '3_MLP', '4_HEAD', '4_QK', '4_V', '4_MLP', '5_HEAD', '5_QK', '5_V', '5_MLP', '6_HEAD', '6_QK', '6_V', '6_MLP', '7_HEAD', '7_QK', '7_V', '7_MLP', '8_HEAD', '8_QK', '8_V', '8_MLP', '9_HEAD', '9_QK', '9_V', '9_MLP', '10_HEAD', '10_QK', '10_V', '10_MLP', '11_HEAD', '11_QK', '11_V', '11_MLP'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_NUMBER = 12\n",
    "# BLOCK_NUMBER = 1\n",
    "total_latency = (mlp_lut[-1, -1] + vandproj_lut[-1, -1, -1] + qk_lut[-1, -1, -1]) * BLOCK_NUMBER\n",
    "target_latency = total_latency * 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConcreteModel()\n",
    "# Define variables\n",
    "variable_slices_by_type = {}\n",
    "counter = 0\n",
    "variable_slices_by_type[\"EMB\"] = (counter, counter+all_variable_specs[\"EMB\"][2])\n",
    "counter += all_variable_specs[\"EMB\"][2]\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        variable_slices_by_type[f\"{block_idx}_{var_type}\"] = (counter, counter+all_variable_specs[var_type][2])\n",
    "        counter += all_variable_specs[var_type][2]\n",
    "\n",
    "all_items = list(range(counter))\n",
    "model.decision_vars = Var(all_items, domain=Binary)\n",
    "\n",
    "# Define importance and uniqueness constraints\n",
    "importance = 0\n",
    "model.group_unique_constraint = ConstraintList()\n",
    "\n",
    "def add_uniqueness_constraint_and_importance_expr(layer_name):\n",
    "    # uniqueness constraint\n",
    "    # only selecting one configuration\n",
    "    cur_slices = variable_slices_by_type[layer_name]\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in range(cur_slices[0], cur_slices[1])]\n",
    "    model.group_unique_constraint.add(sum(cur_decision_vars) == 1)\n",
    "    # get importance expr\n",
    "    cur_importance = importance_dict[layer_name][\"importance\"]\n",
    "    cur_importance_expr = sum(cur_decision_vars[i] * cur_importance[i] for i in range(len(cur_decision_vars)))\n",
    "    return cur_importance_expr\n",
    "\n",
    "cur_importance_expr = add_uniqueness_constraint_and_importance_expr(\"EMB\")\n",
    "importance += cur_importance_expr\n",
    "\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        cur_importance_expr = add_uniqueness_constraint_and_importance_expr(f\"{block_idx}_{var_type}\")\n",
    "        importance += cur_importance_expr\n",
    "\n",
    "model.obj = Objective(expr=importance, sense=maximize)\n",
    "\n",
    "# Define latency constraint\n",
    "# Add latency constraint\n",
    "latency_expr = 0\n",
    "emb_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1])])\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    head_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_HEAD\"][0], variable_slices_by_type[f\"{block_idx}_HEAD\"][1])])\n",
    "    qk_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_QK\"][0], variable_slices_by_type[f\"{block_idx}_QK\"][1])])\n",
    "    v_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_V\"][0], variable_slices_by_type[f\"{block_idx}_V\"][1])])\n",
    "    mlp_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_MLP\"][0], variable_slices_by_type[f\"{block_idx}_MLP\"][1])])\n",
    "    \n",
    "    T1 = np.tensordot(emb_vectors, mlp_vectors, axes=0)\n",
    "    latency_expr_mlp = np.sum(T1 * mlp_lut)\n",
    "    T2 = np.tensordot(head_vectors, np.tensordot(emb_vectors, v_vectors, axes=0), axes=0)\n",
    "    latency_expr_vandproj = np.sum(T2 * vandproj_lut)\n",
    "    T3 = np.tensordot(head_vectors, np.tensordot(emb_vectors, qk_vectors, axes=0), axes=0)\n",
    "    latency_expr_qk = np.sum(T3 * qk_lut)\n",
    "    latency_expr += latency_expr_mlp + latency_expr_vandproj + latency_expr_qk\n",
    "\n",
    "model.latency_constraint = Constraint(expr=latency_expr <= target_latency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "              Mixed-Integer Nonlinear Decomposition Toolbox in Pyomo (MindtPy)               \n",
      "---------------------------------------------------------------------------------------------\n",
      "For more information, please visit https://pyomo.readthedocs.io/en/stable/contributed_packages/mindtpy.html\n",
      "Original model has 50 constraints (1 nonlinear) and 0 disjunctions, with 2112 variables, of which 2112 are binary, 0 are integer, and 0 are continuous.\n",
      "Moving objective to constraint set.\n",
      "FP is the initial strategy being used.\n",
      "\n",
      " ===============================================================================================\n",
      " Iteration | Subproblem Type | Objective Value | Primal Bound |   Dual Bound |   Gap   | Time(s)\n",
      "\n",
      "         -       Relaxed NLP            722853           -inf         722853      nan%    197.08\n",
      "         1            FP-MIP           1.48487           -inf         722853      nan%    251.86\n",
      "         1            FP-NLP       5.40237e-06           -inf         722853      nan%    328.32\n",
      "*        1         Fixed NLP            721629         721629         722853     0.17%    432.51\n",
      "FP-MIP infeasible\n",
      "         1              MILP            722764         721629         722764     0.16%    447.44\n",
      "*        2         Fixed NLP            722764         722764         722764     0.00%    557.18\n",
      "MindtPy exiting on bound convergence. |Primal Bound: 722764.0072276375 - Dual Bound: 722764.0| / (1e-10 + |Primal Bound|:722764.0072276375) <= relative tolerance: 0.001\n",
      "Solve the main problem without the last no_good cut to fix the bound.zero_tolerance is set to 1E-4\n",
      "Fixed bound values: Primal Bound: 722764.0072276375  Dual Bound: 722764.0\n",
      " ===============================================================================================\n",
      " Primal integral          :   427086.1505 \n",
      " Dual integral            :   17469.4778 \n",
      " Primal-dual gap integral :   444555.6282 \n"
     ]
    }
   ],
   "source": [
    "solver = SolverFactory('mindtpy')\n",
    "# solver = SolverFactorypyomopyo('glpk')\n",
    "# solver.solve(model)\n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, solver_tee=True, time_limit=1800) \n",
    "results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, time_limit=1800) \n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt') \n",
    "# results = solver.solve(model) \n",
    "# results = solver.solve(model, mip_solver='glpk', nlp_solver='ipopt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMB': [768, 16, 48],\n",
       " 'HEAD': [12, 1, 12],\n",
       " 'QK': [64, 2, 32],\n",
       " 'V': [64, 2, 32],\n",
       " 'MLP': [3072, 32, 96]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variable_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 29 95\n",
      "HEAD 9 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 5 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 7 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 8 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 29 95\n",
      "HEAD 9 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 5 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 7 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 8 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "full_values = []\n",
    "pruned_values = []\n",
    "\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "var_type = \"EMB\"\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "names.append(\"EMB\")\n",
    "full_values.append(all_variable_specs[var_type][2])\n",
    "pruned_values.append(np.argmax(cur_decision_vars_value)+1)\n",
    "\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "        names.append(\"EMB\")\n",
    "        full_values.append(all_variable_specs[var_type][2])\n",
    "        pruned_values.append(np.argmax(cur_decision_vars_value)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
