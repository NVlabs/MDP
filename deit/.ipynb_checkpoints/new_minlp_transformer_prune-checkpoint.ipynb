{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_allow_trim(dict):\n",
    "    res = False\n",
    "    if \"allow_trim\" in dict.keys():\n",
    "        if dict[\"allow_trim\"]:\n",
    "            res = True\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_criteria(list_criteria_per_layer, layers_group, group_size=1):\n",
    "    '''\n",
    "    Function combine criteria per neuron into groups of size group_size.\n",
    "    Output is a list of groups organized by layers. Length of output is a number of layers.\n",
    "    The criterion for the group is computed as an average of member's criteria.\n",
    "    Input:\n",
    "    list_criteria_per_layer - list of criteria per neuron organized per layer\n",
    "    group_size - number of neurons per group\n",
    "    layers_group - layers can form a group, e.g. residual connection, they will be pruned together\n",
    "\n",
    "    Output:\n",
    "    groups - groups organized per layer. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    groups_unique - groups organized per UNIQUE layers only. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    '''\n",
    "    assert len(list_criteria_per_layer) == len(layers_group)\n",
    "    groups = list()\n",
    "\n",
    "    for layer_indx, layer_criteria in enumerate(list_criteria_per_layer):\n",
    "        layer = layer_criteria\n",
    "\n",
    "#         if layer_indx == 0:\n",
    "#             group_size = 16\n",
    "#         else:\n",
    "#             if layer_indx%4 == 1:\n",
    "#                 group_size = 1\n",
    "#             elif layer_indx%4 == 2:\n",
    "#                 group_size = 2\n",
    "#             elif layer_indx%4 == 3:\n",
    "#                 group_size = 2\n",
    "#             elif layer_indx%4 == 0:\n",
    "#                 group_size = 32\n",
    "        if layer_indx == 0:\n",
    "            group_size = 16\n",
    "        else:\n",
    "            if layer_indx%4 == 1:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 2:\n",
    "                group_size = 8\n",
    "            elif layer_indx%4 == 3:\n",
    "                group_size = 8\n",
    "            elif layer_indx%4 == 0:\n",
    "                group_size = 16\n",
    "                    \n",
    "        if layers_group[layer_indx] != -1:\n",
    "\n",
    "            #if layer/parameter is a part of the group\n",
    "            #then we aggregate importance across the group\n",
    "            #this procedure is repeated for each layer/parameter in the group\n",
    "\n",
    "            all_criteria = np.asarray([lc for li, lc in enumerate(list_criteria_per_layer) if layers_group[layer_indx]==layers_group[li]])\n",
    "\n",
    "            all_criteria = all_criteria.sum(0)\n",
    "            layer = all_criteria\n",
    "\n",
    "        groups_in_layer = list()\n",
    "        indeces = np.argsort(layer)\n",
    "        for group_id in range(int(np.ceil(len(layer)/group_size))):\n",
    "            current_group = slice(group_id*group_size, min((group_id+1)*group_size, len(layer)))\n",
    "            values = [layer[ind] for ind in indeces[current_group]]\n",
    "            group = [indeces[current_group], sum(values)]\n",
    "\n",
    "            groups_in_layer.append(group)\n",
    "        groups.append(groups_in_layer)\n",
    "\n",
    "    if all( [l==-1 for l in layers_group]):\n",
    "        #group with index -1 means no group\n",
    "        unique_groups = groups\n",
    "    else:\n",
    "        unique_groups = list()\n",
    "        groups_exists = list()\n",
    "        for gi, g in enumerate(groups):\n",
    "            if (layers_group[gi] == -1) or (layers_group[gi] not in groups_exists):\n",
    "                unique_groups.append(g)\n",
    "                groups_exists.append(layers_group[gi])\n",
    "\n",
    "    # if torch.distributed.get_rank() == 0:\n",
    "    #     import pdb;pdb.set_trace()\n",
    "\n",
    "    return groups, unique_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency():\n",
    "    def write_to_debug(what_write_name, what_write_value):\n",
    "        # Aux function to store information in the text file\n",
    "        with open(prune_engine.log_debug, 'a') as f:\n",
    "            f.write(\"{} {}\\n\".format(what_write_name,what_write_value))\n",
    "\n",
    "    def nothing(what_write_name, what_write_value):\n",
    "        pass\n",
    "\n",
    "    #store the mask for future needs\n",
    "    old_mask = copy.deepcopy(prune_engine.pruning_gates)\n",
    "\n",
    "    if prune_engine.method == 50:\n",
    "        write_to_debug = nothing\n",
    "\n",
    "    if not (not torch.distributed.is_initialized() or torch.distributed.get_rank()==0):\n",
    "        write_to_debug = nothing\n",
    "\n",
    "    # compute loss since the last pruning and decide if to prune:\n",
    "    if prune_engine.util_loss_tracker_num > 0:\n",
    "        # validation_error = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        validation_loss = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        # validation_error_long = validation_error\n",
    "        acc = prune_engine.util_acc_tracker / prune_engine.util_loss_tracker_num\n",
    "    else:\n",
    "        print(\"compute loss and run prune_engine.util_add_loss(loss.item()) before running this\")\n",
    "        validation_error = 0.0\n",
    "        acc = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "    prune_engine.util_training_loss = validation_loss\n",
    "    prune_engine.util_training_acc = acc\n",
    "\n",
    "    # reset training loss tracker\n",
    "    prune_engine.util_loss_tracker = 0.0\n",
    "    prune_engine.util_acc_tracker = 0.0\n",
    "    prune_engine.util_loss_tracker_num = 0\n",
    "\n",
    "\n",
    "    if (validation_loss > prune_engine.pruning_threshold) and (prune_engine.pruning_threshold != -1.0):\n",
    "        ## if error is big then skip pruning\n",
    "        print(\"skipping pruning because current loss is: \", validation_loss, \"while limit is set to\", prune_engine.pruning_threshold)\n",
    "        if prune_engine.method != 4:\n",
    "            prune_engine.res_pruning = -1\n",
    "            return -1\n",
    "\n",
    "    if prune_engine.maximum_pruning_iterations <= prune_engine.pruning_iterations_done:\n",
    "        # if reached max number of pruning iterations -> exit\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "    prune_engine.full_list_of_criteria = list()\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "        if prune_engine.iterations_done > 0:\n",
    "            # momentum turned to be useless and even reduces performance\n",
    "            contribution = prune_engine.prune_network_accomulate[\"by_layer\"][layer] / prune_engine.iterations_done\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if len(prune_engine.prune_network_accomulate[\"averaged\"][layer])==0 or not prune_engine.use_momentum or (prune_engine.method in [4, 40, 50, 25]):\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = contribution\n",
    "            else:\n",
    "                # use momentum to accumulate criteria over several pruning iterations:\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = prune_engine.momentum_coeff*prune_engine.prune_network_accomulate[\"averaged\"][layer]+(1.0- prune_engine.momentum_coeff)*contribution\n",
    "\n",
    "            current_layer = prune_engine.prune_network_accomulate[\"averaged\"][layer]\n",
    "            if not (prune_engine.method in [1, 4, 40, 15, 50, 25]):\n",
    "                current_layer = current_layer.cpu().numpy()\n",
    "\n",
    "            if prune_engine.l2_normalization_per_layer:\n",
    "                eps = 1e-8\n",
    "                current_layer = current_layer / (np.linalg.norm(current_layer) + eps)\n",
    "\n",
    "            prune_engine.prune_network_accomulate[\"averaged_cpu\"][layer] = current_layer\n",
    "        else:\n",
    "            print(\"First do some add_criteria iterations\")\n",
    "            return -1\n",
    "\n",
    "        for unit in range(len(prune_engine.parameters[layer])):\n",
    "            criterion_now = current_layer[unit]\n",
    "\n",
    "            # make sure that pruned neurons have 0 criteria\n",
    "            if not prune_engine.push_down:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now * prune_engine.pruning_gates[layer][unit]\n",
    "            else:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "            if prune_engine.method == 50:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "    # count number of neurons\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "    prune_engine.neuron_units = neuron_units\n",
    "    prune_engine.all_neuron_units = all_neuron_units\n",
    "\n",
    "    # store criteria_result into file\n",
    "    if not prune_engine.pruning_silent:\n",
    "\n",
    "        if not torch.distributed.is_initialized() or torch.distributed.get_rank()==0:\n",
    "            import pickle\n",
    "            store_criteria = prune_engine.prune_network_accomulate[\"averaged_cpu\"]\n",
    "            pickle.dump(store_criteria, open(prune_engine.folder_to_write_debug + \"criteria_%04d.pickle\"%prune_engine.pruning_iterations_done, \"wb\"))\n",
    "            if prune_engine.pruning_iterations_done == 0:\n",
    "                pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d.pickle\"%prune_engine.method, \"wb\"))\n",
    "            pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d_final.pickle\"%prune_engine.method, \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "    if not prune_engine.fixed_criteria:\n",
    "        prune_engine.iterations_done = 0\n",
    "\n",
    "    prune_network_criteria_updated = prune_engine.prune_network_criteria\n",
    "\n",
    "    # Compute current model statistic\n",
    "    model_dim = list()\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "        if layer == 0:\n",
    "            model_dim.append(np.nonzero(prune_engine.pruning_gates[layer])[0].size)\n",
    "        else:\n",
    "            if layer%4 == 1:\n",
    "                head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 2:\n",
    "                qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 3:\n",
    "                v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 0:\n",
    "                mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                model_dim.append({'head':head,'QK':qk,'V':v,'MLP':mlp})\n",
    "    \n",
    "    # create groups per layer\n",
    "    # comment later\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "    return groups, unique_groups, prune_network_criteria_updated\n",
    "\n",
    "    # Compute latency and adjust importance\n",
    "    if prune_engine.latency_regularization:\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer==0 and not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                latency_improve = 0.\n",
    "                for blk in range(12):\n",
    "                    qk_head = model_dim[blk+1]['head']\n",
    "                    qk = model_dim[blk+1]['QK']\n",
    "                    v = model_dim[blk+1]['V']\n",
    "                    mlp = model_dim[blk+1]['MLP']\n",
    "                    latency_improve += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb-1,qk_head,qk,v,mlp)\n",
    "                pc = np.array(prune_network_criteria_updated[layer])\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "            elif not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                qk_head = model_dim[(layer-1)//4+1]['head']\n",
    "                qk = model_dim[(layer-1)//4+1]['QK']\n",
    "                v = model_dim[(layer-1)//4+1]['V']\n",
    "                mlp = model_dim[(layer-1)//4+1]['MLP']\n",
    "                latency_improve = 0.\n",
    "                if layer%4 == 1 and qk_head>2:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head-1,qk,v,mlp)\n",
    "                elif layer%4 == 2 and qk>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk-1,v,mlp)\n",
    "                elif layer%4 == 3 and v>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v-1,mlp)\n",
    "                elif layer%4 == 0 and mlp>16:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v,mlp-1)\n",
    "\n",
    "                pc = np.array(prune_network_criteria_updated[layer])#+1\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "\n",
    "\n",
    "    # create groups per layer\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "\n",
    "    # get an array of all criteria from groups\n",
    "    all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)\n",
    "    # from IPython import embed; embed()\n",
    "\n",
    "    prune_neurons_now = (prune_engine.pruning_iterations_done * prune_engine.prune_per_iteration)//prune_engine.group_size - 1\n",
    "    if prune_engine.push_down:\n",
    "        removed_gates = sum([(a==0.0).sum() for a in prune_engine.pruning_gates])\n",
    "        prune_additionally = prune_engine.prune_per_iteration\n",
    "        prune_neurons_now = (removed_gates + prune_additionally) // prune_engine.group_size - 1\n",
    "\n",
    "    if prune_engine.prune_neurons_max != -1:\n",
    "        prune_neurons_now = max(0,min(len(all_criteria)-1, min(prune_neurons_now, prune_engine.prune_neurons_max//prune_engine.group_size - 1)))\n",
    "\n",
    "    if prune_engine.push_down:\n",
    "        prune_engine.reset_gates_to_1()\n",
    "\n",
    "    # adaptively estimate threshold given a number of neurons to be removed\n",
    "    threshold_now = np.sort(all_criteria)[prune_neurons_now]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if np.isnan(threshold_now):\n",
    "        print(\"skipping\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "    prune_engine.pruning_iterations_done += 1\n",
    "\n",
    "    prune_engine.log_debug = prune_engine.folder_to_write_debug + 'debugOutput_pruning_%08d' % (\n",
    "        prune_engine.pruning_iterations_done) + '.txt'\n",
    "    write_to_debug(\"method\", prune_engine.method)\n",
    "    write_to_debug(\"pruned_neurons\", prune_engine.pruned_neurons)\n",
    "    write_to_debug(\"pruning_iterations_done\", prune_engine.pruning_iterations_done)\n",
    "    write_to_debug(\"neuron_units\", neuron_units)\n",
    "    write_to_debug(\"all_neuron_units\", all_neuron_units)\n",
    "    write_to_debug(\"threshold_now\", threshold_now)\n",
    "    write_to_debug(\"groups_total\", sum([len(layer) for layer in groups]))\n",
    "    write_to_debug(\"uniquegroups_total\", sum([len(layer) for layer in unique_groups]))\n",
    "\n",
    "    if prune_engine.pruning_iterations_done < prune_engine.start_pruning_after_n_iterations:\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        write_to_debug(\"\\nLayer:\", layer)\n",
    "        write_to_debug(\"units:\", len(prune_engine.parameters[layer]))\n",
    "        write_to_debug(\"layers_group:\", prune_engine.layers_group[layer])\n",
    "\n",
    "        shapes = [\" \".join([\"{}\".format(a[\"parameter\"].shape) for a in prune_engine.pruning_parameters[layer][\"compute_criteria_from\"]])]\n",
    "        write_to_debug(\"layers_shapes:\", shapes)\n",
    "        for a in prune_engine.pruning_parameters[layer][\"compute_criteria_from\"]:\n",
    "            write_to_debug(\"compute_criteria_from:\", \"{}, dim : {}\".format(a[\"parameter_name\"], a[\"dim\"]))\n",
    "        for a in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "            write_to_debug(\"set_to_zero:\", \"{}, dim : {}\".format(a[\"parameter_name\"], a[\"dim\"]))\n",
    "\n",
    "\n",
    "        if prune_engine.prune_per_iteration == 0:\n",
    "            continue\n",
    "\n",
    "        total_groups_in_layer = len(groups[layer])\n",
    "        zeroed_groups = 0\n",
    "\n",
    "        for group in groups[layer]:\n",
    "            if group[1] <= threshold_now:\n",
    "\n",
    "                #add skip if all groups are set to zero in the current layer:\n",
    "                if (zeroed_groups >= total_groups_in_layer-1) and prune_engine.leave_at_least_one_group:\n",
    "                    print(\"PRUNING: skipping the group because others are zero\")\n",
    "                    continue\n",
    "\n",
    "                zeroed_groups += 1\n",
    "                for unit in group[0]:\n",
    "                    # do actual pruning\n",
    "                    if prune_engine.leave_at_least_one_group and (prune_engine.pruning_gates[layer].sum()<=1):\n",
    "                        print(\"PRUNING: skipping setting the last neuron to zero\")\n",
    "                        continue\n",
    "\n",
    "                    prune_engine.pruning_gates[layer][unit] *= 0.0\n",
    "\n",
    "\n",
    "                    if not prune_engine.push_down:\n",
    "                        for param in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "\n",
    "                            if check_allow_trim(param):\n",
    "                                in_the_range = unit + param[\"shift\"] < param[\"parameter\"].data.shape[param[\"dim\"]]\n",
    "                                if (not in_the_range) or not(unit + param[\"shift\"] >= 0):\n",
    "                                    continue\n",
    "\n",
    "                            if param[\"dim\"] == 0:\n",
    "                                param[\"parameter\"].data[unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 1:\n",
    "                                param[\"parameter\"].data[:, unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 2:\n",
    "                                param[\"parameter\"].data[:, :, unit + param[\"shift\"]] *= 0.0\n",
    "\n",
    "        write_to_debug(\"pruned_perc:\", [np.nonzero(1.0-prune_engine.pruning_gates[layer])[0].size, len(prune_engine.pruning_gates[layer])])\n",
    "\n",
    "    # count number of neurons\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        model_dim = np.zeros((1,49))\n",
    "        latency = 0.\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer == 0:\n",
    "                model_dim[0,0] = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            else:\n",
    "                if layer%4 == 1:\n",
    "                    qk_head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+1] = qk_head\n",
    "                elif layer%4 == 2:\n",
    "                    qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+13] = qk\n",
    "                elif layer%4 == 3:\n",
    "                    v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+25] = v\n",
    "                elif layer%4 == 0:\n",
    "                    mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+36] = mlp\n",
    "                    latency += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)\n",
    "\n",
    "        prune_engine.current_latency = latency\n",
    "\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "\n",
    "    prune_engine.pruned_neurons = all_neuron_units-neuron_units\n",
    "\n",
    "    if prune_engine.method == 25:\n",
    "        prune_engine.method_25_first_done = True\n",
    "\n",
    "    prune_engine.threshold_now = threshold_now\n",
    "    try:\n",
    "        prune_engine.min_criteria_value = (all_criteria[all_criteria > 0.0]).min()\n",
    "        prune_engine.max_criteria_value = (all_criteria[all_criteria > 0.0]).max()\n",
    "        prune_engine.median_criteria_value = np.median(all_criteria[all_criteria > 0.0])\n",
    "\n",
    "        prune_engine.min_max_crit_stats =list()\n",
    "        for layer_id, layer in enumerate(unique_groups):\n",
    "            criterias_group = np.asarray([group[1] for group in layer])\n",
    "            min_c = criterias_group[criterias_group>0.0].min()\n",
    "            max_c = criterias_group[criterias_group>0.0].max()\n",
    "            mean_c = criterias_group[criterias_group>0.0].mean()\n",
    "            prune_engine.min_max_crit_stats.append({\"min\": min_c, \"max\": max_c, \"mean_c\": mean_c})\n",
    "\n",
    "    except:\n",
    "        prune_engine.min_criteria_value = 0.0\n",
    "        prune_engine.max_criteria_value = 0.0\n",
    "        prune_engine.median_criteria_value = 0.0\n",
    "\n",
    "    #get overlap\n",
    "    prune_engine.overlap_score = prune_engine.compute_mask_overlap(old_mask, prune_engine.pruning_gates)\n",
    "\n",
    "    # set result to successful\n",
    "    prune_engine.res_pruning = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb, (head, qk, v, mlp) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/workspace/alex/NViT/nvit/pre_compute_saliency.pkl\", 'rb') as f:\n",
    "    prune_engine = pickle.load(f)\n",
    "    prune_engine.group_criteria = group_criteria\n",
    "# model_dim = compute_saliency()\n",
    "groups, unique_groups, prune_network_criteria_updated = compute_saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([14, 16, 34, 43,  2, 29,  5,  0]), 1.115016083019782e-06],\n",
       " [array([46, 26, 28, 36,  9, 58, 38, 12]), 1.8328444753024087e-06],\n",
       " [array([11, 18, 45, 15, 47, 63, 30, 27]), 2.2001161426032922e-06],\n",
       " [array([57, 62, 41, 44, 33, 42, 35, 31]), 2.8756964809417696e-06],\n",
       " [array([61, 56, 10, 54,  6,  1, 53, 59]), 3.7230004181765253e-06],\n",
       " [array([ 7, 22, 17, 19, 40, 32, 50, 37]), 5.148238471974764e-06],\n",
       " [array([ 8, 55, 25, 48,  3, 21, 13, 49]), 8.756599527259823e-06],\n",
       " [array([ 4, 51, 52, 60, 23, 20, 24, 39]), 1.99494749040241e-05]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_groups[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_importance = np.min(all_criteria)\n",
    "offset = np.abs(min_importance) + 1e-8\n",
    "IMPORTANCE_SCALE = 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = 256\n",
    "# NUM_TOKENS = 198\n",
    "# WARMUP = 20\n",
    "# TOTAL = 35\n",
    "BS = 576\n",
    "NUM_TOKENS = 198\n",
    "WARMUP = 20\n",
    "TOTAL = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = f\"mlp_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "# with open(save_name, 'rb') as f:\n",
    "#     mlp_lut = pickle.load(f)\n",
    "#     mlp_lut = mlp_lut[1:, 1:]\n",
    "# save_name = f\"qk_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "# with open(save_name, 'rb') as f:\n",
    "#     qk_lut = pickle.load(f)\n",
    "#     qk_lut = qk_lut[1:, 1:, 1:]\n",
    "# save_name = f\"vandproj_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "# with open(save_name, 'rb') as f:\n",
    "#     vandproj_lut = pickle.load(f)\n",
    "#     vandproj_lut = vandproj_lut[1:, 1:, 1:]\n",
    "save_name = f\"new_mlp_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    mlp_lut = pickle.load(f)\n",
    "    mlp_lut = mlp_lut[1:, 1:]\n",
    "save_name = f\"new_qk_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    qk_lut = pickle.load(f)\n",
    "    qk_lut = qk_lut[1:, 1:, 1:]\n",
    "save_name = f\"new_vandproj_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    vandproj_lut = pickle.load(f)\n",
    "    vandproj_lut = vandproj_lut[1:, 1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pyomo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pyomo.environ import *\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "# all_variable_specs = {\n",
    "#     \"EMB\": [768, 16, 768//16+1],\n",
    "#     \"HEAD\": [12, 1, 12//1+1],\n",
    "#     \"QK\": [64, 2, 64//2+1],\n",
    "#     \"V\": [64, 2, 64//2+1],\n",
    "#     \"MLP\": [3072, 32, 3072//32+1],\n",
    "# }\n",
    "\n",
    "EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "# all_variable_specs = {\n",
    "#     \"EMB\": [768, 16, 768//16],\n",
    "#     \"HEAD\": [12, 1, 12//1],\n",
    "#     \"QK\": [64, 2, 64//2],\n",
    "#     \"V\": [64, 2, 64//2],\n",
    "#     \"MLP\": [3072, 32, 3072//32],\n",
    "# }\n",
    "all_variable_specs = {\n",
    "    \"EMB\": [768, 16, 768//16],\n",
    "    \"HEAD\": [12, 2, 12//2],\n",
    "    \"QK\": [64, 8, 64//8],\n",
    "    \"V\": [64, 8, 64//8],\n",
    "    \"MLP\": [3072, 16, 3072//16],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "importance_dict = {}\n",
    "NAMES = [\"HEAD\", \"QK\", \"V\", \"MLP\"]\n",
    "for layer_idx, layer_importance in enumerate(unique_groups):\n",
    "    block_idx = (layer_idx - 1) // 4\n",
    "    layer_name = \"EMB\" if layer_idx == 0 else f\"{block_idx}_{NAMES[(layer_idx-1) % 4]}\"\n",
    "    running_total_importance = 0\n",
    "    running_total_indices = []\n",
    "    importance_dict[layer_name] = {\"importance\":[], \"indices\":[]}\n",
    "    for x in reversed(layer_importance):\n",
    "        group_indices, group_importance = x\n",
    "#         transformed_group_importance = (group_importance + offset) * IMPORTANCE_SCALE\n",
    "#         transformed_group_importance = int((group_importance + offset) * IMPORTANCE_SCALE)\n",
    "        transformed_group_importance = group_importance * IMPORTANCE_SCALE\n",
    "        \n",
    "        running_total_importance += transformed_group_importance\n",
    "        running_total_indices += list(group_indices)\n",
    "        \n",
    "        importance_dict[layer_name][\"importance\"].append(running_total_importance)\n",
    "        importance_dict[layer_name][\"indices\"].append(deepcopy(running_total_indices))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EMB', '0_HEAD', '0_QK', '0_V', '0_MLP', '1_HEAD', '1_QK', '1_V', '1_MLP', '2_HEAD', '2_QK', '2_V', '2_MLP', '3_HEAD', '3_QK', '3_V', '3_MLP', '4_HEAD', '4_QK', '4_V', '4_MLP', '5_HEAD', '5_QK', '5_V', '5_MLP', '6_HEAD', '6_QK', '6_V', '6_MLP', '7_HEAD', '7_QK', '7_V', '7_MLP', '8_HEAD', '8_QK', '8_V', '8_MLP', '9_HEAD', '9_QK', '9_V', '9_MLP', '10_HEAD', '10_QK', '10_V', '10_MLP', '11_HEAD', '11_QK', '11_V', '11_MLP'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_NUMBER = 12\n",
    "# BLOCK_NUMBER = 1\n",
    "total_latency = (mlp_lut[-1, -1] + vandproj_lut[-1, -1, -1] + qk_lut[-1, -1, -1]) * BLOCK_NUMBER\n",
    "target_latency = total_latency * 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConcreteModel()\n",
    "# Define variables\n",
    "variable_slices_by_type = {}\n",
    "counter = 0\n",
    "variable_slices_by_type[\"EMB\"] = (counter, counter+all_variable_specs[\"EMB\"][2])\n",
    "counter += all_variable_specs[\"EMB\"][2]\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        variable_slices_by_type[f\"{block_idx}_{var_type}\"] = (counter, counter+all_variable_specs[var_type][2])\n",
    "        counter += all_variable_specs[var_type][2]\n",
    "\n",
    "all_items = list(range(counter))\n",
    "model.decision_vars = Var(all_items, domain=Binary)\n",
    "\n",
    "# Define importance and uniqueness constraints\n",
    "importance = 0\n",
    "model.group_unique_constraint = ConstraintList()\n",
    "\n",
    "def add_uniqueness_constraint_and_importance_expr(layer_name):\n",
    "    # uniqueness constraint\n",
    "    # only selecting one configuration\n",
    "    cur_slices = variable_slices_by_type[layer_name]\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in range(cur_slices[0], cur_slices[1])]\n",
    "    model.group_unique_constraint.add(sum(cur_decision_vars) == 1)\n",
    "    # get importance expr\n",
    "    cur_importance = importance_dict[layer_name][\"importance\"]\n",
    "    cur_importance_expr = sum(cur_decision_vars[i] * cur_importance[i] for i in range(len(cur_decision_vars)))\n",
    "    return cur_importance_expr\n",
    "\n",
    "cur_importance_expr = add_uniqueness_constraint_and_importance_expr(\"EMB\")\n",
    "importance += cur_importance_expr\n",
    "\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        cur_importance_expr = add_uniqueness_constraint_and_importance_expr(f\"{block_idx}_{var_type}\")\n",
    "        importance += cur_importance_expr\n",
    "\n",
    "model.obj = Objective(expr=importance, sense=maximize)\n",
    "\n",
    "# Define latency constraint\n",
    "# Add latency constraint\n",
    "latency_expr = 0\n",
    "emb_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1])])\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    head_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_HEAD\"][0], variable_slices_by_type[f\"{block_idx}_HEAD\"][1])])\n",
    "    qk_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_QK\"][0], variable_slices_by_type[f\"{block_idx}_QK\"][1])])\n",
    "    v_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_V\"][0], variable_slices_by_type[f\"{block_idx}_V\"][1])])\n",
    "    mlp_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_MLP\"][0], variable_slices_by_type[f\"{block_idx}_MLP\"][1])])\n",
    "    \n",
    "    T1 = np.tensordot(emb_vectors, mlp_vectors, axes=0)\n",
    "    latency_expr_mlp = np.sum(T1 * mlp_lut)\n",
    "    T2 = np.tensordot(head_vectors, np.tensordot(emb_vectors, v_vectors, axes=0), axes=0)\n",
    "    latency_expr_vandproj = np.sum(T2 * vandproj_lut)\n",
    "    T3 = np.tensordot(head_vectors, np.tensordot(emb_vectors, qk_vectors, axes=0), axes=0)\n",
    "    latency_expr_qk = np.sum(T3 * qk_lut)\n",
    "    latency_expr += latency_expr_mlp + latency_expr_vandproj + latency_expr_qk\n",
    "\n",
    "model.latency_constraint = Constraint(expr=latency_expr <= target_latency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "              Mixed-Integer Nonlinear Decomposition Toolbox in Pyomo (MindtPy)               \n",
      "---------------------------------------------------------------------------------------------\n",
      "For more information, please visit https://pyomo.readthedocs.io/en/stable/contributed_packages/mindtpy.html\n",
      "Original model has 50 constraints (1 nonlinear) and 0 disjunctions, with 2616 variables, of which 2616 are binary, 0 are integer, and 0 are continuous.\n",
      "Moving objective to constraint set.\n",
      "FP is the initial strategy being used.\n",
      "\n",
      " ===============================================================================================\n",
      " Iteration | Subproblem Type | Objective Value | Primal Bound |   Dual Bound |   Gap   | Time(s)\n",
      "\n",
      "         -       Relaxed NLP           7176.28           -inf        7176.28      nan%     56.15\n",
      "         1            FP-MIP          0.323746           -inf        7176.28      nan%     71.89\n",
      "         1            FP-NLP       7.42324e-06           -inf        7176.28      nan%     97.29\n",
      "*        1         Fixed NLP           7174.46        7174.46        7176.28     0.03%    130.88\n",
      "FP-MIP infeasible\n",
      "         1              MILP           7175.56        7174.46        7175.56     0.02%    136.29\n",
      "MindtPy exiting on bound convergence. |Primal Bound: 7174.457453954064 - Dual Bound: 7175.56194358592| / (1e-10 + |Primal Bound|:7174.457453954064) <= relative tolerance: 0.001\n",
      "Solve the main problem without the last no_good cut to fix the bound.zero_tolerance is set to 1E-4\n",
      "*        2         Fixed NLP           7175.56        7175.56        7175.56     0.00%    171.34\n",
      "Fixed bound values: Primal Bound: 7175.56201533903  Dual Bound: 7175.56194358592\n",
      " ===============================================================================================\n",
      " Primal integral          :   124.1342 \n",
      " Dual integral            :   40.4225 \n",
      " Primal-dual gap integral :   164.5568 \n"
     ]
    }
   ],
   "source": [
    "solver = SolverFactory('mindtpy')\n",
    "# solver = SolverFactorypyomopyo('glpk')\n",
    "# solver.solve(model)\n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, solver_tee=True, time_limit=1800) \n",
    "results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, time_limit=1800) \n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt') \n",
    "# results = solver.solve(model) \n",
    "# results = solver.solve(model, mip_solver='glpk', nlp_solver='ipopt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMB': [768, 16, 48],\n",
       " 'HEAD': [12, 2, 6],\n",
       " 'QK': [64, 8, 8],\n",
       " 'V': [64, 8, 8],\n",
       " 'MLP': [3072, 16, 192]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variable_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 23 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n",
      "HEAD 5 5\n",
      "QK 7 7\n",
      "V 7 7\n",
      "MLP 191 191\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
