{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_allow_trim(dict):\n",
    "    res = False\n",
    "    if \"allow_trim\" in dict.keys():\n",
    "        if dict[\"allow_trim\"]:\n",
    "            res = True\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_criteria(list_criteria_per_layer, layers_group, group_size=1):\n",
    "    '''\n",
    "    Function combine criteria per neuron into groups of size group_size.\n",
    "    Output is a list of groups organized by layers. Length of output is a number of layers.\n",
    "    The criterion for the group is computed as an average of member's criteria.\n",
    "    Input:\n",
    "    list_criteria_per_layer - list of criteria per neuron organized per layer\n",
    "    group_size - number of neurons per group\n",
    "    layers_group - layers can form a group, e.g. residual connection, they will be pruned together\n",
    "\n",
    "    Output:\n",
    "    groups - groups organized per layer. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    groups_unique - groups organized per UNIQUE layers only. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    '''\n",
    "    assert len(list_criteria_per_layer) == len(layers_group)\n",
    "    groups = list()\n",
    "\n",
    "    for layer_indx, layer_criteria in enumerate(list_criteria_per_layer):\n",
    "        layer = layer_criteria\n",
    "\n",
    "        if layer_indx == 0:\n",
    "            group_size = 16\n",
    "        else:\n",
    "            if layer_indx%4 == 1:\n",
    "                group_size = 1\n",
    "            elif layer_indx%4 == 2:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 3:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 0:\n",
    "                group_size = 32\n",
    "\n",
    "        if layers_group[layer_indx] != -1:\n",
    "\n",
    "            #if layer/parameter is a part of the group\n",
    "            #then we aggregate importance across the group\n",
    "            #this procedure is repeated for each layer/parameter in the group\n",
    "\n",
    "            all_criteria = np.asarray([lc for li, lc in enumerate(list_criteria_per_layer) if layers_group[layer_indx]==layers_group[li]])\n",
    "\n",
    "            all_criteria = all_criteria.sum(0)\n",
    "            layer = all_criteria\n",
    "\n",
    "        groups_in_layer = list()\n",
    "        indeces = np.argsort(layer)\n",
    "        for group_id in range(int(np.ceil(len(layer)/group_size))):\n",
    "            current_group = slice(group_id*group_size, min((group_id+1)*group_size, len(layer)))\n",
    "            values = [layer[ind] for ind in indeces[current_group]]\n",
    "            group = [indeces[current_group], sum(values)]\n",
    "\n",
    "            groups_in_layer.append(group)\n",
    "        groups.append(groups_in_layer)\n",
    "\n",
    "    if all( [l==-1 for l in layers_group]):\n",
    "        #group with index -1 means no group\n",
    "        unique_groups = groups\n",
    "    else:\n",
    "        unique_groups = list()\n",
    "        groups_exists = list()\n",
    "        for gi, g in enumerate(groups):\n",
    "            if (layers_group[gi] == -1) or (layers_group[gi] not in groups_exists):\n",
    "                unique_groups.append(g)\n",
    "                groups_exists.append(layers_group[gi])\n",
    "\n",
    "    # if torch.distributed.get_rank() == 0:\n",
    "    #     import pdb;pdb.set_trace()\n",
    "\n",
    "    return groups, unique_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency():\n",
    "    def write_to_debug(what_write_name, what_write_value):\n",
    "        # Aux function to store information in the text file\n",
    "        with open(prune_engine.log_debug, 'a') as f:\n",
    "            f.write(\"{} {}\\n\".format(what_write_name,what_write_value))\n",
    "\n",
    "    def nothing(what_write_name, what_write_value):\n",
    "        pass\n",
    "\n",
    "    #store the mask for future needs\n",
    "    old_mask = copy.deepcopy(prune_engine.pruning_gates)\n",
    "\n",
    "    \n",
    "    write_to_debug = nothing\n",
    "\n",
    "    # compute loss since the last pruning and decide if to prune:\n",
    "    if prune_engine.util_loss_tracker_num > 0:\n",
    "        # validation_error = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        validation_loss = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        # validation_error_long = validation_error\n",
    "        acc = prune_engine.util_acc_tracker / prune_engine.util_loss_tracker_num\n",
    "    else:\n",
    "        print(\"compute loss and run prune_engine.util_add_loss(loss.item()) before running this\")\n",
    "        validation_error = 0.0\n",
    "        acc = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "    prune_engine.util_training_loss = validation_loss\n",
    "    prune_engine.util_training_acc = acc\n",
    "\n",
    "    # reset training loss tracker\n",
    "    prune_engine.util_loss_tracker = 0.0\n",
    "    prune_engine.util_acc_tracker = 0.0\n",
    "    prune_engine.util_loss_tracker_num = 0\n",
    "\n",
    "\n",
    "    if (validation_loss > prune_engine.pruning_threshold) and (prune_engine.pruning_threshold != -1.0):\n",
    "        ## if error is big then skip pruning\n",
    "        print(\"skipping pruning because current loss is: \", validation_loss, \"while limit is set to\", prune_engine.pruning_threshold)\n",
    "        if prune_engine.method != 4:\n",
    "            prune_engine.res_pruning = -1\n",
    "            return -1\n",
    "\n",
    "    if prune_engine.maximum_pruning_iterations <= prune_engine.pruning_iterations_done:\n",
    "        # if reached max number of pruning iterations -> exit\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "    prune_engine.full_list_of_criteria = list()\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "        if prune_engine.iterations_done > 0:\n",
    "            # momentum turned to be useless and even reduces performance\n",
    "            contribution = prune_engine.prune_network_accomulate[\"by_layer\"][layer] / prune_engine.iterations_done\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if len(prune_engine.prune_network_accomulate[\"averaged\"][layer])==0 or not prune_engine.use_momentum or (prune_engine.method in [4, 40, 50, 25]):\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = contribution\n",
    "            else:\n",
    "                # use momentum to accumulate criteria over several pruning iterations:\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = prune_engine.momentum_coeff*prune_engine.prune_network_accomulate[\"averaged\"][layer]+(1.0- prune_engine.momentum_coeff)*contribution\n",
    "\n",
    "            current_layer = prune_engine.prune_network_accomulate[\"averaged\"][layer]\n",
    "            if not (prune_engine.method in [1, 4, 40, 15, 50, 25]):\n",
    "                current_layer = current_layer.cpu().numpy()\n",
    "\n",
    "            if prune_engine.l2_normalization_per_layer:\n",
    "                eps = 1e-8\n",
    "                current_layer = current_layer / (np.linalg.norm(current_layer) + eps)\n",
    "\n",
    "            prune_engine.prune_network_accomulate[\"averaged_cpu\"][layer] = current_layer\n",
    "        else:\n",
    "            print(\"First do some add_criteria iterations\")\n",
    "            return -1\n",
    "\n",
    "        for unit in range(len(prune_engine.parameters[layer])):\n",
    "            criterion_now = current_layer[unit]\n",
    "\n",
    "            # make sure that pruned neurons have 0 criteria\n",
    "            if not prune_engine.push_down:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now * prune_engine.pruning_gates[layer][unit]\n",
    "            else:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "            if prune_engine.method == 50:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "    # count number of neurons\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "    prune_engine.neuron_units = neuron_units\n",
    "    prune_engine.all_neuron_units = all_neuron_units\n",
    "\n",
    "    # store criteria_result into file\n",
    "    if not prune_engine.pruning_silent:\n",
    "\n",
    "        if not torch.distributed.is_initialized() or torch.distributed.get_rank()==0:\n",
    "            import pickle\n",
    "            store_criteria = prune_engine.prune_network_accomulate[\"averaged_cpu\"]\n",
    "            pickle.dump(store_criteria, open(prune_engine.folder_to_write_debug + \"criteria_%04d.pickle\"%prune_engine.pruning_iterations_done, \"wb\"))\n",
    "            if prune_engine.pruning_iterations_done == 0:\n",
    "                pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d.pickle\"%prune_engine.method, \"wb\"))\n",
    "            pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d_final.pickle\"%prune_engine.method, \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "    if not prune_engine.fixed_criteria:\n",
    "        prune_engine.iterations_done = 0\n",
    "\n",
    "    prune_network_criteria_updated = prune_engine.prune_network_criteria\n",
    "\n",
    "    # Compute current model statistic\n",
    "    model_dim = list()\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "        if layer == 0:\n",
    "            model_dim.append(np.nonzero(prune_engine.pruning_gates[layer])[0].size)\n",
    "        else:\n",
    "            if layer%4 == 1:\n",
    "                head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 2:\n",
    "                qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 3:\n",
    "                v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 0:\n",
    "                mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                model_dim.append({'head':head,'QK':qk,'V':v,'MLP':mlp})\n",
    "    \n",
    "    # create groups per layer\n",
    "    # comment later\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "    return groups, unique_groups, prune_network_criteria_updated\n",
    "\n",
    "    # Compute latency and adjust importance\n",
    "    if prune_engine.latency_regularization:\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer==0 and not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                latency_improve = 0.\n",
    "                for blk in range(12):\n",
    "                    qk_head = model_dim[blk+1]['head']\n",
    "                    qk = model_dim[blk+1]['QK']\n",
    "                    v = model_dim[blk+1]['V']\n",
    "                    mlp = model_dim[blk+1]['MLP']\n",
    "                    latency_improve += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb-1,qk_head,qk,v,mlp)\n",
    "                pc = np.array(prune_network_criteria_updated[layer])\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "            elif not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                qk_head = model_dim[(layer-1)//4+1]['head']\n",
    "                qk = model_dim[(layer-1)//4+1]['QK']\n",
    "                v = model_dim[(layer-1)//4+1]['V']\n",
    "                mlp = model_dim[(layer-1)//4+1]['MLP']\n",
    "                latency_improve = 0.\n",
    "                if layer%4 == 1 and qk_head>2:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head-1,qk,v,mlp)\n",
    "                elif layer%4 == 2 and qk>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk-1,v,mlp)\n",
    "                elif layer%4 == 3 and v>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v-1,mlp)\n",
    "                elif layer%4 == 0 and mlp>16:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v,mlp-1)\n",
    "\n",
    "                pc = np.array(prune_network_criteria_updated[layer])#+1\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "\n",
    "\n",
    "    # create groups per layer\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "#     return groups, unique_groups, prune_network_criteria_updated\n",
    "    \n",
    "    # get an array of all criteria from groups\n",
    "    all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)\n",
    "    # from IPython import embed; embed()\n",
    "\n",
    "    prune_neurons_now = (prune_engine.pruning_iterations_done * prune_engine.prune_per_iteration)//prune_engine.group_size - 1\n",
    "    if prune_engine.push_down:\n",
    "        removed_gates = sum([(a==0.0).sum() for a in prune_engine.pruning_gates])\n",
    "        prune_additionally = prune_engine.prune_per_iteration\n",
    "        prune_neurons_now = (removed_gates + prune_additionally) // prune_engine.group_size - 1\n",
    "\n",
    "    if prune_engine.prune_neurons_max != -1:\n",
    "        prune_neurons_now = max(0,min(len(all_criteria)-1, min(prune_neurons_now, prune_engine.prune_neurons_max//prune_engine.group_size - 1)))\n",
    "\n",
    "    if prune_engine.push_down:\n",
    "        prune_engine.reset_gates_to_1()\n",
    "\n",
    "    # adaptively estimate threshold given a number of neurons to be removed\n",
    "#     prune_neurons_now = 100\n",
    "    threshold_now = np.sort(all_criteria)[prune_neurons_now]\n",
    "    print(prune_neurons_now, threshold_now)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if np.isnan(threshold_now):\n",
    "        print(\"skipping\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "    prune_engine.pruning_iterations_done += 1\n",
    "\n",
    "\n",
    "    if prune_engine.pruning_iterations_done < prune_engine.start_pruning_after_n_iterations:\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if prune_engine.prune_per_iteration == 0:\n",
    "            continue\n",
    "\n",
    "        total_groups_in_layer = len(groups[layer])\n",
    "        zeroed_groups = 0\n",
    "\n",
    "        for group in groups[layer]:\n",
    "            if group[1] <= threshold_now:\n",
    "                print(layer, group)\n",
    "                #add skip if all groups are set to zero in the current layer:\n",
    "                if (zeroed_groups >= total_groups_in_layer-1) and prune_engine.leave_at_least_one_group:\n",
    "                    print(\"PRUNING: skipping the group because others are zero\")\n",
    "                    continue\n",
    "\n",
    "                zeroed_groups += 1\n",
    "                for unit in group[0]:\n",
    "                    # do actual pruning\n",
    "                    if prune_engine.leave_at_least_one_group and (prune_engine.pruning_gates[layer].sum()<=1):\n",
    "                        print(\"PRUNING: skipping setting the last neuron to zero\")\n",
    "                        continue\n",
    "\n",
    "                    prune_engine.pruning_gates[layer][unit] *= 0.0\n",
    "\n",
    "\n",
    "                    if not prune_engine.push_down:\n",
    "                        for param in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "\n",
    "                            if check_allow_trim(param):\n",
    "                                in_the_range = unit + param[\"shift\"] < param[\"parameter\"].data.shape[param[\"dim\"]]\n",
    "                                if (not in_the_range) or not(unit + param[\"shift\"] >= 0):\n",
    "                                    continue\n",
    "\n",
    "                            if param[\"dim\"] == 0:\n",
    "                                param[\"parameter\"].data[unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 1:\n",
    "                                param[\"parameter\"].data[:, unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 2:\n",
    "                                param[\"parameter\"].data[:, :, unit + param[\"shift\"]] *= 0.0\n",
    "\n",
    "        write_to_debug(\"pruned_perc:\", [np.nonzero(1.0-prune_engine.pruning_gates[layer])[0].size, len(prune_engine.pruning_gates[layer])])\n",
    "\n",
    "    # count number of neurons\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        model_dim = np.zeros((1,49))\n",
    "        latency = 0.\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer == 0:\n",
    "                model_dim[0,0] = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            else:\n",
    "                if layer%4 == 1:\n",
    "                    qk_head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+1] = qk_head\n",
    "                elif layer%4 == 2:\n",
    "                    qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+13] = qk\n",
    "                elif layer%4 == 3:\n",
    "                    v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+25] = v\n",
    "                elif layer%4 == 0:\n",
    "                    mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+36] = mlp\n",
    "                    latency += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)\n",
    "\n",
    "        prune_engine.current_latency = latency\n",
    "\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "\n",
    "    prune_engine.pruned_neurons = all_neuron_units-neuron_units\n",
    "\n",
    "    if prune_engine.method == 25:\n",
    "        prune_engine.method_25_first_done = True\n",
    "\n",
    "    prune_engine.threshold_now = threshold_now\n",
    "    try:\n",
    "        prune_engine.min_criteria_value = (all_criteria[all_criteria > 0.0]).min()\n",
    "        prune_engine.max_criteria_value = (all_criteria[all_criteria > 0.0]).max()\n",
    "        prune_engine.median_criteria_value = np.median(all_criteria[all_criteria > 0.0])\n",
    "\n",
    "        prune_engine.min_max_crit_stats =list()\n",
    "        for layer_id, layer in enumerate(unique_groups):\n",
    "            criterias_group = np.asarray([group[1] for group in layer])\n",
    "            min_c = criterias_group[criterias_group>0.0].min()\n",
    "            max_c = criterias_group[criterias_group>0.0].max()\n",
    "            mean_c = criterias_group[criterias_group>0.0].mean()\n",
    "            prune_engine.min_max_crit_stats.append({\"min\": min_c, \"max\": max_c, \"mean_c\": mean_c})\n",
    "\n",
    "    except:\n",
    "        prune_engine.min_criteria_value = 0.0\n",
    "        prune_engine.max_criteria_value = 0.0\n",
    "        prune_engine.median_criteria_value = 0.0\n",
    "\n",
    "    #get overlap\n",
    "    prune_engine.overlap_score = prune_engine.compute_mask_overlap(old_mask, prune_engine.pruning_gates)\n",
    "\n",
    "    # set result to successful\n",
    "    prune_engine.res_pruning = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb, (head, qk, v, mlp) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"/workspace/alex/NViT/nvit/pre_compute_saliency.pkl\", 'rb') as f:\n",
    "with open(\"/workspace/alex/NViT/nvit/after_prunes_pre_compute_saliency.pkl\", 'rb') as f:\n",
    "    prune_engine = pickle.load(f)\n",
    "    prune_engine.group_criteria = group_criteria\n",
    "# model_dim = compute_saliency()\n",
    "groups, unique_groups, prune_network_criteria_updated = compute_saliency()\n",
    "# compute_saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = [\"HEAD\", \"QK\", \"V\", \"MLP\"]\n",
    "active_group_size = {}\n",
    "for layer_idx in range(len(unique_groups)):\n",
    "    layer_groups = unique_groups[layer_idx]\n",
    "    block_idx = (layer_idx - 1) // 4\n",
    "    layer_name = \"EMB\" if layer_idx == 0 else f\"{block_idx}_{NAMES[(layer_idx-1) % 4]}\"\n",
    "    kept_groups = [x for x in layer_groups if x[1] > 0]\n",
    "    active_group_size[layer_name] = len(kept_groups)\n",
    "    unique_groups[layer_idx] = kept_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_importance = np.min(all_criteria)\n",
    "offset = np.abs(min_importance) + 1e-8\n",
    "IMPORTANCE_SCALE = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTIVE_IMPORTANCE_SCALE = 1/min_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6707554806316693e-07, 13598804.202148438)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(all_criteria), np.max(all_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5985316.293093504"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADAPTIVE_IMPORTANCE_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 256\n",
    "NUM_TOKENS = 198\n",
    "WARMUP = 20\n",
    "TOTAL = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f\"mlp_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    mlp_lut = pickle.load(f)\n",
    "    full_mlp_lut = mlp_lut[1:, 1:]\n",
    "save_name = f\"qk_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    qk_lut = pickle.load(f)\n",
    "    full_qk_lut = qk_lut[1:, 1:, 1:]\n",
    "save_name = f\"vandproj_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    vandproj_lut = pickle.load(f)\n",
    "    full_vandproj_lut = vandproj_lut[1:, 1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pyomo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pyomo.environ import *\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "# all_variable_specs = {\n",
    "#     \"EMB\": [768, 16, 768//16+1],\n",
    "#     \"HEAD\": [12, 1, 12//1+1],\n",
    "#     \"QK\": [64, 2, 64//2+1],\n",
    "#     \"V\": [64, 2, 64//2+1],\n",
    "#     \"MLP\": [3072, 32, 3072//32+1],\n",
    "# }\n",
    "\n",
    "EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "all_variable_specs = {\n",
    "    \"EMB\": [768, 16, 768//16],\n",
    "    \"HEAD\": [12, 1, 12//1],\n",
    "    \"QK\": [64, 2, 64//2],\n",
    "    \"V\": [64, 2, 64//2],\n",
    "    \"MLP\": [3072, 32, 3072//32],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, group in enumerate(unique_groups):\n",
    "    vals = [x[1] for x in group]\n",
    "    if 0 in vals:\n",
    "        print(i)\n",
    "        print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_importance(val, min_val = None, max_val = None, scale=None):\n",
    "#     return val\n",
    "    return val/scale\n",
    "#     return np.log(1+val)\n",
    "#     return (val - float(min_val)) / (float(max_val) - float(min_val))\n",
    "#     return int(val * (1 / min_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "importance_dict = {}\n",
    "for layer_idx, layer_importance in enumerate(unique_groups):\n",
    "    block_idx = (layer_idx - 1) // 4\n",
    "    layer_name = \"EMB\" if layer_idx == 0 else f\"{block_idx}_{NAMES[(layer_idx-1) % 4]}\"\n",
    "    running_total_importance = 0\n",
    "    running_total_indices = []\n",
    "    importance_dict[layer_name] = {\"importance\":[], \"indices\":[]}\n",
    "    for x in reversed(layer_importance):\n",
    "        group_indices, group_importance = x\n",
    "#         transformed_group_importance = (group_importance + offset) * IMPORTANCE_SCALE\n",
    "#         transformed_group_importance = int((group_importance + offset) * IMPORTANCE_SCALE)\n",
    "#         transformed_group_importance = group_importance / IMPORTANCE_SCALE\n",
    "#         transformed_group_importance = transform_importance(group_importance)\n",
    "#         transformed_group_importance = transform_importance(group_importance, np.min(all_criteria), np.max(all_criteria))\n",
    "#         transformed_group_importance = transform_importance(group_importance, np.min(all_criteria))\n",
    "        transformed_group_importance = transform_importance(group_importance, scale=100)\n",
    "        \n",
    "        running_total_importance += transformed_group_importance\n",
    "        running_total_indices += list(group_indices)\n",
    "        \n",
    "        importance_dict[layer_name][\"importance\"].append(running_total_importance)\n",
    "        importance_dict[layer_name][\"indices\"].append(deepcopy(running_total_indices))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EMB', '0_HEAD', '0_QK', '0_V', '0_MLP', '1_HEAD', '1_QK', '1_V', '1_MLP', '2_HEAD', '2_QK', '2_V', '2_MLP', '3_HEAD', '3_QK', '3_V', '3_MLP', '4_HEAD', '4_QK', '4_V', '4_MLP', '5_HEAD', '5_QK', '5_V', '5_MLP', '6_HEAD', '6_QK', '6_V', '6_MLP', '7_HEAD', '7_QK', '7_V', '7_MLP', '8_HEAD', '8_QK', '8_V', '8_MLP', '9_HEAD', '9_QK', '9_V', '9_MLP', '10_HEAD', '10_QK', '10_V', '10_MLP', '11_HEAD', '11_QK', '11_V', '11_MLP'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_NUMBER = 12\n",
    "# BLOCK_NUMBER = 1\n",
    "total_latency = (mlp_lut[-1, -1] + vandproj_lut[-1, -1, -1] + qk_lut[-1, -1, -1]) * BLOCK_NUMBER\n",
    "# target_latency = total_latency * 0.61\n",
    "target_latency = total_latency * 0.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ConcreteModel()\n",
    "# # Define variables\n",
    "# variable_slices_by_type = {}\n",
    "# counter = 0\n",
    "# variable_slices_by_type[\"EMB\"] = (counter, counter+all_variable_specs[\"EMB\"][2])\n",
    "# counter += all_variable_specs[\"EMB\"][2]\n",
    "# for block_idx in range(BLOCK_NUMBER):\n",
    "#     for var_type, var_spec in all_variable_specs.items():\n",
    "#         if var_type == \"EMB\":\n",
    "#             continue\n",
    "#         variable_slices_by_type[f\"{block_idx}_{var_type}\"] = (counter, counter+all_variable_specs[var_type][2])\n",
    "#         counter += all_variable_specs[var_type][2]\n",
    "\n",
    "# all_items = list(range(counter))\n",
    "# model.decision_vars = Var(all_items, domain=Binary)\n",
    "\n",
    "# # Define importance and uniqueness constraints\n",
    "# importance = 0\n",
    "# model.group_unique_constraint = ConstraintList()\n",
    "# model.no_single_layer_prune = ConstraintList()\n",
    "\n",
    "# def add_uniqueness_constraint_and_importance_expr(layer_name):\n",
    "#     # uniqueness constraint\n",
    "#     # only selecting one configuration\n",
    "#     cur_slices = variable_slices_by_type[layer_name]\n",
    "#     cur_decision_vars = [model.decision_vars[k] for k in range(cur_slices[0], cur_slices[1])]\n",
    "#     model.group_unique_constraint.add(sum(cur_decision_vars) == 1)\n",
    "#     # can't prune all of this layer\n",
    "#     model.no_single_layer_prune.add(cur_decision_vars[0] == 0)\n",
    "#     # get importance expr\n",
    "#     cur_importance = importance_dict[layer_name][\"importance\"]\n",
    "#     cur_importance_expr = sum(cur_decision_vars[i] * cur_importance[i] for i in range(len(cur_decision_vars)))\n",
    "#     return cur_importance_expr\n",
    "\n",
    "# cur_importance_expr = add_uniqueness_constraint_and_importance_expr(\"EMB\")\n",
    "# importance += cur_importance_expr\n",
    "\n",
    "# for block_idx in range(BLOCK_NUMBER):\n",
    "#     for var_type, var_spec in all_variable_specs.items():\n",
    "#         if var_type == \"EMB\":\n",
    "#             continue\n",
    "#         cur_importance_expr = add_uniqueness_constraint_and_importance_expr(f\"{block_idx}_{var_type}\")\n",
    "#         importance += cur_importance_expr\n",
    "\n",
    "# model.obj = Objective(expr=importance, sense=maximize)\n",
    "\n",
    "# # Define latency constraint\n",
    "# # Add latency constraint\n",
    "# latency_expr = 0\n",
    "# emb_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1])])\n",
    "# for block_idx in range(BLOCK_NUMBER):\n",
    "#     head_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_HEAD\"][0], variable_slices_by_type[f\"{block_idx}_HEAD\"][1])])\n",
    "#     qk_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_QK\"][0], variable_slices_by_type[f\"{block_idx}_QK\"][1])])\n",
    "#     v_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_V\"][0], variable_slices_by_type[f\"{block_idx}_V\"][1])])\n",
    "#     mlp_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_MLP\"][0], variable_slices_by_type[f\"{block_idx}_MLP\"][1])])\n",
    "    \n",
    "#     T1 = np.tensordot(emb_vectors, mlp_vectors, axes=0)\n",
    "#     latency_expr_mlp = np.sum(T1 * mlp_lut)\n",
    "#     T2 = np.tensordot(head_vectors, np.tensordot(emb_vectors, v_vectors, axes=0), axes=0)\n",
    "#     latency_expr_vandproj = np.sum(T2 * vandproj_lut)\n",
    "#     T3 = np.tensordot(head_vectors, np.tensordot(emb_vectors, qk_vectors, axes=0), axes=0)\n",
    "#     latency_expr_qk = np.sum(T3 * qk_lut)\n",
    "#     latency_expr += latency_expr_mlp + latency_expr_vandproj + latency_expr_qk\n",
    "\n",
    "# model.latency_constraint = Constraint(expr=latency_expr <= target_latency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the active group size into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAMES = [\"HEAD\", \"QK\", \"V\", \"MLP\"]\n",
    "# active_group_size = {}\n",
    "# for layer_idx in range(len(unique_groups)):\n",
    "#     layer_groups = unique_groups[layer_idx]\n",
    "#     block_idx = (layer_idx - 1) // 4\n",
    "#     layer_name = \"EMB\" if layer_idx == 0 else f\"{block_idx}_{NAMES[(layer_idx-1) % 4]}\"\n",
    "#     kept_groups = [x for x in layer_groups if x[1] > 0]\n",
    "#     active_group_size[layer_name] = len(kept_groups)\n",
    "#     unique_groups[layer_idx] = kept_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EMB', '0_HEAD', '0_QK', '0_V', '0_MLP', '1_HEAD', '1_QK', '1_V', '1_MLP', '2_HEAD', '2_QK', '2_V', '2_MLP', '3_HEAD', '3_QK', '3_V', '3_MLP', '4_HEAD', '4_QK', '4_V', '4_MLP', '5_HEAD', '5_QK', '5_V', '5_MLP', '6_HEAD', '6_QK', '6_V', '6_MLP', '7_HEAD', '7_QK', '7_V', '7_MLP', '8_HEAD', '8_QK', '8_V', '8_MLP', '9_HEAD', '9_QK', '9_V', '9_MLP', '10_HEAD', '10_QK', '10_V', '10_MLP', '11_HEAD', '11_QK', '11_V', '11_MLP'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_group_size.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = [\"HEAD\", \"QK\", \"V\", \"MLP\"]\n",
    "model = ConcreteModel()\n",
    "# Define variables\n",
    "variable_slices_by_type = {}\n",
    "counter = 0\n",
    "\n",
    "for layer_name in active_group_size:  \n",
    "    variable_slices_by_type[layer_name] = (counter, counter+active_group_size[layer_name])\n",
    "    counter += active_group_size[layer_name]\n",
    "\n",
    "all_items = list(range(counter))\n",
    "model.decision_vars = Var(all_items, domain=Binary)\n",
    "\n",
    "# Define importance and uniqueness constraints\n",
    "importance = 0\n",
    "model.group_unique_constraint = ConstraintList()\n",
    "# model.no_single_layer_prune = ConstraintList()\n",
    "\n",
    "def add_uniqueness_constraint_and_importance_expr(layer_name):\n",
    "    cur_slices = variable_slices_by_type[layer_name]\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in range(cur_slices[0], cur_slices[1])]\n",
    "    # uniqueness constraint\n",
    "    # only selecting one configuration\n",
    "#     model.group_unique_constraint.add(sum(cur_decision_vars) == 1)\n",
    "    model.group_unique_constraint.add(sum(cur_decision_vars) >= 1)\n",
    "    model.group_unique_constraint.add(sum(cur_decision_vars) <= 1)\n",
    "    # can't prune all of this layer\n",
    "#     model.no_single_layer_prune.add(cur_decision_vars[0] == 0)\n",
    "#     model.no_single_layer_prune.add(cur_decision_vars[0] <= 0)\n",
    "    # get importance expr\n",
    "    cur_importance = importance_dict[layer_name][\"importance\"]\n",
    "    cur_importance_expr = sum(cur_decision_vars[i] * cur_importance[i] for i in range(len(cur_decision_vars)))\n",
    "    return cur_importance_expr\n",
    "\n",
    "for layer_name in active_group_size:\n",
    "    cur_importance_expr = add_uniqueness_constraint_and_importance_expr(layer_name)\n",
    "    importance += cur_importance_expr   \n",
    "\n",
    "model.obj = Objective(expr=importance, sense=maximize)\n",
    "\n",
    "# Define latency constraint\n",
    "# Add latency constraint\n",
    "latency_expr = 0\n",
    "emb_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1])])\n",
    "active_emb_group_size = active_group_size[\"EMB\"]\n",
    "\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    cur_head_name = f\"{block_idx}_HEAD\"\n",
    "    cur_qk_name = f\"{block_idx}_QK\"\n",
    "    cur_v_name = f\"{block_idx}_V\"\n",
    "    cur_mlp_name = f\"{block_idx}_MLP\"\n",
    "    \n",
    "    active_head_group_size = active_group_size[cur_head_name]\n",
    "    active_qk_group_size = active_group_size[cur_qk_name]\n",
    "    active_v_group_size = active_group_size[cur_v_name]\n",
    "    active_mlp_group_size = active_group_size[cur_mlp_name]\n",
    "    \n",
    "    head_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[cur_head_name][0], variable_slices_by_type[cur_head_name][1])])\n",
    "    qk_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[cur_qk_name][0], variable_slices_by_type[cur_qk_name][1])])\n",
    "    v_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[cur_v_name][0], variable_slices_by_type[cur_v_name][1])])\n",
    "    mlp_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[cur_mlp_name][0], variable_slices_by_type[cur_mlp_name][1])])\n",
    "    \n",
    "    mlp_lut = full_mlp_lut[:active_emb_group_size, :active_mlp_group_size]\n",
    "    vandproj_lut = full_vandproj_lut[:active_head_group_size, :active_emb_group_size, :active_v_group_size]\n",
    "    qk_lut = full_qk_lut[:active_head_group_size, :active_emb_group_size, :active_qk_group_size]\n",
    "    \n",
    "    T1 = np.tensordot(emb_vectors, mlp_vectors, axes=0)\n",
    "    latency_expr_mlp = np.sum(T1 * mlp_lut)\n",
    "    T2 = np.tensordot(head_vectors, np.tensordot(emb_vectors, v_vectors, axes=0), axes=0)\n",
    "    latency_expr_vandproj = np.sum(T2 * vandproj_lut)\n",
    "    T3 = np.tensordot(head_vectors, np.tensordot(emb_vectors, qk_vectors, axes=0), axes=0)\n",
    "    latency_expr_qk = np.sum(T3 * qk_lut)\n",
    "    latency_expr += latency_expr_mlp + latency_expr_vandproj + latency_expr_qk\n",
    "\n",
    "model.latency_constraint = Constraint(expr=latency_expr <= target_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "              Mixed-Integer Nonlinear Decomposition Toolbox in Pyomo (MindtPy)               \n",
      "---------------------------------------------------------------------------------------------\n",
      "For more information, please visit https://pyomo.readthedocs.io/en/stable/contributed_packages/mindtpy.html\n",
      "Original model has 99 constraints (1 nonlinear) and 0 disjunctions, with 2103 variables, of which 2103 are binary, 0 are integer, and 0 are continuous.\n",
      "Moving objective to constraint set.\n",
      "FP is the initial strategy being used.\n",
      "\n",
      " ===============================================================================================\n",
      " Iteration | Subproblem Type | Objective Value | Primal Bound |   Dual Bound |   Gap   | Time(s)\n",
      "\n",
      "         -       Relaxed NLP            584824           -inf         584824      nan%    193.79\n",
      "         1            FP-MIP           1.51047           -inf         584824      nan%    255.22\n",
      "         1            FP-NLP       7.08404e-06           -inf         584824      nan%    333.48\n",
      "*        1         Fixed NLP            584822         584822         584824     0.00%    437.96\n",
      "FP-MIP infeasible\n",
      "         1              MILP            584824         584822         584824     0.00%    451.97\n",
      "MindtPy exiting on bound convergence. |Primal Bound: 584822.4647853578 - Dual Bound: 584824.158464873| / (1e-10 + |Primal Bound|:584822.4647853578) <= relative tolerance: 0.001\n",
      "Solve the main problem without the last no_good cut to fix the bound.zero_tolerance is set to 1E-4\n",
      "*        2         Fixed NLP            584824         584824         584824     0.00%    554.92\n",
      "Fixed bound values: Primal Bound: 584824.1643131091  Dual Bound: 584824.158464873\n",
      " ===============================================================================================\n",
      " Primal integral          :   647.0700 \n",
      " Dual integral            :   43.7540 \n",
      " Primal-dual gap integral :   690.8240 \n"
     ]
    }
   ],
   "source": [
    "solver = SolverFactory('mindtpy')\n",
    "# solver = SolverFactorypyomopyo('glpk')\n",
    "# solver.solve(model)\n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, solver_tee=True, time_limit=1800) \n",
    "results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, time_limit=1800) \n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt') \n",
    "# results = solver.solve(model) \n",
    "# results = solver.solve(model, mip_solver='glpk', nlp_solver='ipopt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Problem': [{'Name': 'unknown', 'Lower bound': 584824.1643131091, 'Upper bound': 584824.158464873, 'Number of objectives': 1, 'Number of constraints': 99, 'Number of variables': 2103, 'Number of binary variables': 2103, 'Number of integer variables': 0, 'Number of continuous variables': 0, 'Number of nonzeros': None, 'Sense': 'maximize', 'Number of disjunctions': 0}], 'Solver': [{'Name': 'MindtPyOA', 'Status': 'ok', 'Message': None, 'User time': None, 'System time': None, 'Wallclock time': None, 'Termination condition': 'optimal', 'Termination message': None, 'Timing': Bunch(Call after main solve = 1.7490005120635033e-05, OA cut generation = 97.43282637721859, fixed subproblem = 0.45826924010179937, fp main = 10.258696942124516, fp subproblem = 41.954691983992234, initialization = 411.8047901650425, main = 1.0614472089800984, main loop = 111.10152304405347, main_timer_start_time = 1316058.744665955, no_good cut generation = 0.28666990622878075, total = 556.0255484280642), 'Iterations': 1, 'Num infeasible nlp subproblem': 0, 'Best solution found time': 528.8288661250845, 'Primal integral': 647.0699571631471, 'Dual integral': 43.75400142186483, 'Primal dual gap integral': 690.823958585012}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMB': [768, 16, 48],\n",
       " 'HEAD': [12, 1, 12],\n",
       " 'QK': [64, 2, 32],\n",
       " 'V': [64, 2, 32],\n",
       " 'MLP': [3072, 32, 96]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variable_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 21 47\n",
      "HEAD 0 11\n",
      "QK 6 31\n",
      "V 5 31\n",
      "MLP 1 95\n",
      "HEAD 0 11\n",
      "QK 0 31\n",
      "V 0 31\n",
      "MLP 55 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 25 31\n",
      "MLP 75 95\n",
      "HEAD 10 11\n",
      "QK 23 31\n",
      "V 25 31\n",
      "MLP 88 95\n",
      "HEAD 7 11\n",
      "QK 25 31\n",
      "V 31 31\n",
      "MLP 91 95\n",
      "HEAD 9 11\n",
      "QK 31 31\n",
      "V 29 31\n",
      "MLP 93 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 8 11\n",
      "QK 29 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 31 31\n",
      "MLP 91 95\n",
      "HEAD 8 11\n",
      "QK 27 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 6 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 93 95\n"
     ]
    }
   ],
   "source": [
    "var_type = \"EMB\"\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 21 47\n",
      "HEAD 1 11\n",
      "QK 2 31\n",
      "V 1 31\n",
      "MLP 1 95\n",
      "HEAD 1 11\n",
      "QK 2 31\n",
      "V 2 31\n",
      "MLP 51 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 25 31\n",
      "MLP 75 95\n",
      "HEAD 10 11\n",
      "QK 23 31\n",
      "V 25 31\n",
      "MLP 88 95\n",
      "HEAD 7 11\n",
      "QK 25 31\n",
      "V 31 31\n",
      "MLP 91 95\n",
      "HEAD 9 11\n",
      "QK 31 31\n",
      "V 29 31\n",
      "MLP 93 95\n",
      "HEAD 10 11\n",
      "QK 29 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 7 11\n",
      "QK 29 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 31 31\n",
      "MLP 91 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 6 11\n",
      "QK 30 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 93 95\n"
     ]
    }
   ],
   "source": [
    "var_type = \"EMB\"\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 14 47\n",
      "HEAD 1 11\n",
      "QK 1 31\n",
      "V 4 31\n",
      "MLP 1 95\n",
      "HEAD 1 11\n",
      "QK 1 31\n",
      "V 4 31\n",
      "MLP 94 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 94 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 29 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n"
     ]
    }
   ],
   "source": [
    "var_type = \"EMB\"\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421.97923698127266\n"
     ]
    }
   ],
   "source": [
    "var_type = \"EMB\"\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "cur_emb = np.argmax(cur_decision_vars_value)\n",
    "latency = 0\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    indices = list(range(variable_slices_by_type[f\"{block_id}_HEAD\"][0], variable_slices_by_type[f\"{block_id}_HEAD\"][1]))\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "    cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "    cur_head = np.argmax(cur_decision_vars_value)\n",
    "    \n",
    "    indices = list(range(variable_slices_by_type[f\"{block_id}_QK\"][0], variable_slices_by_type[f\"{block_id}_QK\"][1]))\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "    cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "    cur_qk = np.argmax(cur_decision_vars_value)\n",
    "    \n",
    "    indices = list(range(variable_slices_by_type[f\"{block_id}_V\"][0], variable_slices_by_type[f\"{block_id}_V\"][1]))\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "    cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "    cur_v = np.argmax(cur_decision_vars_value)\n",
    "    \n",
    "    indices = list(range(variable_slices_by_type[f\"{block_id}_MLP\"][0], variable_slices_by_type[f\"{block_id}_MLP\"][1]))\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "    cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "    cur_mlp = np.argmax(cur_decision_vars_value)\n",
    "    \n",
    "    latency += (full_mlp_lut[cur_emb, cur_mlp] + full_vandproj_lut[cur_head, cur_emb, cur_v] + full_qk_lut[cur_head, cur_emb, cur_qk])\n",
    "\n",
    "print(latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421.9835233955384"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_latency * 0.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prune_engine.prune_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_engine.push_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([143, 335, 751,   8, 296, 458,  70, 451, 119, 739, 128, 284, 151,\n",
       "        17, 372, 657])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['importance', 'indices'])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict[\"EMB\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_dict = {}\n",
    "\n",
    "for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "    if not if_prune:\n",
    "        continue\n",
    "\n",
    "    if prune_engine.prune_per_iteration == 0:\n",
    "        continue\n",
    "\n",
    "    total_groups_in_layer = len(groups[layer])\n",
    "    zeroed_groups = 0\n",
    "    # get the layer_name\n",
    "    block_idx = (layer - 1) // 4\n",
    "    layer_name = \"EMB\" if layer == 0 else f\"{block_idx}_{NAMES[(layer-1) % 4]}\"\n",
    "    indices = list(range(variable_slices_by_type[layer_name][0], variable_slices_by_type[layer_name][1]))\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "    cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "    selected_config = np.argmax(cur_decision_vars_value)\n",
    "    kept_indices = importance_dict[layer_name]['indices'][selected_config]\n",
    "    \n",
    "    for group in groups[layer]:\n",
    "        if group[0][0] not in set(kept_indices):\n",
    "#             print(\"pruned\")\n",
    "            pass\n",
    "#             #add skip if all groups are set to zero in the current layer:\n",
    "#             if (zeroed_groups >= total_groups_in_layer-1) and self.leave_at_least_one_group:\n",
    "#                 print(\"PRUNING: skipping the group because others are zero\")\n",
    "#                 continue\n",
    "\n",
    "#             zeroed_groups += 1\n",
    "#             for unit in group[0]:\n",
    "#                 # do actual pruning\n",
    "#                 if self.leave_at_least_one_group and (self.pruning_gates[layer].sum()<=1):\n",
    "#                     print(\"PRUNING: skipping setting the last neuron to zero\")\n",
    "#                     continue\n",
    "\n",
    "#                 self.pruning_gates[layer][unit] *= 0.0\n",
    "\n",
    "\n",
    "#                 if not self.push_down:\n",
    "#                     for param in self.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "\n",
    "#                         if check_allow_trim(param):\n",
    "#                             in_the_range = unit + param[\"shift\"] < param[\"parameter\"].data.shape[param[\"dim\"]]\n",
    "#                             if (not in_the_range) or not(unit + param[\"shift\"] >= 0):\n",
    "#                                 continue\n",
    "\n",
    "#                         if param[\"dim\"] == 0:\n",
    "#                             param[\"parameter\"].data[unit + param[\"shift\"]] *= 0.0\n",
    "#                         elif param[\"dim\"] == 1:\n",
    "#                             param[\"parameter\"].data[:, unit + param[\"shift\"]] *= 0.0\n",
    "#                         elif param[\"dim\"] == 2:\n",
    "#                             param[\"parameter\"].data[:, :, unit + param[\"shift\"]] *= 0.0\n",
    "        else:\n",
    "            cur_val = kept_dict.get(layer_name, 0)\n",
    "            kept_dict[layer_name] = cur_val + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMB': 22,\n",
       " '0_HEAD': 1,\n",
       " '0_QK': 7,\n",
       " '0_V': 6,\n",
       " '0_MLP': 2,\n",
       " '1_HEAD': 1,\n",
       " '1_QK': 1,\n",
       " '1_V': 1,\n",
       " '1_MLP': 56,\n",
       " '2_HEAD': 8,\n",
       " '2_QK': 28,\n",
       " '2_V': 26,\n",
       " '2_MLP': 76,\n",
       " '3_HEAD': 11,\n",
       " '3_QK': 24,\n",
       " '3_V': 26,\n",
       " '3_MLP': 89,\n",
       " '4_HEAD': 8,\n",
       " '4_QK': 26,\n",
       " '4_V': 32,\n",
       " '4_MLP': 92,\n",
       " '5_HEAD': 10,\n",
       " '5_QK': 32,\n",
       " '5_V': 30,\n",
       " '5_MLP': 94,\n",
       " '6_HEAD': 11,\n",
       " '6_QK': 32,\n",
       " '6_V': 32,\n",
       " '6_MLP': 94,\n",
       " '7_HEAD': 9,\n",
       " '7_QK': 30,\n",
       " '7_V': 32,\n",
       " '7_MLP': 94,\n",
       " '8_HEAD': 8,\n",
       " '8_QK': 28,\n",
       " '8_V': 32,\n",
       " '8_MLP': 92,\n",
       " '9_HEAD': 9,\n",
       " '9_QK': 28,\n",
       " '9_V': 32,\n",
       " '9_MLP': 94,\n",
       " '10_HEAD': 7,\n",
       " '10_QK': 32,\n",
       " '10_V': 32,\n",
       " '10_MLP': 94,\n",
       " '11_HEAD': 12,\n",
       " '11_QK': 32,\n",
       " '11_V': 32,\n",
       " '11_MLP': 94}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 21 47\n",
      "HEAD 1 11\n",
      "QK 2 31\n",
      "V 1 31\n",
      "MLP 1 95\n",
      "HEAD 1 11\n",
      "QK 2 31\n",
      "V 2 31\n",
      "MLP 51 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 25 31\n",
      "MLP 75 95\n",
      "HEAD 10 11\n",
      "QK 23 31\n",
      "V 25 31\n",
      "MLP 88 95\n",
      "HEAD 7 11\n",
      "QK 25 31\n",
      "V 31 31\n",
      "MLP 91 95\n",
      "HEAD 9 11\n",
      "QK 31 31\n",
      "V 29 31\n",
      "MLP 93 95\n",
      "HEAD 10 11\n",
      "QK 29 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 7 11\n",
      "QK 29 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 31 31\n",
      "MLP 91 95\n",
      "HEAD 7 11\n",
      "QK 27 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 6 11\n",
      "QK 30 31\n",
      "V 31 31\n",
      "MLP 93 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 93 95\n"
     ]
    }
   ],
   "source": [
    "var_type = \"EMB\"\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 29 47\n",
      "HEAD 10 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 6 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 8 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 12 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 9 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 12 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAHwCAYAAAAxacIvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzde7RddX3v/fdHEBCFROViCI/SGkBFikpoEBR3JBQsUCDjeNQqagv1Wqng7WC1x1Z96FOfSjh6UKk8XhBBTxsUSUglAiISkUSDiJKovUglGOQSLhIg8Hn+mL+FKyvrtpO9155r7c9rjDWYl9/6ze/ajuH4Zl4+U7aJiIiIiNHzhKkuICIiIiImRxq9iIiIiBGVRi8iIiJiRKXRi4iIiBhRafQiIiIiRlQavYiIiIgRlUYvImpL0kslrZnqOiaKpKslnTrVdUTE9JFGLyImjaT/kPSgpPsk3SPpOklvkdTX//fY/o7t/VvmWzB5FdePpH0kWdL2kzT/mKT/moy5I2LqpdGLiMl2vO1dgGcBfw+8Dzh/akuqSNpuqmuYCJPVBNb92BHRWxq9iBgI2xtsXwq8CniDpOcDSNpR0v8r6ZeSfi3p05KeVPY9frZJ0gXAM4FvSLpf0ntbj9EYL+n9kn5TzgC+tmn/5yV9StJSSQ8A81svp0p6o6Rrm9ZdzkL+rJyV/N+S1LT/zyX9VNLdkv5V0rOa9h0l6RZJGyR9Enj8e21q/0NJKyXdW/4OHy+7rin/vaf87heXGr8r6WxJdwIfkvQhSV9qmm+zM4GSnibpc5JuK7V+TdKTgcuBvcrc90vaq/ydPtL6d21a/w9J75P0I+ABSduX7/2LpDsk/buk0zr91ogYnDR6ETFQtr8P/Bfw0rLp74H9gBcAc4DZwN+0+d7JwC+pzhA+xfY/dDjEM4DdyjxvAM6TtH/T/j8FPgrsAly75dfbOg44BPgD4L8DRwNIOgF4P7AQ2B34DnBR2bcbsBj4QKnnF8DhXY5xDnCO7V2BZwNfLduPKP+dWX73irI+D/g3YM/ye3q5ANgZOADYAzjb9gPAK4DbytxPsX1bH3MBvAY4FpgJPAZ8A7iR6u9+JPBOSUf3OVdETJI0ehExFW4DnlbOjL0JON32XbbvA/5v4NXbOP8HbT9k+9vAEqrmrOHrtr9r+zHbG/uc7+9t32P7l8BVVE0pwFuAs2z/1PamUvsLylm9PwZutv3Pth8BFgG3dznGI8AcSbvZvt/293rUdJvtT9jeZPvBbgMlzaJq6N5i+27bj5S/zbb4X7ZvLcc+BNjd9t/Zftj2vwH/xLb/7xgR2yiNXkRMhdnAXVRnwXYGVpXLovcAy8r2rXV3OVPV8J/AXk3rt27FnM0N2m+Bp5TlZwHnNNV+F9Xl2dnlmI8fy7Z7HPsUqjObt0i6QdJxPWoaz+/4v4C7bN89ju/00nz8Z1Fd/r2n6W/xfqqzjRExhXITbUQMlKRDqBqha4HfAA8CB9j+VR9fdx9jnirpyU3N3jOBH3eZ4wGqZrPhGX0co+FW4KO2L2zdIWlfqgarsa7m9Va2fwa8pjyRvBD4Z0lPb1Pv419pWe/2O26lOoM60/Y9PebpNVe7790K/LvtfTvUGhFTJGf0ImIgJO1azlJdDHzJ9k22H6O6xHe2pD3KuNld7u36NfD7fRzubyXtIOmlVPfX/Z8uY1cDCyXtLGkO1Zm1fn0aOFPSAaX2GZJeWfYtAQ6QtLA8EHEaXZpISa+TtHv5mzSasceAO8p/e/3u1cARkp4paQZwZmOH7XVUD12cK+mpkp4oqXHv36+Bp5fvNM/1x+UBjmcA7+xx7O8D95UHNJ4kaTtJzy9NfURMoTR6ETHZviHpPqqzPn8NfBz4s6b97wN+DnxP0r3AcmD/LWapnAV8oFwefHeHMbcDd1PdB3gh1X1pt3Sp72zgYaqG5wvlO32xfQnw/wAXl9p/THUvHLZ/A7yS6mGTO4F9ge92me4Y4GZJ91M9mPFq2w/a/i3VwxbfLb/70A61XAF8BfgRsAq4rGXIyVT3Ad4CrKc0b+VvcxHwb2X+vage3LgR+A/gm2Xebn+HR6ka6hcA/051pvazwIxu34uIyafqtpGIiOEnaYzqbOHeU11LREQd5IxeRERExIhKoxcRERExonLpNiIiImJE5YxeRERExIhKoxcRERExohKYDOy2227eZ599prqMiIiIiJ5WrVr1G9t9vUEojR6wzz77sHLlyqkuIyIiIqInSf/Z79hcuo2IiIgYUWn0IiIiIkZUGr2IiIiIEZVGLyIiImJEpdGLiIiIGFFp9CIiIiJGVBq9iIiIiBGVRi8iIiJiRKXRi4iIiBhRafQiIiIiRlQavYiIiIgRlUYvIiIiYkSl0YuIiIgYUWn0IiIiIkZUGr2IiIiIEZVGLyIiImJEpdGLiIiIGFFp9CIiIiJG1PZTXUAdPLJhLeuWHDXVZURERMSQmnXsFVNdQltTckZP0l6SfiBpo6TtW9c7fGdM0iZJe5T1QyRZ0j6S3ijp1JbxGyRdLelaSfsN4ndFRERE1MlUXbq9CzgS+F6H9U5WAyeU5ZOAlV3G3mR7DHgX8N6trjQiIiJiSE1Jo2d7o+27O613cSVVQwhwAHBzH99ZDezdulHSmyStlLTyzg2P9FN2RERExFAZtocxHgY2SjoU+Gmf3zkCWNu60fZ5tufanvv0GU+cyBojIiIiamHYGj2ApcCngcU9xh0o6SrgbcBZk15VRERERM0M41O3S4GjgRt6jLvJ9vx+JnzijP1q+7RMRERExNaaqqdunyhpOXAQ8K+S5rWud/qu7fttn2LbLbvOkLS8fJ47ieVHREREDAVt2S9NPwftu6uXLerYW0bUXs5IR0RMH5JW2Z7bz9ha3qMnaUbJwLtO0n2SHpP0tD7z9q6RNLNpfZGklw2u+oiIiIh6qGWjZ3tDycB7OfBM4BrgXvrL27sMOK5p/XDg2smpNCIiIqK+atnoNWxl3t5iSqiypBcBN9p+tHVQcvQiIiJi1NW60dsatn8OzJa0E9XbMy7pMC45ehERETHSRq7RK64AFpTP8imuJSIiImJKDGOOXj8WA+cAv7T9UK/BydGLiIiIUVTrM3pbm7dn+0aq99u2vWwbERERMR3U+oye7UeoLr82a13v9N05/R7nkQ1rWbfkqPGUFlErOSMdERHt1OKMXms+Xj95eeV7J0m6p3xuLdl7MyRdW/bvLmmFpP0G92siIiIi6qEuZ/Qa+XiXdFjfgqTdgNOB2bYfkHQmsMH2BklI2hn4KnCG7bWTW35ERERE/dTijN5W5uUdC1xg+4GyfjZwYlneDrgQOMf2inZfTo5eREREjLpaNHpbaRZwW2PF9kZgh7I6E3g2sKzTl5OjFxEREaNumBu9dcBejZUSkNx4A8adVPEqn52CuiIiIiJqoS736G2Ny4F/kfTlcvn2dJru6bN9vqTnSzrT9lndJkqOXkRERIyiWpzR25q8PNvrgbOAZZJuAl4AfKpl2LuBl0g6YXJ/QURERET9yPZU17DNJD2L6gnb40sDOC4H7burly1qm708reSsZkRERP1JWmV7bj9jJ+2M3tZk40kak/SRpvXPS5oj6a2SftuUmbeq5OXtJOk+YJbtebbXS1oj6SpJ35J0rqRdJus3RkRERNTZZF66bWThfa/D+ng8CJxme2b5HGx7A3A0cDFwUtPYO2zPt30kcD3w4a3+BRERERFDbNIava3MxhuvE4APAgd2qOELVPfubSE5ehERETHqavEwRouTy6vMrgaOadr+nsZ2Sc8sl39n2r4duFHSAR3ma3sTYnL0IiIiYtTVMV7lAtsfgOoevabtH7P9eC6epAXAcyQtA3YBHgBubjOfJrHWiIiIiNqqY6PXr4VUT9n+AkDSktYBkk4GftBrouToRURExCiazKdux52N10PzpdujgIMbTV5xb4lZ2b3x1C3wYuB/TsTviYiIiBg2I5Gjt62SoxcRERHbYpBXBmuRo9dNydRbLekxSd8uZ+lulXSdpHM6fGdM0iZJe5T1QyRZ0j6S3ijp1JbxG8q810rabxC/KyIiIqJOpuqp27uA+cA1VNl6ZwDLbB8G7CDpkA7fW00VqQJVdt7KLse4yfYY8C7gvRNRdERERMQwmZJGr02m3qFA45zncqp769q5kqoxBDiA9k/ZtloN7N26MTl6ERERMerqkqM3E7i3LG8o6+08DGyUdCjw0z7nPgJY27oxOXoREREx6urS6G0Adi3LuwL3dBm7FPg0sLjHnAdKugp4G3DWNlcYERERMWTq0uit4HeXZBfQ/X24S4FVwA095rypvPP2JNvrJqDGiIiIiKEyJYHJkp4IXE7J1APeT3VJ9jvAatvf7/Rd2/cDp5R5mnedIenVZfkd46kngckRERExipKjR3L0YvjlHyoREdNH7XP0epE0o2TgXSfpvpK397Sy72xJ3+mSt3eNpJlN64skvWxQtUdERETURS0bPdsbSgbey4FnUuXt3SvpRcBTbL+Uznl7lwHHNa0fDlw7ySVHRERE1E4tG72GrczbW0wJVS6N4Y22H20dlBy9iIiIGHW1bvTa6Jm3Z/vnwGxJO1G9PeOSdhMlRy8iIiJG3bA1ev3m7V1BFdOygOrMX0RERMS0M2yNXr95e4uBdwO/tP3QIAqLiIiIqJspydHr19bm7dm+UdLeVG/Q6Ck5ehERETGKat3o2X6E6sxds+v7/O6cfo/zyIa1rFty1HhKi6iV/EMlIiLaqcWlW0l7SfqBpI2Sti/buubllTEnSbqnfG4t2XszJF1b9u8uaYWk/Qb1WyIiIiLqohaNHnAX1b1334PHY1G65uVJ2g04HZhteyZwLvBV2xvK/p2BrwJn2F47mJ8RERERUR+1aPS2Mi/vWOAC2w+U9bOBE8vydsCFwDm2V0xCyRERERG1V4tGr42eeXnALOC2xortjcAOTd9/NrCs0wESmBwRERGjrq6NXj95eeuAvRorJSC58QaMO4FzgM92OkACkyMiImLU1bXR6ycv73Lg9ZKeXNZPp+ktGLbPB+6QdOZkFhoRERFRV7WIV9mavDzb6yWdBSyTNBP4CfCnLcPeDVwq6QTbX+90/OToRURExCiS7amuYZtJehbVE7bH214/3u8ftO+uXrZo3sQXVmMr5nxxi20L9581BZVERETEeEhaZXtuP2Mn7dLt1mTjSRqT9JGm9c9LmiPprZJ+25SZt6rk5e0k6T5glu155SzfGklXSfqWpHMl7TJZvzEiIiKizibzHr1xZ+N18SBwmu2Z5XNwycs7GrgYOKlp7B2259s+kuotGh+eiB8TERERMWwmrdHbymy88ToB+CBwYIcavgC8YAKOExERETF0BvnUbT/ZeAAnl1eZXQ0c07T9PY3tkp5ZLgfPtH07cKOkAzrM1/YmxOToRURExKgb5FO3/WTjQfW2iw9AdY9e0/aP2X48F0/SAuA5kpYBuwAPADe3mU/tDmL7POA8qB7G6P9nRERERAyHQZ7R6ycbbzwWUj1le4ztw2lzKVjSycAPtvE4EREREUNp0s7obU02Xg/vkfS6svxR4GDbv2jaf2+JWdld0lXAY8Aa4H29Jp6OOXoLp7qAiIiImHQjkaO3raZjjl5ERERMnEGeMKpFjl4vJQev8XDFtyWtl3SNpH9oM3YfSZY0t6zvKWlTyd3bLHuv7G9k6V0v6bBB/aaIiIiIOpmyRs/2BttjtseATwJn2z4CeJKkg9p8ZRW/y8s7Afhhl+nvsD2f6grl30xg2RERERFDY8oavRa/D/yoLK8G2p2FuwV4blleQJXF15XtXwE7TkSBEREREcOmLo3eGuBlZXk+nTP2bpE0D9gIPNRrUknPoUOMS3L0IiIiYtTVpdH7BtUl229RNXC/7jDua8BngMt6zNd48nYR1dO+W7B9nu25tuc+fcYTt7LsiIiIiPoaZGByR7YfBd4BIOk8qjiWdm6guldvKfC8LlM27tGLiIiImLZq0ehJmg1cSJV998Vyb90WXGXBnFK+07zrtZIOLcsfaf1eL9MxRy8iIiJGX3L0gLlz53rlypVTXUZERERET+PJ0avFGb1Wki4HntS06c2215R92wNfAvYEbrD93pbvXgR81PaPy/o7gbttf6HT8R7ZsJZ1S46a4F8RMTg5Ix0REe3U5WGMzdh+RSNjr3zWNO0+Cbix3IPXLnPvEuDEpvXjqR72iIiIiJhWatno9dArc28pcDSApN2BTbbvGlx5EREREfUwjI1e18w92/cDd0naG/gT4NJ2kyRHLyIiIkbdMDZ6/WTufY3q8u2JZXkLydGLiIiIUTd0jZ7tR22/w/aRwKO0z9y7FHgVsGunqJaIiIiIUVfLp2676Sdzz/adkh4G+noUMTl6ERERMYqGrtErjd1YH+OOnPxqIiIiIupr6Bq9Vt0y9/qVHL0YdjkjHRER7dT2Hj1J20u6WNJVkv6hw5iDgZ3K6r8Df2x7jaSry/efJOlKSYcPrPCIiIiImqhto0ePYGRJOwCfAF5lewz4JvChpiFPoLqX75O2vzuQiiMiIiJqpM6NXq9g5BcDV9leD2D7IuDQpv2fKPsXt5s8OXoREREx6urc6HUNRgZmAbe1bHusafko4MudJk+OXkRERIy6Ojd6vYKR1wF7tWxrfrjkL4GLJaWLi4iIiGmptk/d2n4UeAeApPPYMhj5e8DfS9rD9npJrynbGr4J7Et1Cfct3Y6VHL2IiIgYRbU9oydpdnl69krgutZgZNsPAacBX5G0CngT8D9bxpwDPEHSOwZVd0RERERdyPZU17DNJD0VWAL82Xgz9AAO2ndXL1s0b+ILG7AVc764Td9fuP+sCaokIiIiJoukVbbn9jN2oGf0emXjSdpH0pea1j8kaYGkMUkPSrqnfG6StH8Z8zPgaNuHNWXoXS1puaQLJO05wJ8YERERURuDvnTbNRuvh3+0PbN8DixN3UHAtcDxLWMX2F4AfA741MSUHhERETFcBt3o9crGG6+FwLnAzpJ2bN1p+0pghqTtWvclRy8iIiJG3aAbvV7ZeABHNS6/Am9s2n5y02XZPyzbXmj7BmAZsKDDMdcDu7VuTI5eREREjLpBx6t8AziyZOP9B1tm4wFcYft1UN2j17T9AtsfaKxImgMcKGkZsCOwluqBjFZ7AL+ZkOojIiIihshAG70+svHGYyFwqu1vlfkulbTZGUpJLwPuLsftaFRy9BZOdQERERFRKwNt9CTNBi6kelXZF1uz8Xo4WdJLyvL5wLFUYcgNPwFeWpaXS9oE3A68fduqjoiIiBhOI5Gjt61GJUcvIiIipsYgrwzWNkevlaTLy8MV10i6U9L1kr7e+gRtydHbJGmPsn6IJJfcvTdKOrVl/IYy77WS9hvkb4qIiIioiylt9Gy/wvYYsAg4x/Y84PvAMW2GrwZOKMsnASu7TH1TmfddwHsnrOCIiIiIIVKXd93+AnhyWZ4J3NlmzJXAkWX5AODmPuZdDezdbkdy9CIiImLU1aXR+xnwYkk3A3OB69qMeRjYKOlQ4Kd9znsEVezKFpKjFxEREaOuLo3eG4Bv2D6AKgvvdR3GLQU+DSzuMd+Bkq4C3gacNWFVRkRERAyRQQcmdyLgrrL8G2BGh3FLgaOBG3rMd1N5n25fRiVHLyIiIqJZXRq9LwNfkXQy8AjwqnaDbN8PnAIgqXnXGZJeXZbfMYl1RkRERAyN5OiRHL0YfjkjHRExfQxNjl47kmaUDLzmz4ym/TtLWlK2t8vcu0bSzKb1ReVVaBERERHTSu0aPdsbbI+1fDY0DTkGuL7k5LXL3LsMOK5p/XDg2kktOiIiIqKGatfo9aFX5t5iSrCypBcBN9p+tHWS5OhFRETEqBvGRq9r5p7tnwOzJe1E9QaNS9pNkhy9iIiIGHXD2Oj1k7l3BbCgfJYPsLaIiIiI2qhLvMp49JO5txg4B/il7Yd6TZgcvYiIiBhFw9jo9czcs32jpL2p3qIRERERMS0NXaNn+x6qt2P0Gjen3zkf2bCWdUuO2qa6IqZSzkhHREQ7tb1Hr1deXhlzkqR7yufWRuaepGvL/t0lrZC03+B/QURERMTUqm2jR4+8PEm7AacDs23PBM4FvtrI3JO0M/BV4AzbawdZeEREREQd1LnR65WXdyxwge0HyvrZwIlleTvgQuAc2yvaTZ4cvYiIiBh1dW70uublAbOA2xortjcCO5TVmcCzgWWdJk+OXkRERIy6Ojd6vfLy1gF7NVZKQHLjDRh3UsWrfHYAdUZERETUUp0bvV55eZcDr5fUuLx7Ok1vwbB9PnCHpDMnu9CIiIiIOqpzvErXvDzb6yWdBSyTNBP4CfCnLXO8G7hU0gm2v97pQAlMjoiIiFEk21NdwzaT9CyqJ2yPt71+vN8/aN9dvWzRvIkvrMZWzPniFtsW7j9rCiqJiIiI8ZC0yvbcfsYO9NJtr2w8SWOSPtK0/nlJcyS9VdJvmzLzVpW8vJ0k3QfMsj2vnOVbI+kqSd+SdK6kXQb5GyMiIiLqYtD36HXNxuviQeA02zPL5+CSl3c0cDFwUtPYO2zPt30kcD3w4YkrPyIiImJ4DLrR65WNN14nAB8EDmy30/YXgBe025ccvYiIiBh1g270emXjAZxcLu1ezeZn/N7T2C7pmZK2B2bavh24UdIBHY7Z9ibE5OhFRETEqBt0o9crGw+qt12Mlcu7zYHHH2tst/1LYAx4jqRlwBFsfvm2mSas+oiIiIghMuh4lV7ZeOOxkOop218ASFqyxcGqaJYfbMMxIiIiIobWoBu9rtl4PbxHUuMM4EeBgxtNXnFviVnZXdJVwGPAGuB9vSaejjl6C6e6gIiIiJh0I5Gjt62mY45eRERETJxBnjCqbY5es5KD13i44kclH+87ktZJOrFl7D6SLGluWd9T0qaSu7dZ9l7Z38jSu17SYYP8XRERERF1MWWvQCs5eGOt2yVdDyxv85VVVA9crKSKVflhl+nvsD1f0mzgfPrP64uIiIgYGVN2Rq8dSb8P/Nr2/W123wI8tywvoH0zuBnbvwJ27DUuIiIiYhTVqtGjekbgki77b5E0D9gIPNRrMknPAe7psC+ByRERETHS6tboHQ9c2mX/14DPAJf1mKfx5O0i4P3tBiQwOSIiIkbdlN2j10rSM4CHbXd7LdoNVPfqLQWe12XcHbbnT2R9EREREcOmNo0e1QMWX+82wFUWzCkA0mYvvHitpEPL8kdav9fLdMzRi4iIiNGXHD2SoxfDL/9QiYiYPoYiR68bSZc3ZexdLWn/pn3HNG1vl7l3kaTnN62/U9IbBll/RERERB3U6dLt42y/osu+ZcAy6Ji5dwlwIvDjsn488MpJKDMiIiKi1mp5Rq8fXTL3lgJHlzG7A5ts3zXo+iIiIiKm2tA2enTI3CuN312S9gb+hA5xLcnRi4iIiFE3zI1et8y9r1Fdvj2xLG8hOXoREREx6mp5j14vfWTuXUrV4D1WXoMWERERMe0MZaNHj8w923dKehjoK3MiOXoRERExioay0bP9mT7GHDmIWiIiIiLqqraNnqRjgP9RVvcH3mr7ay1jDga+CWwHPAj8DPgLqvfhLgCeCCwBPmj7u52O9ciGtaxbctSE/4aIQckZ6YiIaKe2jV6vvDxJOwCfAJ5re72k1wAvtL2mvB7tCcCFwCe7NXkRERERo6r2T912yct7MXCV7fUAti8CDm3a/4myf/FgKo2IiIiol9o3enTIywNmAbe1bHusafko4MudJk2OXkRERIy6YWj0OuXlrQP2atnWfCn6L4GLJbUNyUuOXkRERIy6Wjd6PfLyvge8XNIeZexryraGbwKXUV3CjYiIiJh2avswRtExL8/2Q5JOA74iaVfgXuC4ljHnSDpP0jtsd2z4kqMXERERo6jWjV6vvDzbNwDzJT2VKkZlb2CN7bGmMW+a1CIjIiIiaqrWjV4zSZcDT2ra9GbbawBs3w0ctrVzT8ccvRVzvrjFtoX7z5qCSiIiImKyDPQePUnHSLq6fNZJOrFl/z6SvtS0/iFJCySNAc9rGvrxRpMn6WeSXt30ncb8yyVdIGnPSf5ZEREREbU00DN6vUKQe7jA9geaN0g6CLiW6snci5t2LbC9SdLLgU9RRbRERERETCtT8tRtlxDk8VoInAvsLGnH1p22rwRmSNquTQ3J0YuIiIiRNlXxKp1CkAGOalx+Bd7YtP3kpsuyf1i2vbA8kLGM6t227awHdmvdmBy9iIiIGHVT9TDG8XS+nHqF7ddBdY9e0/bNLt1KmgMcKGkZsCOwlurJ21Z7AL+ZiKIjIiIihsnAG70eIcjjsRA41fa3yryXStrsDKWklwF3236020TTMUcvNy1GRESMvqk4o9cxBLmHkyW9pCyfDxzL5m+9+Anw0rK8XNIm4Hbg7VtbaERERMQwk+3uA6TDgdW2H5D0OuBFwDm2/3MQBQ7CQfvu6mWL5k11GRERETGkBnllUNIq23P7GdvPwxifAn5bokzeBfwC2DJtdytIurzpAYufSlpRlme3jBuTtKnpvbaHSHLJ3XujpFNbxm8o81wrab+JqDUiIiJi2PTT6G1yddrvBOCTtv83sMtEHNz2K8rryl4LXGf7xbbHbP+qzfDVpQaAk4CVXaa+qcz7LuC9E1FrRERExLDpp9G7T9KZwMnAkvLAw0TnkRwNbCfpW5I+0S73DrgSOLIsHwDc3Me8q6nef7uF5OhFRETEqOun0XsV8BDw57Zvp2qcPjbBdewJ7GD7SOC3/O7MXbOHgY2SDgV+2ue8R1DFrmwhOXoREREx6no2eqW5+xeqrDqoMuk6hR1vrQ3At8vylcBzO4xbCnwaWNxjvgMlXQW8DThrQiqMiIiIGDI941Uk/QXwJuBpwLOB2VTN1pHdvjdO1wF/UZZfAPx7h3FLqS7z3tBjvptsz+/34NMxRy8iIiJGXz+Xbt8OHA7cC2D7Z1Rvm5gwtlcDD5bXnh0C/HOHcffbPsVbZsKcIWl5+XQ6GxgRERExrfSTo3e97XmSfmj7hZK2B35g+w8GU+LkS45eDLuckY6ImD4mOkfv25LeDzxJ0lHA/wG+sS0FdiNpRlO2XuMzo2XM68sTusfd1dMAACAASURBVO0y966RNLNpfVF5FVpERETEtNLPK9D+B3AKcBPwZqr75D47WQXZ3gCMddpfGruXlSd027kMOA74Ulk/nCpPLyIiImJa6eep28ds/5PtV9r+b2W5+/XeydUrc28xJZ5F0ouAG20/2jpJcvQiIiJi1PVs9CQdJ+mHku6SdK+k+yTdO4jiOuiauWf758BsSTtRvUGjbRRMcvQiIiJi1PVzj94i4A3A023vansX27tOcl3d9JO5dwWwoHyWD6iuiIiIiFrp5x69W4EfT/Hl2mb9ZO4tBs4Bfmn7oV4TJkcvIiIiRlE/jd57gaWSvk31KjQAbH980qrqwvZqSY3Mvd8AZ7cZc6OkvamCnSMiIiKmpX4avY8C9wM7ATtMbjn9sf3uPsbM6Xe+RzasZd2So7atqIgplDPSERHRTj+N3l62nz/plbQh6fVU9wduB7zW9q9a9p8EfK6s3gf8gurhjCW2XyJpd+BS4A221w6u8oiIiIip10+jt1TSH9n+5qRX06RXXp6k3YDTgdm2H5B0JrDB9gZJSNoZ+CpwRpq8iIiImI76eer2rcCycl/cIONVeuXlHQtcYPuBsn42cGJZ3g64EDjH9op2kydHLyIiIkZdP4HJu9h+gu0nDThepWteHjALuK2pzo387h7CmcCzgWWdJk+OXkRERIy6fi7dIumpwL5UD2QAYPuaySqqaM3La3157zpgr6YadwIab8C4k+revc8Cr5vcMiMiIiLqqWejJ+lU4K+AvYHVwKHACuDlk1taz7y8y4F/kfTlcvn2dJregmH7fEnPl3Sm7bO6HSg5ehERETGK+rlH76+AQ4D/tD0feCFwz6RWRZWXBzTy8g4B/rll/3rgLKr7B2+iagY/1TLNu4GXSGq97BsREREx8vq5dLvR9sbyJOuOtm+RtP+kV0bvvDzbS6meCn4W1RO2TwfW235J2f8o1UMbXSVHr5KzmhEREaOlnzN6/yVpJvA14ApJXwf+c2sPKOn15Unaq0uESvO+MUkfaVr/vKQ5kt4q6beS7imfVZJmSNpJ0n3ALNvzbK+XtEbSVeUY50raZWtrjYiIiBhmPc/o2T6pLH5I0lXADLo8zdpNr2y8Lh4ETrP92Zb5TgAuBk4Cvlc231EuMSPpDcCHgXduTb0RERERw6yfM3qPs/1t25fafngrj9crG2+8TgA+CBzYbqftL1Ddu7eF5OhFRETEqOvY6DWCkct/72ta/62kTVt5vF7ZeAAnl8u6VwPHNG1/T2O7pGdK2h6Yaft24EZJB3Q4pttuTI5eREREjLiOl25tb3Zvm6SnAG8H3kxTjMk49crGg+ptFx8ox/x80/aPNV+6lbQAeI6kZcAuwAPAzW3m01bWGhERETHU+snRm0l1j9vrgS8Dh9i+cyuP1ysbbzwWAsfb/kWpc0nrAEknAz/oNVFy9CIiImIUdWz0JO0GvAt4FfD/AS+0vWFbDmZ7dXln7tXAb6jeT9uv90hqvOXio8DBjSavuLfErOxeHhp5DFgDvG9bao6IiIgYVrLb3sKGpAeAO6heJXZf637bH5/c0gbnoH139bJF86a6jIiIiBhSg7wyKGmV7Xa3v22h26Xbj/G7BxkmPItO0gzg62V1J+BFwPXAg7b/qGXsPlSXeQ+xvVLSnsCvgAVlyILGfX1l/BrgNmBn4HTb1010/RERERF11+1hjA9N5oHLZeAxeLyR+4jt13X5yiqqvLyVVE/r/rDL2Dtszy+5feez+dO7EREREdPCuHL0Jtl8Sd+RdHqH/bcAzy3LC4DlvSa0/Stgx3b7kqMXERERo64ujd46YD9gPrBA0h90GHeLpHnARuChXpNKeg5wT7t9ydGLiIiIUVeLRs/2Q7YfsL0JuAx4foehXwM+U8Z003jydhHw/omrNCIiImJ49Gz0JO0p6XxJl5f150k6ZSKLkNT8sMfhwC86DL2B6l69pT2mvMP2fNvH2P7pRNQYERERMWx6BiYDn6eKWPnrsr4W+ArVQw4T5aWSPkx1OfY7tq9vN8hVFswpANJmL7x4raRDy/JHxnvwBCZHRETEKOqYo/f4AOkG24dI+qHtF5Ztq22/YCAVDkBy9GLY5R8qERHTx3hy9Pq5R+8BSU+nZOqVM2fb9IaMXiRdLunqps/+Tfv2kfTrsv2bbb57kaTnN62/U9IbJrPeiIiIiDrq59LtGcClwLMlfRfYHfhvk1mU7Vf0GHJFl8y9S4ATgR+X9eOBV05UbRERERHDoucZPds/AF4GHAa8GTjA9o8mu7AeumXuLQWOBpC0O7DJ9l2tg5KjFxEREaOu4xk9SQs77NpPErYXT1JNvTQy9x4Cvi7pW82Np+37Jd0laW+qhu/SdpPYPg84D6p79Ca/7IiIiIjB6nbp9vgu+wxMSaNn+yFKWLKkRuZe6xnGr1Fdvj0aeMtAC4yIiIioiW7vuv2zQRbSL0m72L6vrB4OfKLNsEupmr3HymvQIiIiIqadng9jSPqbdttt/93El9OXnpl7tu+U9DDQV+ZEcvQiIiJiFPXz1O0DTcs7AccBU/a2CdtL6f1mDGwf2e+cj2xYy7olR21TXRFTKf9QiYiIdvp56vYfmz4fBcaA35/swnrl5ZUxB0u6U9I9ktZJukbS/uU720t6kqQrJR0+2fVGRERE1E0/Z/Ra7QzsPdGFdNAxL0/SDlT35z3X9npJrwFeaHtNeT3aE4ALgU/a/u6A6o2IiIiojX7u0buJ8lYMYDuqwORB3Z83X9J3gMW2z27Z92LgKtvrAWxfJOmtTfs/UfZPVQxMRERExJTq54zecU3Lm4Bf2940SfU065qXB8wCbmv5zmNNy0cB7+80uaQ3AW8CmL37ThNScERERESddLxHT9LTJD0NuK/p8yCwa9k+qWw/ZPuB0lQ28vKarQP2atnW3Lj+JXCxpCd2mP8823Ntz336jLZDIiIiIoZat4cxVgEry3/vANYCPyvLqya7MEm7NK0eDvyiZcj3gJdL2qOMf03Z1vBNqgaxXc5eRERExMjrFpj8ewCS/gm4pMSaIOkVVG+dmGxd8/JsPyTpNOArknYF7mXzy8zYPkfSeZLeYbtjw5ccvYiIiBhFsru/5lXSTbYP7LVtKkl6KrAE+DPba8b7/YP23dXLFs2b+MJqbMWcL26xbeH+s6agkoiIiBgPSatsz+1nbM8cPeA2SR8ouXb7SPprtnwIot/Cumbjlf1falr/kKQFksYkPVjy8u6RdJOk/cuYnwFH2z6sRKtcXT7LJV0gac+tqTUiIiJi2PXT6L2GKlLlkvLZo2zbWlfYHrP9R+P83j/anlk+B5am7iDgWuD4lrELbC8APgd8ahtqjYiIiBha/bwZ4y7bfwUcAbzU9l/Zvmsbjjlf0ncknb4NczQsBM4Fdpa0Y+tO21cCMyRtNwHHioiIiBgqPRs9SQdK+iHwY+BmSasktUad9KuRjTcfWCDpD9qMOapx+RV4Y9P2k5suy/5h2fZC2zcAy4AFHY65HtitdaOkN0laKWnlnRse2cqfExEREVFf/QQmfwY4w/ZVAJLGgPOAw8Z7MNsPUT1Fi6RGNt6PWoY9/tozSR9q2n6B7Q80ViTNAQ6UtAzYkSr+ZUmbw+4B/KZNLeeV38FB++7a/YmUiIiIiCHUzz16T240eQC2rwaevDUH6yMbbzwWAqfaPsb2fGCWpM1+j6SXAXfbfnQbjhMRERExlPo5o/dvkj4IXFDWXwf821Yer2s2Xg8nS3pJWT4fOJbNw5B/Ary0LC+XtAm4HXh7r4mnY47ewqkuICIiIiZdPzl6TwX+Fmg0Wd8BPmT77kmubWDmzp3rlStXTnUZERERET2NJ0ev5xm90tCdts1VtSHpcuBJTZuuA46w/ZKWcWPAcmAv2+slHQJ8H/g9YAzY3vZnm8ZvAH5I9fv+3PbabnU8smEt65Ycte0/KCIiIqalul4Z7NjoSbq02xdt/8m2Htz2K5qOtyPl4YgOVgMnAP8EnET1Ht5ObrI9Jmke8F7g1G2tNSIiImLYdDuj92LgVuAi4HpAk1zLKcAXgL/rsP9K4EiqRu8A4OY+5lwN7D0h1UVEREQMmW5P3T4DeD9VBMo5wFHAb2x/2/a3J7IISU8ExkrAcScPAxslHQr8tM+pj6CKXWl3zOToRURExEjr2OjZftT2MttvAA4Ffg5cLekvJ6GOk4Ev9zFuKfBpYHGPcQdKugp4G3BWuwG2z7M91/bcp8944riKjYiIiBgGXR/GKPfNHUv1btt9gP9F9b7bibY/8AJJbwEOkPQO259oM24pcDRwQ4/5birZehERERHTVreHMb5Iddl2KfC3tn88WUXYfl/Tca/t0ORh+36qe/mQNrtl8AxJry7L7xjv8adjjl5ERESMvo45epIeAx4oq82DBNj2rpNc28AkRy8iIiKGxYTk6Nnu5/VoE07SDODrLZtPsL1hso6ZHL0YdjkjHRER7UxJM9eN7Q22x1o+WzR5kk6XdG2b7ddImtm0vqi88zYiIiJiWqldo9eP8pDICzrsvgw4rmn9cGCLhjAiIiJi1A1lo8fvwpXbWUz1Bg0kvQi40fajrYOSoxcRERGjbugavV7hyrZ/DsyWtBPVq9LaxsEkRy8iIiJG3dA1evQXrnwFsKB8lk96RRERERE11DUwuab6CVdeTPXatl/afqjXhMnRi4iIiFE0dI1eP+HKtm+UtDfV69IiIiIipqWha/Sa2X5Jl31z+p0nOXox7HJGOiIi2qn9PXqd8vLKvpMk3VM+t0q6WtKMxnhJu0taIWm/wVYdERERMfVqfUavW16epN2A04HZth+QdCawwfYGSUjaGfgqcIbttYOrOiIiIqIe6n5Gr1te3rHABbYb7+M9GzixLG8HXAicY3tFuy8nRy8iIiJGXW0bvV55ecAs4LbGiu2NwA5ldSbwbGBZp/mToxcRERGjrraNHr3z8tYBezVWSkBy4w0Yd1LFq3x20qqLiIiIqLk636PXKy/vcuBfJH25XL49naa3YNg+X9LzJZ1p+6xuB0qOXkRERIyi2jZ6vfLybK+XdBawTNJM4CfAn7ZM827gUkkn2P76pBcdERERUSOyPdU1bDNJz6J6wvZ42+vH+/2D9t3VyxbNm/jCamzFnC9usW3h/rOmoJKIiIgYD0mrbM/tZ+yU3KPXKRtP0pikjzStf17SHElvlfTbpsy8VSUvbydJ9wGzbM8rZ/nWSLpK0rcknStpl4H+uIiIiIiaGHij1y0br4sHgdNszyyfg21vAI4GLgZOahp7h+35to8Ergc+PCGFR0RERAyZqTij1y0bb7xOAD4IHNhup+0v0DlwOTl6ERERMdIG2uj1kY0HcHJ5ldnVwDFN29/T2C7pmZK2B2bavh24UdIBHeZrexNicvQiIiJi1A36qdte2XhQve3iA1Ddo9e0/WO2H8/Fk7QAeI6kZcAuwAPAzW3m0zZVHBERETGkBt3o9crGG4+FVE/Z/gJA0pLWAZJOBn7Qa6LpmKO3cKoLiIiIiEk30EavVzZeD++R9Lqy/FHg4EaTV9xbYlZ2l3QV8BiwBngfEREREdPQSOTobavpmKMXERERE2eQVwZrn6MHUHLwGg9X3CBpg6TrJH1OklrGjknaJGmPsn6IJEvaR9IbJZ3aMn5DmfdaSfsN8ndFRERE1MWUNXq2N9gesz0GHGZ7hu3Dyu52XepqqjgVqHLzVnaZ/qYy77uA905QyRERERFDZcoavWa2m4PsHgJubTPsSuDIsnwA7Z+wbbUa2LvdjuToRURExKirRaMHIOlPJP0Y2BO4s82Qh4GNkg4FftrntEcAa9vtSI5eREREjLraNHq2L7X9fOC/gOM6DFsKfBpY3GO6A8uTt28Dzpq4KiMiIiKGx6Bz9NqStKPth8rqvVTvtm1nKdX7bW/oMeVNtuf3e/zpmKMXERERo68uZ/SOkfRtSd+munT7zXaDbN9v+xRvmQlzhqTl5fPcSa82IiIiYggkR4/k6MXwyxnpiIjpYyhy9DppyddrfGY07X9+ydv7TofMvWskzWxaXyTpZYP8DRERERF1ULtGrzlfr+mzoWnIGtuH2X5pWW/taC9j84c5DgeuncyaIyIiIuqodo1eL31k7i2mBCtLehFwo+1HW+dJjl5ERESMuqFr9KB75p7tnwOzJe1E9QaNS9rNkRy9iIiIGHVD2ej1kbl3BbCgfJYPsraIiIiIuhi6Rk/Sjk2rnTL3FgPvBn7ZlM8XERERMa3UIjB5nI6RdEZZ/hltMvds3yhpb6q3aPSUwOSIiIgYRUPX6Nn+OvD1PsbN6XfORzasZd2So7aproiplH+oREREO7W9dNsrL6+MOUnSPeVzayNzT9K1Zf/uklZI2m/wvyAiIiJiatW20aNHXp6k3YDTgdm2ZwLnAl9tZO5J2hn4KnCG7bUDrDsiIiKiFmrb6PWRl3cscIHtB8r62cCJZXk74ELgHNsr2s2fHL2IiIgYdbVt9KB7Xh4wC7itsWJ7I7BDWZ0JPBtY1mnu5OhFRETEqKt1o9cjL28dsFdjpQQkN96AcSdwDvDZQdQZERERUUe1bfT6yMu7HHi9pCeX9dNpeguG7fOBOySdOamFRkRERNRUneNVuubl2V4v6SxgmaSZwE+AP22Z493ApZJOKLEsbSVHLyIiIkaRbE91DdtM0rOonrA93vb68X7/oH139bJF8ya+sBpbMeeLW2xbuP+sKagkIiIixkPSKttze48c8KXbXtl4ksYkfaRp/fOS5kh6q6TfNmXmrSp5eTtJug+YZXteOcu3RtJVkr4l6VxJuwzyN0ZERETUxaDv0euajdfFg8BptmeWz8ElL+9o4GLgpKaxd9ieb/tI4HrgwxNWfURERMQQGWij10c23nidAHwQOLDD8b4AvKDdvuToRURExKgb+FO3PbLxAE4urzK7Gjimaft7GtslPVPS9sBM27cDN0o6oMMh296EmBy9iIiIGHUDb/R6ZONB9baLMdtjbB54/LHGdtu/BMaA50haBhzB5pdvm23xjtyIiIiI6WCg8SqSdrT9UFltl403HgupnrL9RZl7SZvjnQz8YBuOERERETG0Bp2j1zUbr4f3SHpdWf4ocHCjySvuLTEru0u6CngMWAO8r9fE0zFHb+FUFxARERGTbiRy9LbVdMzRi4iIiIkzyBNGtc3Ra1Zy8BoPV6yStEHSCklntxm7jyRLmlvW95S0qeTubZa9V/Y3svSul3TYoH5TRERERJ1M2SvQSg7eGICkZwD32N4o6UJJB9q+qeUrq6geuFhJFavywy7T32F7vqTZwPls/vRuRERExLRQi3fdloiUhkeAR9sMuwV4blleACzvY95fSdpx2yuMiIiIGD5Tdum2HUl/AOxu+ycdhtwiaR6wkSpwudd8zwHu6bAvgckREREx0mrT6El6GvBJ4JQuw74GfAa4rMd0jSdvFwHvbzcggckREREx6mpx6ba85eJLwLtbLuO2uoHqXr2lwPO6jLvD9vwJLDEiIiJi6NSi0QNeCRwC/IMkgDNtr2gd5CoL5hSAMq7htZIOLcsfaf1eL9MxRy8iIiJGX3L0SI5eDL/8QyUiYvoYihy9biRd3pSxd7Wk/Zv2zZN0naRrO2TuXSTp+U3r75T0hkHVHhEREVEXdbl0uxnbr+iy+z+Bl3fJ3LsEOBH4cVk/nurScERERMS0Usszet3Yvt32xrLaLnNvKXA0gKTdgU227xpgiRERERG1MHSNXkOnzD3b9wN3Sdob+BPg0g7fT45eREREjLShbPT6yNz7GtXl2xPL8haSoxcRERGjrpb36HXTZ+bepVQN3mO2fzWw4iIiIiJqZOgaPfrI3LN9p6SHgb4yJ5KjFxEREaNo6Bo92xcBF/Ux7sgBlBMRERFRW7Vt9CTNA84GHgNusH16mzEHA98EtgMeBH4G/AXV+3AXAE8ElgAftP3dTsd6ZMNa1i05asJ/Q8Sg5Ix0RES0U+eHMRp5eS8B9pB0YPNOSTsAnwCea3smcAbwPdtrypAnABcCn+zW5EVERESMqto2en3k5b0YuMr2+jL+IuDQpv2fKPsXT3qxERERETVU20avoVNeHjALuK1l22NNy0cBX+4yb3L0IiIiYqTVutHrkZe3DtirZVvzPYd/CVwsqW1IXnL0IiIiYtTVttHrIy/ve8DLJe1Rxr+mbGv4JnAZ1SXciIiIiGmntk/d0iMvz/ZDkk4DviJpV+Be4LjmCWyfI+k8Se+w3bHhS45eREREjKLaNnr95OXZvgGYL+mpVDEqewNrbI81jXnTZNYZERERUVe1bfRaSboceFLTpjc3olRs3w0ctrVzD1uO3oo5X5yUeRfuP2tS5o2IiIipMdB79CTNk3SdpGslnd1m/z6SvtS0/iFJCySNAc9rGvrxRpMn6WeSXt30navLZ7mkCyTtOYk/KSIiIqK2Bv0wRtcQ5B4usD1WPpcCSDoIuBY4vmXsAtsLgM8Bn5qIwiMiIiKGzUAbvT5CkMdrIXAusLOkHdsc70pghqTtWvclRy8iIiJG3ZTEq3QJQQY4qnH5FXhj0/aTmy7L/mHZ9sLyQMYyqnfbtrMe2K11Y3L0IiIiYtQN/GGMphDk/95hyBW2X1fGfqhp+wW2P9A0zxzgQEnLgB2BtVRP3rbaA/jNBJQeERERMVQG2uj1EYI8HguBU21/q8x9qaTNzlBKehlwt+2ul4iHLUdv4VQXEBEREUNh0Gf0uoYg93CypJeU5fOBY9n8rRc/AV5alpdL2gTcDrx9m6uOiIiIGEKyPdU1TLmD9t3VyxbNm+oyIiIiYkgN8sqgpFW25/YzdkoDk1tCkHcAZgDPBp5ie1PL2DFgObCX7fWSDgG+D/weMAZsb/uzTeM3AD+k+o1/bnvt5P6aiIiIiHqZ0kbP9isay5J2omr6LunyldXACcA/AScBK7uMvcn2mKR5wHuBU7e94oiIiIjhMSXxKu3Y3lheZdbNlcCRZfkA4OY+pl5N9Q7czSRHLyIiIkZdbRq9Pj0MbJR0KPDTPr9zBFX0ymaSoxcRERGjbtgaPYClwKeBxT3GHSjpKuBtwFmTXlVEREREzUzpPXpbaSlwNHBDj3E32Z7fz4TDlqMXERER0Y/anNGT9ERJy4GDgH8tD1Fswfb9tk/xlrkwZ0haXj7PnfSCIyIiImouOXokRy+GX85IR0RMH+PJ0avNGb1mkmZIurrlM6Ps20vSDyRtLK9Ua/3uNZJmNq0vKq9Ci4iIiJhWanmPnu0NVCHI7dxFFbHSKW/vMuA4qnfqAhwOvGsi64uIiIgYBrU8o9dNH3l7i6lClZH0IuBG24+2DkqOXkRERIy6oWv0erH9c2B2edPGSXQ485ccvYiIiPj/27v36Kqqc+/j34ebQUEoChbBAykKkkAIATQRLOR4oa1RwFolRY6eetqjFhE8XlFHdShVW4eo9VBbxRP1RQMDRVAQK0hewIIICoJA8BYFRImclyg3CeR5/9grcZPs3CBkX/L7jLEHe93mmisrCx7mnOuZiS7hAr3Am8D5wWdhlOsiIiIiEhUxOUavAbwMPAZ84e7f17az8uiJiIhIIoq7Fr265Ntz97WE5ret7oUNERERkYQXdy167l5KqEu2tv1Or2uZpSWb2T7vgqOql0g0qUVaREQiidkWvdry5QX7jDKzXcFnS3m+PTNbFmzvaGbLzaxn49ZeREREJPpiuUWvxnx5ZnYyMBHo4u57zOwOoMTdS8wMMzsemAnc5O6bG63WIiIiIjEiZlv06pAv7yLgeXffEyxPAUYG35sD04HH3H15pIOVR09EREQSXcwGenXQGfiyfMHd9wOtgsX2QA9gQXUHK4+eiIiIJLp4DvS2A6eWLwQJkstnwNhJKL3K01Gol4iIiEhMiOUxerV5HXjJzF4Ium8nEjaez92nmVkfM7vD3R+oqSDl0RMREZFEFLMterXly3P3HcADwAIzWwekA3+tVMzNwBAzG9EYdRYRERGJJebu0a7DUTOzboTesL04CADrpd8ZJ/qCR6vkXW5Uy09/LqrnB7i0V+doV0FERERqYWar3X1gXfZt1Ba92nLjmdkwM7s/bDnPzE43s+vMbG9YzrzVQb68JDP7Dujs7me7+w4zKzSzxWa2yMymmlnbxrxGERERkVjR2F235bnxVtTzuH3AeHdvH3wGuHsJMBzIB0aF7Vvs7tnufh7wDnBfQ1RcREREJN40aqBXh9x49TUCuBvoW835niU0dq8K5dETERGRRBeLL2OMDaYyKwB+Frb+lvL1ZvYvQddve3f/ClhrZqnVlBdxEKLy6ImIiEiii8X0Ks+7+10QGqMXtv7P7l6RF8/MzgfONLMFQFtgD/BhhPLsGNZVREREJGbFYqBXV5cSesv2EwAzm1d5BzMbC7zX2BUTERERiQWNGuiZWUtCiY7Lc+NNcvd36nj4LWZ2ZfB9MjCgPMgLfBukWeloZouBMqAQuK22gmMhYfKlUT27iIiIJKKEyKN3tGIhj56IiIjEr8ZsMIrZPHrhgjx4BWGfLWb2TzN7LMK+w8zsoJl1CpYHmZmbWXczu9rM/qPS/iVBmcvMrGdjXZOIiIhILIlaoOfuJe4+zN2HATcBC9z9HKCVmQ2KcMgaQulUIJQ3b1UNxa8Lyv0v4NaGq7WIiIhI/IiV9CqZQHmb50IgK8I+bxFKtgyQSuQ3bCtbA3SNtEF59ERERCTRxUqg1x74NvheEixXdgDYb2aZwMY6lvtTYHOkDcqjJyIiIokuVgK9EuDE4PuJwK5q9psPPAm8XEt5fYM3b68HHmiQGoqIiIjEmVgJ9JbzQ7fs+VQ/F+58YDXwbi3lrQvmux3l7tsbqI4iIiIicSUmEia7+3tmtt/MlgJr3H1lNfvtBq4BMDtswoubzGx08P2G+p4/FvLoiYiIiDQ05dFDefQk/uk/KiIiTUdc5NGrToT8egVm1q7SPlPMbGk1jBpMAgAAFnlJREFUOfeWmFn7sOVHzWxoY9RdREREJJbEXKAXnl8v7FNSvt3MMoA27n4ukXPuvQbkhC0PBpYd+5qLiIiIxJaYC/TqoLacey8TJFYOgsK17n6ociHKoyciIiKJLh4DvRpz7rn7x0AXM0siNIPG7EiFKI+eiIiIJLp4DPTqknPvTUJpWs4n1OonIiIi0uTEY6BXl5x7LwM3A1+4+/eNVTERERGRWBITefTqoy4599x9rZl1JTSLRq2UR09EREQSUdwFegDufmMd9jm9ruWVlmxm+7wLjq5SIlGk/6iIiEgkMd11W1O+vGD7KDPbFXy2lOfcM7NlwfaOZrbczHo2bs1FREREoi9mA73a8uWZ2cnARKCLu7cHpgIzy3PumdnxwEzgJnff3Li1FxEREYm+mA30qD1f3kXA8+6+J1ieAowMvjcHpgOPufvyY11RERERkVgUy4FejfnygM7Al+UL7r4faBV2bA9gQXWFK2GyiIiIJLpYfhmjtnx524FTyxeCBMnlM2DsBP4HeBq4MlLh7v534O8A/c440Rus1iIiEtdKS0vZunUr+/fvj3ZVpIlLSkqia9eutGx55BM7xHKgtxz4T0Lj7M4H8iptfx14ycxeCLpvJxI2C4a7TzOzPmZ2h7s/0Eh1FhGROLd161batm1L9+7dMbNoV0eaKHdn586dbN26leTk5CMuJ2YDvdry5bn7DjN7AFhgZu2BDcCvKxVzMzDXzEa4+5zqzqU8eiIiUm7//v0K8iTqzIyTTjqJ4uLioyonZgM9qD1fnrvPB+abWTdCLX8nATvcfUiw/RChlzZqlMh59Jaf/lyd9720V+djWBMRkfihIE9iQUP8Hjb6yxg15cYzs2Fmdn/Ycp6ZnW5m15nZ3rCceauDfHlJZvYd0Nndzw5a+QrNbLGZLTKzqWbWtlEvUERE5Cg1b96c9PT0ik9RUVGN+3fv3p1vvvkGgDZt2lTZnp2dzRtvvHHYukcffZTrrruuxnIjlSXxpVFb9MJz45nZX81skLu/W4dD9wHj3f3pSuWNAPKBUfww522xu2cH268C7gMmNNhFiIhIk/Jy4fYGLa8uvSetW7dmzZo1DXbO3Nxc8vPzGT58eMW6/Px8/vSnPzXYOSQ2NXaLXm258eprBHA30DfSRnd/Fkg/ynOIiIhEXV5eHuPGjatYzsnJoaCgoE7HXnbZZcybN48DBw4AUFRUxJdffsm5557L7t27Oe+888jIyKBv377MmVN1SHtBQQE5OTkVy+PGjSMvLw+A1atXM3ToUAYMGMDw4cPZvj0UGD/++OOkpKSQlpbG6NGjj/Cq5Wg19hi99sCnwfcSIDXCPmPNbEjw/UygvCv3FjMrT5Xyb4Ry6LV396/MbK2Zpbr7hxHKi5g6xcx+B/wOoEvHpPpfiYiIyDGyb98+0tND7RTJycnMnj27liNq1qFDB8466yxef/11RowYQX5+PpdffjlmRlJSErNnz+bEE0/km2++ITMzk0suuaRO48NKS0u54YYbmDNnDh07dmTGjBnceeedPPPMMzz44IN89tlnHHfccezaVTlDmjSWxg70asuNB6HZLu6C0Bi9sPV/Du+6NbPzgTPNbAHQFtgDRAr0Iv6mKo+eiIjEqobuuoUfum/LA71p06YBoTQekyZNYsmSJTRr1oxt27bx9ddf8+Mf/7jWMgsLC1m/fj0XXBB6ofHQoUN07hzqmk5LS2PMmDGMHDmSkSNH1lSMHEONHejVlhuvPi4FLnb3TwDMbF7lHcxsLPDeUZxDREQkJrRo0YKysrKK5fomdB4xYgQTJ07kvffeY+/evQwYMACA6dOnU1xczOrVq2nZsiXdu3evUnZ153Z3UlNTWb686myj8+bNY8mSJbz66qtMnjyZdevW0aJFTCf7SEiN+hOvLTdeLcK7bicDA8qDvMC3QZqVjma2GCgDCoHbais4kfPoXRrtCoiISIPo3r07U6dOpaysjG3btrFyZX3+CQ29QZudnc1vfvMbcnNzK9aXlJTQqVMnWrZsyeLFi/n888+rHNutWzc2bNjA999/z759+1i0aBFDhgyhV69eFBcXs3z5crKysigtLWXz5s307t2bLVu2kJ2dzZAhQ8jPz2f37t20b195NlM51ho9tK4pN567FwAFYctXB18/pmrr32GRmbuX/9b2OsoqioiIxJzBgweTnJxMSkoKvXv3JiMjo95l5ObmMmrUKPLz8yvWjRkzhosvvpi+ffsycOBAzjzzzCrHnXbaaVx++eX06dOH5ORk+vfvD0CrVq2YNWsW48ePp6SkhIMHDzJhwgR69uzJlVdeSUlJCe7O+PHjFeRFiblHZ3iambUDKr/aM8LdSyLsOwx4FvgsWPUI8DJwapA7bxCwEkgGhgF3EJoL14HR7v51TXXpd8aJvuDRs4/8YkSOgfoku5ZjS8nEm5aNGzfSu3fvaFdDBIj8+2hmq919YF2Ob/SEyeXcvcTdh1X6VAnywjxfvh/wLbCGUHoVCOXRWxW275+D/Z6i6rRoIiIiIk1C1AK9BvAWcF7wPZXIb9yeSCgoFBEREWly4un1l/D8evOBA8B+M8sENgLh74HfYmZXA6cBEftklUdPREREEl08teiFd92Wv2o0H3iS0Hi9cH929yFADqE3dKtw97+7+0B3H3hSu5bHqs4iIiIiURNPgV4k84HVQHXz5e4COjRedURERERiR7x23U4DcPfdwDVA5alabjGzMUAr4KbaCk7kPHoSv5QDUUREjlZctOi5e4G7dwt7O7dimrSwfa529yJ3z3P3Xu6e7e6D3f2daNVbRETkSLRp06bi+/z58+nZs2fERMa1KSoq4oUXXoi47Sc/+QmFhYWHrZswYQIPPfRQjeX16dOn3vVoCLm5uaSlpTFlyhTy8vL48ssvj6q8vLw8OnbsSHp6OikpKTz11FMNVFN48sknee652EiRFU8teiIiIo2v4OKGLW/Yq3XeddGiRYwfP5433niDbt261ftU5YHer39dNdPY6NGjyc/P5w9/+AMAZWVlzJo1i7fffrve5znWvvrqK959910+/vhjAIYNG0afPn049dRT61zGwYMHq0zBdsUVV/DEE0+wY8cOUlNTueSSSzjllFNqPKYurr322nofc6zERYueiIhIU7NkyRJ++9vf8tprr9GjRw8AiouL+eUvf8mgQYMYNGhQRVB2zz33MHbsWLKysjjjjDMqWqduv/12li5dSnp6OlOmTDms/NzcXGbMmHHY+bp160a3bt0oKiri3HPPJSMjg4yMDP75z39WqV9eXh7jxo2rWM7JyaGgoACAf/zjH2RlZZGRkcGvfvUrdu/eXVGflJQU0tLSuPnmm6uUuXLlSrKysujfvz/nnHNORYvjhRdeyLZt20hPT+e+++5j1apVjBkzhvT0dPbt28fq1asZOnQoAwYMYPjw4Wzfvh0IBYQTJkxg4MCBPPbYY9X+rDt16kSPHj34/PPPK36WgwcPZuzYsTVeZ5s2bbjzzjvp168fmZmZfP311xX34+GHH66ow2233cZZZ51Fz549Wbp0KQB79+7l8ssvJyUlhVGjRnH22WezatUqGppa9ERERGLM999/z8iRIykoKDhsSrIbb7yRiRMnMmTIEL744guGDx/Oxo0bAfjggw9YsWIFe/bsoX///lx00UU8+OCDPPzww7z22mtVztG3b1+aNWvG2rVr6devH/n5+RVz4Hbq1Ik333yTpKQkPvroI3Jzc+schHzzzTfcf//9LFy4kBNOOIGHHnqIRx55hN///vfMnj2bTZs2YWbs2rWryrFnnnkmS5cupUWLFixcuJBJkybx0ksvMXfuXHJyclizZg0Qaul8+OGHGThwIKWlpdxwww3MmTOHjh07MmPGDO68806eeeYZAA4cOFBr3T/99FM+/fRTTj/9dAA2bNjAsmXLaN26NXl5edUet2fPHjIzM5k8eTK33norTz31FHfddVeV/Q4ePMjKlSuZP38+9957LwsXLmTq1Kn86Ec/YsOGDaxfv5709PQ6/XzrS4GeiIhIjGnZsiXnnHMO06ZNO6wlauHChWzYsKFi+dtvv61oLRsxYgStW7emdevWZGdns3Llylrnl83NzSU/P5/U1FReeeUV7r33XgBKS0sZN24ca9asoXnz5mzevLnOdV+xYgUbNmxg8ODBQCjQysrKol27diQlJXHNNdeQk5NDTk5OlWNLSkq46qqr+OijjzAzSktLaz1fYWEh69ev54ILLgDg0KFDdO78w7SFV1xxRbXHzpgxg2XLlnHcccfxt7/9jQ4dQok6LrnkElq3bl3ruVu1alVxHQMGDODNNyO/2HnppZdW7FNUVATAsmXLuPHGGwHo06cPaWlptZ7vSCjQExERiTHNmjVj5syZnHfeefzxj39k0qRJQGgc3YoVK0hKqprov1L2iSrLkYwePZoLL7yQoUOHkpaWVjE+bcqUKZxyyimsXbuWsrKyiOdr0aIFZWVlFcv79+8HwN254IILePHFF6scs3LlShYtWsSsWbN44okneOuttw7bfvfdd5Odnc3s2bMpKipi2LBhtV6Du5Oamsry5csjbj/hhBOqPbZ8jF5Nx1R3nRAKyMt/zs2bN+fgwYMRz3PcccfVus+xojF6IiIiMej4449n3rx5TJ8+nWnTpgGhsWp/+ctfKvYp78oEmDNnDvv372fnzp0UFBQwaNAg2rZty3fffVftOXr06MHJJ5/M7bffXtFtC6GWtc6dO9OsWTOef/55Dh06VOXY7t27s2bNGsrKytiyZQsrV4bmMsjMzOTtt9+ueHFiz549bN68md27d1NSUsIvfvELpkyZwtq1a6uUWVJSQpcuXQBq7DINv65evXpRXFxcEeiVlpby4YeRZkU9MtVd59EaPHgwM2fOBEJdxevWrWuQcitToCciIhKjOnTowIIFC7j//vuZO3cujz/+OKtWrSItLY2UlBSefPLJin3T0tLIzs4mMzOTu+++m1NPPZW0tDSaN29Ov379qryMUS43N5dNmzZVdC8CXH/99Tz77LP069ePTZs2RWwVGzx4MMnJyaSkpDB+/HgyMjIA6NixI3l5eRXpULKysti0aRPfffcdOTk5pKWlMWTIEB555JEqZd56663ccccd9O/fv8aWr6uvvpprr72W9PR0Dh06xKxZs7jtttvo168f6enpEV8eOVLVXefRuv766ykuLiYlJYW77rqL1NRU2rVr1yBlhzN3b/BC483AgQP9WLzpIiIi8Wfjxo307t072tWol3vuuYc2bdpEfJNVYtOhQ4coLS0lKSmJTz75hPPPP5/CwkJatWp12H6Rfh/NbLW7D6zLeTRGT0RERKSR7d27l+zsbEpLS3F3pk6dWiXIawgK9EREROLcPffcE+0qSD21bdv2mOTNq0xj9EREREQSlAI9ERGRSjR+XWJBQ/weKtATEREJk5SUxM6dOxXsSVS5Ozt37oyYw7A+NEZPREQkTNeuXdm6dSvFxcXRroo0cUlJSXTt2vWoylCgJyIiEqZly5YkJydHuxoiDUJdtyIiIiIJSoGeiIiISIJSoCciIiKSoDQFGmBm3wGF0a6H1NvJwDfRroQcEd27+KV7F7907+JTpPvWzd071uVgvYwRUljXOeMkdpjZKt23+KR7F7907+KX7l18Otr7pq5bERERkQSlQE9EREQkQSnQC/l7tCsgR0T3LX7p3sUv3bv4pXsXn47qvullDBEREZEEpRY9ERERkQTVpAM9M/uZmRWa2cdmdnu06yPVM7PTzGyxmW0wsw/N7MZgfQcze9PMPgr+/FG06ypVmVlzM3vfzF4LlpPN7J3g2ZthZq2iXUepyszam9ksM9tkZhvNLEvPXHwws4nB35XrzexFM0vScxebzOwZM9thZuvD1kV8zizk8eAefmBmGbWV32QDPTNrDvw38HMgBcg1s5To1kpqcBD4L3dPATKB3wf363ZgkbufASwKliX23AhsDFt+CJji7qcD/w+4Jiq1kto8Bixw9zOBfoTuoZ65GGdmXYDxwEB37wM0B0aj5y5W5QE/q7Suuufs58AZwed3wF9rK7zJBnrAWcDH7v6pux8A8oERUa6TVMPdt7v7e8H37wj9g9OF0D17NtjtWWBkdGoo1TGzrsBFwNPBsgH/CswKdtF9i0Fm1g74KTANwN0PuPsu9MzFixZAazNrARwPbEfPXUxy9yXA/1ZaXd1zNgJ4zkNWAO3NrHNN5TflQK8LsCVseWuwTmKcmXUH+gPvAKe4+/Zg01fAKVGqllTvUeBWoCxYPgnY5e4Hg2U9e7EpGSgG/ifodn/azE5Az1zMc/dtwMPAF4QCvBJgNXru4kl1z1m9Y5emHOhJHDKzNsBLwAR3/zZ8m4deIddr5DHEzHKAHe6+Otp1kXprAWQAf3X3/sAeKnXT6pmLTcF4rhGEgvVTgROo2jUoceJon7OmHOhtA04LW+4arJMYZWYtCQV509395WD11+XN1sGfO6JVP4loMHCJmRURGh7xr4TGfbUPupRAz16s2gpsdfd3guVZhAI/PXOx73zgM3cvdvdS4GVCz6Keu/hR3XNW79ilKQd67wJnBG8htSI0UHVulOsk1QjGdU0DNrr7I2Gb5gJXBd+vAuY0dt2keu5+h7t3dffuhJ6xt9x9DLAYuCzYTfctBrn7V8AWM+sVrDoP2ICeuXjwBZBpZscHf3eW3zs9d/GjuudsLvBvwdu3mUBJWBdvRE06YbKZ/YLQ+KHmwDPuPjnKVZJqmNkQYCmwjh/Gek0iNE5vJvAvwOfA5e5eeVCrxAAzGwbc7O45ZvYTQi18HYD3gSvd/fto1k+qMrN0Qi/RtAI+Bf6dUAOBnrkYZ2b3AlcQyljwPvAfhMZy6bmLMWb2IjAMOBn4GvgD8AoRnrMgcH+CUFf8XuDf3X1VjeU35UBPREREJJE15a5bERERkYSmQE9EREQkQSnQExEREUlQCvREREREEpQCPREREZEEpUBPRCQCM1tsZsMrrZtgZtVOIm5mu499zURE6k6BnohIZC8SSvIcbnSwXkQkLijQExGJbBZwUTBzDmbWndC8oe+b2SIze8/M1pnZiMoHmtkwM3stbPkJM7s6+D7AzP6vma02szfCpjkab2YbzOwDM8s/9pcnIk1Bi9p3ERFpeoIs9CuBnxOafmg0oUz1+4BR7v6tmZ0MrDCzuV6H7PPBfM1/AUa4e7GZXQFMBn4D3A4ku/v3Ztb+GF2WiDQxCvRERKpX3n1bHuhdAxjwRzP7KaHp+LoApwBf1aG8XkAf4M3QTEY0B8rnqfwAmG5mrxCa/khE5Kgp0BMRqd4cYIqZZQDHu/vqoAu2IzDA3UvNrAhIqnTcQQ4fGlO+3YAP3T0rwrkuAn4KXAzcaWZ93f1gw12KiDRFGqMnIlINd98NLAae4YeXMNoBO4IgLxvoFuHQz4EUMzsu6IY9L1hfCHQ0sywIdeWaWaqZNQNOc/fFwG3BOdocswsTkSZDLXoiIjV7EZjND2/gTgdeNbN1wCpgU+UD3H2Lmc0E1gOfAe8H6w+Y2WXA42bWjtDfwY8Cm4H/E6wz4HF333VsL0tEmgKrw/hhEREREYlD6roVERERSVAK9EREREQSlAI9ERERkQSlQE9EREQkQSnQExEREUlQCvREREREEpQCPREREZEEpUBPREREJEH9fzFQPrq56DCPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_values = []\n",
    "pruned_values = []\n",
    "\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "var_type = \"EMB\"\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "names.append(\"EMB\")\n",
    "full_values.append(all_variable_specs[var_type][2])\n",
    "pruned_values.append(np.argmax(cur_decision_vars_value)+1)\n",
    "\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value)+1, all_variable_specs[var_type][2])\n",
    "        names.append(f\"{block_id}_{var_type}\")\n",
    "        full_values.append(all_variable_specs[var_type][2])\n",
    "        pruned_values.append(np.argmax(cur_decision_vars_value)+1)\n",
    "        \n",
    "plt.figure(figsize=(10, 8))  # Adjust the size as needed\n",
    "\n",
    "# Plot full values\n",
    "plt.barh(names, full_values, color='lightblue', label='Full Values')\n",
    "\n",
    "# Plot pruned values\n",
    "plt.barh(names, pruned_values, color='orange', alpha=0.7, label='Kept Values after Pruning')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Module Names')\n",
    "plt.yticks(fontsize=8)\n",
    "plt.title('Deit pruned structure')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_dims = {\"EMB\": 304, \"HEAD\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(48-29) * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
