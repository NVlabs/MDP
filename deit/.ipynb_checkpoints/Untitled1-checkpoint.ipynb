{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\t   hubconf.py\t\t\trun_with_submitit.py\n",
      "Untitled1.ipynb    latency_head.json\t\tsamplers.py\n",
      "__pycache__\t   losses.py\t\t\tsave\n",
      "assets\t\t   main_full_global_latency.py\ttrain_ngc.sh\n",
      "datasets.py\t   model_pruning.py\t\ttrain_ngc_2.sh\n",
      "engine.py\t   models.py\t\t\ttrain_ngc_ours.sh\n",
      "eval_nvit.py\t   pre_compute_saliency.pkl\tutils.py\n",
      "finetune_dense.py  pruning_configs\n",
      "finetune_ngc.sh    pruning_core\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/alex/NViT/nvit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_allres_pruningm(dict):\n",
    "    res = False\n",
    "    if \"allow_trim\" in dict.keys():\n",
    "        if dict[\"allow_trim\"]:\n",
    "            res = True\n",
    "    return res\n",
    "\n",
    "def compute_saliency():\n",
    "    def write_to_debug(what_write_name, what_write_value):\n",
    "        # Aux function to store information in the text file\n",
    "        with open(prune_engine.log_debug, 'a') as f:\n",
    "            f.write(\"{} {}\\n\".format(what_write_name,what_write_value))\n",
    "\n",
    "    def nothing(what_write_name, what_write_value):\n",
    "        pass\n",
    "\n",
    "    #store the mask for future needs\n",
    "    old_mask = copy.deepcopy(prune_engine.pruning_gates)\n",
    "\n",
    "    if prune_engine.method == 50:\n",
    "        write_to_debug = nothing\n",
    "\n",
    "    if not (not torch.distributed.is_initialized() or torch.distributed.get_rank()==0):\n",
    "        write_to_debug = nothing\n",
    "\n",
    "    # compute loss since the last pruning and decide if to prune:\n",
    "    if prune_engine.util_loss_tracker_num > 0:\n",
    "        # validation_error = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        validation_loss = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        # validation_error_long = validation_error\n",
    "        acc = prune_engine.util_acc_tracker / prune_engine.util_loss_tracker_num\n",
    "    else:\n",
    "        print(\"compute loss and run prune_engine.util_add_loss(loss.item()) before running this\")\n",
    "        validation_error = 0.0\n",
    "        acc = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "    prune_engine.util_training_loss = validation_loss\n",
    "    prune_engine.util_training_acc = acc\n",
    "\n",
    "    # reset training loss tracker\n",
    "    prune_engine.util_loss_tracker = 0.0\n",
    "    prune_engine.util_acc_tracker = 0.0\n",
    "    prune_engine.util_loss_tracker_num = 0\n",
    "\n",
    "\n",
    "    if (validation_loss > prune_engine.pruning_threshold) and (prune_engine.pruning_threshold != -1.0):\n",
    "        ## if error is big then skip pruning\n",
    "        print(\"skipping pruning because current loss is: \", validation_loss, \"while limit is set to\", prune_engine.pruning_threshold)\n",
    "        if prune_engine.method != 4:\n",
    "            prune_engine.res_pruning = -1\n",
    "            return -1\n",
    "\n",
    "    if prune_engine.maximum_pruning_iterations <= prune_engine.pruning_iterations_done:\n",
    "        # if reached max number of pruning iterations -> exit\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "    prune_engine.full_list_of_criteria = list()\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "        if prune_engine.iterations_done > 0:\n",
    "            # momentum turned to be useless and even reduces performance\n",
    "            contribution = prune_engine.prune_network_accomulate[\"by_layer\"][layer] / prune_engine.iterations_done\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if len(prune_engine.prune_network_accomulate[\"averaged\"][layer])==0 or not prune_engine.use_momentum or (prune_engine.method in [4, 40, 50, 25]):\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = contribution\n",
    "            else:\n",
    "                # use momentum to accumulate criteria over several pruning iterations:\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = prune_engine.momentum_coeff*prune_engine.prune_network_accomulate[\"averaged\"][layer]+(1.0- prune_engine.momentum_coeff)*contribution\n",
    "\n",
    "            current_layer = prune_engine.prune_network_accomulate[\"averaged\"][layer]\n",
    "            if not (prune_engine.method in [1, 4, 40, 15, 50, 25]):\n",
    "                current_layer = current_layer.cpu().numpy()\n",
    "\n",
    "            if prune_engine.l2_normalization_per_layer:\n",
    "                eps = 1e-8\n",
    "                current_layer = current_layer / (np.linalg.norm(current_layer) + eps)\n",
    "\n",
    "            prune_engine.prune_network_accomulate[\"averaged_cpu\"][layer] = current_layer\n",
    "        else:\n",
    "            print(\"First do some add_criteria iterations\")\n",
    "            return -1\n",
    "\n",
    "        for unit in range(len(prune_engine.parameters[layer])):\n",
    "            criterion_now = current_layer[unit]\n",
    "\n",
    "            # make sure that pruned neurons have 0 criteria\n",
    "            if not prune_engine.push_down:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now * prune_engine.pruning_gates[layer][unit]\n",
    "            else:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "            if prune_engine.method == 50:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "    # count number of neurons\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "    prune_engine.neuron_units = neuron_units\n",
    "    prune_engine.all_neuron_units = all_neuron_units\n",
    "\n",
    "    # store criteria_result into file\n",
    "    if not prune_engine.pruning_silent:\n",
    "\n",
    "        if not torch.distributed.is_initialized() or torch.distributed.get_rank()==0:\n",
    "            import pickle\n",
    "            store_criteria = prune_engine.prune_network_accomulate[\"averaged_cpu\"]\n",
    "            pickle.dump(store_criteria, open(prune_engine.folder_to_write_debug + \"criteria_%04d.pickle\"%prune_engine.pruning_iterations_done, \"wb\"))\n",
    "            if prune_engine.pruning_iterations_done == 0:\n",
    "                pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d.pickle\"%prune_engine.method, \"wb\"))\n",
    "            pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d_final.pickle\"%prune_engine.method, \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "    if not prune_engine.fixed_criteria:\n",
    "        prune_engine.iterations_done = 0\n",
    "\n",
    "    prune_network_criteria_updated = prune_engine.prune_network_criteria\n",
    "\n",
    "    # Compute current model statistic\n",
    "    model_dim = list()\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "        if layer == 0:\n",
    "            model_dim.append(np.nonzero(prune_engine.pruning_gates[layer])[0].size)\n",
    "        else:\n",
    "            if layer%4 == 1:\n",
    "                head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 2:\n",
    "                qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 3:\n",
    "                v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 0:\n",
    "                mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                model_dim.append({'head':head,'QK':qk,'V':v,'MLP':mlp})\n",
    "    \n",
    "    return model_dim\n",
    "    # Compute latency and adjust importance\n",
    "    if prune_engine.latency_regularization:\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer==0 and not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                latency_improve = 0.\n",
    "                for blk in range(12):\n",
    "                    qk_head = model_dim[blk+1]['head']\n",
    "                    qk = model_dim[blk+1]['QK']\n",
    "                    v = model_dim[blk+1]['V']\n",
    "                    mlp = model_dim[blk+1]['MLP']\n",
    "                    latency_improve += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb-1,qk_head,qk,v,mlp)\n",
    "                pc = np.array(prune_network_criteria_updated[layer])\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "            elif not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                qk_head = model_dim[(layer-1)//4+1]['head']\n",
    "                qk = model_dim[(layer-1)//4+1]['QK']\n",
    "                v = model_dim[(layer-1)//4+1]['V']\n",
    "                mlp = model_dim[(layer-1)//4+1]['MLP']\n",
    "                latency_improve = 0.\n",
    "                if layer%4 == 1 and qk_head>2:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head-1,qk,v,mlp)\n",
    "                elif layer%4 == 2 and qk>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk-1,v,mlp)\n",
    "                elif layer%4 == 3 and v>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v-1,mlp)\n",
    "                elif layer%4 == 0 and mlp>16:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v,mlp-1)\n",
    "\n",
    "                pc = np.array(prune_network_criteria_updated[layer])#+1\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "\n",
    "\n",
    "    # create groups per layer\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "\n",
    "    # get an array of all criteria from groups\n",
    "    all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)\n",
    "    # from IPython import embed; embed()\n",
    "\n",
    "    prune_neurons_now = (prune_engine.pruning_iterations_done * prune_engine.prune_per_iteration)//prune_engine.group_size - 1\n",
    "    if prune_engine.push_down:\n",
    "        removed_gates = sum([(a==0.0).sum() for a in prune_engine.pruning_gates])\n",
    "        prune_additionally = prune_engine.prune_per_iteration\n",
    "        prune_neurons_now = (removed_gates + prune_additionally) // prune_engine.group_size - 1\n",
    "\n",
    "    if prune_engine.prune_neurons_max != -1:\n",
    "        prune_neurons_now = max(0,min(len(all_criteria)-1, min(prune_neurons_now, prune_engine.prune_neurons_max//prune_engine.group_size - 1)))\n",
    "\n",
    "    if prune_engine.push_down:\n",
    "        prune_engine.reset_gates_to_1()\n",
    "\n",
    "    # adaptively estimate threshold given a number of neurons to be removed\n",
    "    threshold_now = np.sort(all_criteria)[prune_neurons_now]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if np.isnan(threshold_now):\n",
    "        print(\"skipping\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "    prune_engine.pruning_iterations_done += 1\n",
    "\n",
    "    prune_engine.log_debug = prune_engine.folder_to_write_debug + 'debugOutput_pruning_%08d' % (\n",
    "        prune_engine.pruning_iterations_done) + '.txt'\n",
    "    write_to_debug(\"method\", prune_engine.method)\n",
    "    write_to_debug(\"pruned_neurons\", prune_engine.pruned_neurons)\n",
    "    write_to_debug(\"pruning_iterations_done\", prune_engine.pruning_iterations_done)\n",
    "    write_to_debug(\"neuron_units\", neuron_units)\n",
    "    write_to_debug(\"all_neuron_units\", all_neuron_units)\n",
    "    write_to_debug(\"threshold_now\", threshold_now)\n",
    "    write_to_debug(\"groups_total\", sum([len(layer) for layer in groups]))\n",
    "    write_to_debug(\"uniquegroups_total\", sum([len(layer) for layer in unique_groups]))\n",
    "\n",
    "    if prune_engine.pruning_iterations_done < prune_engine.start_pruning_after_n_iterations:\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        write_to_debug(\"\\nLayer:\", layer)\n",
    "        write_to_debug(\"units:\", len(prune_engine.parameters[layer]))\n",
    "        write_to_debug(\"layers_group:\", prune_engine.layers_group[layer])\n",
    "\n",
    "        shapes = [\" \".join([\"{}\".format(a[\"parameter\"].shape) for a in prune_engine.pruning_parameters[layer][\"compute_criteria_from\"]])]\n",
    "        write_to_debug(\"layers_shapes:\", shapes)\n",
    "        for a in prune_engine.pruning_parameters[layer][\"compute_criteria_from\"]:\n",
    "            write_to_debug(\"compute_criteria_from:\", \"{}, dim : {}\".format(a[\"parameter_name\"], a[\"dim\"]))\n",
    "        for a in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "            write_to_debug(\"set_to_zero:\", \"{}, dim : {}\".format(a[\"parameter_name\"], a[\"dim\"]))\n",
    "\n",
    "\n",
    "        if prune_engine.prune_per_iteration == 0:\n",
    "            continue\n",
    "\n",
    "        total_groups_in_layer = len(groups[layer])\n",
    "        zeroed_groups = 0\n",
    "\n",
    "        for group in groups[layer]:\n",
    "            if group[1] <= threshold_now:\n",
    "\n",
    "                #add skip if all groups are set to zero in the current layer:\n",
    "                if (zeroed_groups >= total_groups_in_layer-1) and prune_engine.leave_at_least_one_group:\n",
    "                    print(\"PRUNING: skipping the group because others are zero\")\n",
    "                    continue\n",
    "\n",
    "                zeroed_groups += 1\n",
    "                for unit in group[0]:\n",
    "                    # do actual pruning\n",
    "                    if prune_engine.leave_at_least_one_group and (prune_engine.pruning_gates[layer].sum()<=1):\n",
    "                        print(\"PRUNING: skipping setting the last neuron to zero\")\n",
    "                        continue\n",
    "\n",
    "                    prune_engine.pruning_gates[layer][unit] *= 0.0\n",
    "\n",
    "\n",
    "                    if not prune_engine.push_down:\n",
    "                        for param in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "\n",
    "                            if check_allow_trim(param):\n",
    "                                in_the_range = unit + param[\"shift\"] < param[\"parameter\"].data.shape[param[\"dim\"]]\n",
    "                                if (not in_the_range) or not(unit + param[\"shift\"] >= 0):\n",
    "                                    continue\n",
    "\n",
    "                            if param[\"dim\"] == 0:\n",
    "                                param[\"parameter\"].data[unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 1:\n",
    "                                param[\"parameter\"].data[:, unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 2:\n",
    "                                param[\"parameter\"].data[:, :, unit + param[\"shift\"]] *= 0.0\n",
    "\n",
    "        write_to_debug(\"pruned_perc:\", [np.nonzero(1.0-prune_engine.pruning_gates[layer])[0].size, len(prune_engine.pruning_gates[layer])])\n",
    "\n",
    "    # count number of neurons\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        model_dim = np.zeros((1,49))\n",
    "        latency = 0.\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer == 0:\n",
    "                model_dim[0,0] = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            else:\n",
    "                if layer%4 == 1:\n",
    "                    qk_head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+1] = qk_head\n",
    "                elif layer%4 == 2:\n",
    "                    qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+13] = qk\n",
    "                elif layer%4 == 3:\n",
    "                    v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+25] = v\n",
    "                elif layer%4 == 0:\n",
    "                    mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+36] = mlp\n",
    "                    latency += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)\n",
    "\n",
    "        prune_engine.current_latency = latency\n",
    "\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "\n",
    "    prune_engine.pruned_neurons = all_neuron_units-neuron_units\n",
    "\n",
    "    if prune_engine.method == 25:\n",
    "        prune_engine.method_25_first_done = True\n",
    "\n",
    "    prune_engine.threshold_now = threshold_now\n",
    "    try:\n",
    "        prune_engine.min_criteria_value = (all_criteria[all_criteria > 0.0]).min()\n",
    "        prune_engine.max_criteria_value = (all_criteria[all_criteria > 0.0]).max()\n",
    "        prune_engine.median_criteria_value = np.median(all_criteria[all_criteria > 0.0])\n",
    "\n",
    "        prune_engine.min_max_crit_stats =list()\n",
    "        for layer_id, layer in enumerate(unique_groups):\n",
    "            criterias_group = np.asarray([group[1] for group in layer])\n",
    "            min_c = criterias_group[criterias_group>0.0].min()\n",
    "            max_c = criterias_group[criterias_group>0.0].max()\n",
    "            mean_c = criterias_group[criterias_group>0.0].mean()\n",
    "            prune_engine.min_max_crit_stats.append({\"min\": min_c, \"max\": max_c, \"mean_c\": mean_c})\n",
    "\n",
    "    except:\n",
    "        prune_engine.min_criteria_value = 0.0\n",
    "        prune_engine.max_criteria_value = 0.0\n",
    "        prune_engine.median_criteria_value = 0.0\n",
    "\n",
    "    #get overlap\n",
    "    prune_engine.overlap_score = prune_engine.compute_mask_overlap(old_mask, prune_engine.pruning_gates)\n",
    "\n",
    "    # set result to successful\n",
    "    prune_engine.res_pruning = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/workspace/alex/NViT/nvit/pre_compute_saliency.pkl\", 'rb') as f:\n",
    "    prune_engine = pickle.load(f)\n",
    "model_dim = compute_saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_engine.res_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print(len(prune_engine.prune_network_criteria))\n",
    "for layer, scores in enumerate(prune_engine.prune_network_criteria):\n",
    "    print(layer)\n",
    "    print(len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prune_engine.prune_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[768,\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072},\n",
       " {'head': 12, 'QK': 64, 'V': 64, 'MLP': 3072}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
