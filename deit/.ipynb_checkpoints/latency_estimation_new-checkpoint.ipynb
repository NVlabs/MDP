{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKV_s(nn.Module):\n",
    "    def __init__(self, emb, qk, v, qkv_bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Q = nn.Linear(emb, qk, bias=qkv_bias)\n",
    "        self.K = nn.Linear(emb, qk, bias=qkv_bias)\n",
    "        self.V = nn.Linear(emb, v, bias=qkv_bias)\n",
    "        self.token_mask = nn.Parameter(torch.ones(198, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.Q(x)*self.token_mask\n",
    "        k = self.K(x)*self.token_mask\n",
    "        v = self.V(x)*self.token_mask\n",
    "        return q,k,v\n",
    "\n",
    "class ATT(nn.Module):\n",
    "    def __init__(self, attn_drop=0., scale=None):\n",
    "        super().__init__()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        attn_r = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn_r.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        return attn\n",
    "\n",
    "class PROJ(nn.Module):\n",
    "    def __init__(self, v, dim, proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.v = v\n",
    "        self.proj = nn.Linear(v, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, attn, v):\n",
    "        x = (attn @ v).transpose(1, 2).reshape(-1, 198, self.v)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, qk, v, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        head_dim = dim // self.num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = 64 ** -0.5\n",
    "\n",
    "        self.qkv = QKV_s(dim, qk, v)\n",
    "        self.att = ATT(attn_drop,self.scale)\n",
    "        self.proj = PROJ(v, dim, proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q, k, v = self.qkv(x)\n",
    "        qk_dim = q.shape[2]\n",
    "        v_dim = v.shape[2]\n",
    "        q = q.reshape(B, N, self.num_heads, qk_dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.num_heads, qk_dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.num_heads, v_dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = self.att(q,k)\n",
    "        x = self.proj(attn,v)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QK(nn.Module):\n",
    "    def __init__(self, emb, qk, head, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.qk_dim = qk * head\n",
    "        self.Q = nn.Linear(emb, self.qk_dim, bias=qkv_bias)\n",
    "        self.K = nn.Linear(emb, self.qk_dim, bias=qkv_bias)\n",
    "        self.scale = head ** -0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        print(q.shape, k.shape)\n",
    "        qk_dim = q.shape[2]\n",
    "        q = q.reshape(B, N, self.head, qk_dim // self.head).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.head, qk_dim // self.head).permute(0, 2, 1, 3)\n",
    "        attn_r = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn_r.softmax(dim=-1)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_QK_input = torch.zeros((128, 198, 768))\n",
    "dummy_QK_input = dummy_QK_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_QK_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QK_model = QK(emb=768, qk=64, head=12)\n",
    "QK_model = QK_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 198, 768]) torch.Size([128, 198, 768])\n"
     ]
    }
   ],
   "source": [
    "attn = QK_model(dummy_QK_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 12, 198, 198])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V + PROJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V_AND_PROJ(nn.Module):\n",
    "    def __init__(self, emb, v, head, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.v_dim = v * head\n",
    "        self.V = nn.Linear(emb, self.v_dim, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(self.v_dim, emb)\n",
    "        self.head = head\n",
    "        \n",
    "    def forward(self, x, attn):\n",
    "        B, N, C = x.shape\n",
    "        v = self.V(x)\n",
    "        v = v.reshape(B, N, self.head, self.v_dim // self.head).permute(0, 2, 1, 3)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(-1, 198, self.v_dim)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_V_input = torch.zeros((256, 198, 16))\n",
    "dummy_V_input = dummy_V_input.cuda()\n",
    "dummy_attn = torch.zeros((256, 12, 198, 198))\n",
    "dummy_attn = dummy_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_AND_PROJ_MODEL = V_AND_PROJ(emb=16, v=64, head=12)\n",
    "V_AND_PROJ_MODEL = V_AND_PROJ_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = V_AND_PROJ_MODEL(dummy_V_input, dummy_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 198, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From PyTorch internals\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
    "            return tuple(x)\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_1tuple = _ntuple(1)\n",
    "to_2tuple = _ntuple(2)\n",
    "to_3tuple = _ntuple(3)\n",
    "to_4tuple = _ntuple(4)\n",
    "to_ntuple = _ntuple\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            hidden_features=None,\n",
    "            out_features=None,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=None,\n",
    "            bias=True,\n",
    "            drop=0.,\n",
    "            use_conv=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n",
    "\n",
    "        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n",
    "        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_MODEL = Mlp(768, 3072)\n",
    "MLP_MODEL = MLP_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mlp(\n",
       "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (act): GELU()\n",
       "  (drop1): Dropout(p=0.0, inplace=False)\n",
       "  (norm): Identity()\n",
       "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (drop2): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_mlp_input = torch.zeros((128, 198, 768))\n",
    "dummy_mlp_input = dummy_mlp_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_MODEL(dummy_mlp_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST(nn.Module):\n",
    "    def __init__(self, emb, v, head, qkv_bias=True):\n",
    "        super().__init__()\n",
    "#         self.qkv = nn.Linear(emb, emb * 3, bias=qkv_bias)\n",
    "        self.qkv = nn.Linear(emb*head, emb*head*3, bias=qkv_bias)\n",
    "        self.V = MHLinear(emb, v, head, bias=qkv_bias)\n",
    "        self.proj = MHTLinear(v, emb, head)\n",
    "        self.head = head\n",
    "        \n",
    "    def forward(self, x, attn):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.reshape(B, N, 3, self.head, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        print(attn.shape)\n",
    "        print(v.shape)\n",
    "        x = attn @ v\n",
    "# #         print(x.shape)\n",
    "#         v = self.V(x)\n",
    "#         v = v.reshape(B, N, 1, self.head, -1).permute(2, 0, 3, 1, 4)\n",
    "#         v = v[0]\n",
    "#         print(v.shape)\n",
    "#         print(attn.shape)\n",
    "#         x = attn @ v\n",
    "#         print(x.shape)\n",
    "# #         print(x.transpose(1, 2).shape)\n",
    "# #         print(B, N, C)\n",
    "#         x = x.transpose(1, 2).reshape(B, N, C)\n",
    "#         x = self.proj(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_V_input = torch.zeros((256, 198, 16))\n",
    "dummy_V_input = dummy_V_input.cuda()\n",
    "dummy_attn = torch.zeros((256, 12, 198, 198))\n",
    "dummy_attn = dummy_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL = TEST(emb=16, v=64, head=12)\n",
    "TEST_MODEL = TEST_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50688x16 and 192x576)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-f874e19de2df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEST_MODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_V_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-0181087f28a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50688x16 and 192x576)"
     ]
    }
   ],
   "source": [
    "TEST_MODEL(dummy_V_input, dummy_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1824768"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256 * 198 * 3 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
