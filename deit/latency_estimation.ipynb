{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from itertools import repeat\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHLinear(nn.Module): # Multihead at output\n",
    "    def __init__(self, in_features, out_features, head, bias=True):\n",
    "        super(MHLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.head = head\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features,in_features,head))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features,head))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = rearrange(self.weight, 'q e h -> (h q) e')\n",
    "        bias = rearrange(self.bias, 'q h -> (h q)')\n",
    "        return nn.functional.linear(input, weight, bias)\n",
    "    \n",
    "class MHTLinear(nn.Module): # Multihead at input\n",
    "    def __init__(self, in_features, out_features, head, bias=True):\n",
    "        super(MHTLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.head = head\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features,in_features,head))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        weight = rearrange(self.weight, 'e v h -> e (h v)')\n",
    "        return nn.functional.linear(input, weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QK(nn.Module):\n",
    "    def __init__(self, emb, qk, head, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.Q = MHLinear(emb, qk, head, bias=qkv_bias)\n",
    "        self.K = MHLinear(emb, qk, head, bias=qkv_bias)\n",
    "        self.scale = head ** -0.5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        print(q.shape, k.shape)\n",
    "        x = torch.cat((q,k),-1)\n",
    "        qk = x.reshape(B, N, 2, self.head, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k = qk.unbind(0)\n",
    "        \n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_QK_input = torch.zeros((128, 198, 768))\n",
    "dummy_QK_input = dummy_QK_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_QK_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "QK_model = QK(emb=768, qk=64, head=12)\n",
    "QK_model = QK_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768, 12])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_model.Q.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 198, 768]) torch.Size([128, 198, 768])\n"
     ]
    }
   ],
   "source": [
    "attn = QK_model(dummy_QK_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 12, 198, 198])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class V(nn.Module):\n",
    "#     def __init__(self, emb, v, head, qkv_bias=True):\n",
    "#         super().__init__()\n",
    "#         self.V = MHLinear(emb, v, head, bias=qkv_bias)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         v = self.V(x)\n",
    "#         return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_V_input = torch.zeros((128, 198, 768))\n",
    "# dummy_V_input = dummy_V_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy_QK_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_model = V(emb=768, v=64, head=12)\n",
    "# V_model = V_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = V_model(dummy_V_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 198, 768])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V + PROJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V_AND_PROJ(nn.Module):\n",
    "    def __init__(self, emb, v, head, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.V = MHLinear(emb, v, head, bias=qkv_bias)\n",
    "        self.proj = MHTLinear(v, emb, head)\n",
    "        self.head = head\n",
    "        \n",
    "    def forward(self, x, attn):\n",
    "        B, N, C = x.shape\n",
    "#         print(x.shape)\n",
    "        v = self.V(x)\n",
    "        v = v.reshape(B, N, 1, self.head, -1).permute(2, 0, 3, 1, 4)\n",
    "        v = v[0]\n",
    "        print(v.shape)\n",
    "        print(attn.shape)\n",
    "        x = attn @ v\n",
    "        print(x.shape)\n",
    "#         print(x.transpose(1, 2).shape)\n",
    "#         print(B, N, C)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_V_input = torch.zeros((256, 198, 16))\n",
    "dummy_V_input = dummy_V_input.cuda()\n",
    "dummy_attn = torch.zeros((256, 12, 198, 198))\n",
    "dummy_attn = dummy_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_V_input = torch.zeros((576, 198, 768))\n",
    "# dummy_V_input = dummy_V_input.cuda()\n",
    "# dummy_attn = torch.zeros((576, 198, 198))\n",
    "# dummy_attn = dummy_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_AND_PROJ_MODEL = V_AND_PROJ(emb=768, v=64, head=12)\n",
    "# V_AND_PROJ_MODEL = V_AND_PROJ_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_AND_PROJ_MODEL = V_AND_PROJ(emb=16, v=64, head=12)\n",
    "V_AND_PROJ_MODEL = V_AND_PROJ_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 12])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_AND_PROJ_MODEL.V.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 12, 198, 64])\n",
      "torch.Size([256, 12, 198, 198])\n",
      "torch.Size([256, 12, 198, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[256, 198, 16]' is invalid for input of size 38928384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-bc2f7fae7a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_AND_PROJ_MODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_V_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-033a2741c9f5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         print(x.transpose(1, 2).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print(B, N, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[256, 198, 16]' is invalid for input of size 38928384"
     ]
    }
   ],
   "source": [
    "x = V_AND_PROJ_MODEL(dummy_V_input, dummy_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 198, 768])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From PyTorch internals\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
    "            return tuple(x)\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "\n",
    "to_1tuple = _ntuple(1)\n",
    "to_2tuple = _ntuple(2)\n",
    "to_3tuple = _ntuple(3)\n",
    "to_4tuple = _ntuple(4)\n",
    "to_ntuple = _ntuple\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            hidden_features=None,\n",
    "            out_features=None,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=None,\n",
    "            bias=True,\n",
    "            drop=0.,\n",
    "            use_conv=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear\n",
    "\n",
    "        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()\n",
    "        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_MODEL = Mlp(768, 3072)\n",
    "MLP_MODEL = MLP_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mlp(\n",
       "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (act): GELU()\n",
       "  (drop1): Dropout(p=0.0, inplace=False)\n",
       "  (norm): Identity()\n",
       "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (drop2): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_mlp_input = torch.zeros((128, 198, 768))\n",
    "dummy_mlp_input = dummy_mlp_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]],\n",
       "\n",
       "        [[ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         ...,\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202],\n",
       "         [ 0.0275, -0.0090,  0.0120,  ...,  0.0070,  0.0174, -0.0202]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_MODEL(dummy_mlp_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST(nn.Module):\n",
    "    def __init__(self, emb, v, head, qkv_bias=True):\n",
    "        super().__init__()\n",
    "#         self.qkv = nn.Linear(emb, emb * 3, bias=qkv_bias)\n",
    "        self.qkv = nn.Linear(emb*head, emb*head*3, bias=qkv_bias)\n",
    "        self.V = MHLinear(emb, v, head, bias=qkv_bias)\n",
    "        self.proj = MHTLinear(v, emb, head)\n",
    "        self.head = head\n",
    "        \n",
    "    def forward(self, x, attn):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.reshape(B, N, 3, self.head, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        print(attn.shape)\n",
    "        print(v.shape)\n",
    "        x = attn @ v\n",
    "# #         print(x.shape)\n",
    "#         v = self.V(x)\n",
    "#         v = v.reshape(B, N, 1, self.head, -1).permute(2, 0, 3, 1, 4)\n",
    "#         v = v[0]\n",
    "#         print(v.shape)\n",
    "#         print(attn.shape)\n",
    "#         x = attn @ v\n",
    "#         print(x.shape)\n",
    "# #         print(x.transpose(1, 2).shape)\n",
    "# #         print(B, N, C)\n",
    "#         x = x.transpose(1, 2).reshape(B, N, C)\n",
    "#         x = self.proj(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_V_input = torch.zeros((256, 198, 16))\n",
    "dummy_V_input = dummy_V_input.cuda()\n",
    "dummy_attn = torch.zeros((256, 12, 198, 198))\n",
    "dummy_attn = dummy_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL = TEST(emb=16, v=64, head=12)\n",
    "TEST_MODEL = TEST_MODEL.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50688x16 and 192x576)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-f874e19de2df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEST_MODEL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_V_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-0181087f28a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50688x16 and 192x576)"
     ]
    }
   ],
   "source": [
    "TEST_MODEL(dummy_V_input, dummy_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1824768"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256 * 198 * 3 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
