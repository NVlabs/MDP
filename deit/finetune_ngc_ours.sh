# python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 300 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /result/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_Global_39000_lat_0.0005_target_0.54_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002

python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 100 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /result/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_50x200_Global_39000_lat_0.0005_target_0.39_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002

python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 100 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /result/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_50x200_Global_39000_lat_0.0005_target_0.19_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002


python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 100 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /result/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_30x1000_Global_39000_lat_0.0005_target_0.19_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002


# 300
python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 300 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /workspace/alex/mdp_transformer_results/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_50x200_Global_39000_lat_0.0005_target_0.19_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002

python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 300 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /workspace/alex/mdp_transformer_results/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_50x200_Global_39000_lat_0.0005_target_0.39_ft_50 --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002

# FLOPS as Targets
python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 100 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /workspace/alex/mdp_transformer_results/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_FLOPs_50x200_Global_39000_lat_0.0005_target_0.23_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002

python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 100 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /workspace/alex/mdp_transformer_results/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_FLOPs_50x200_Global_39000_lat_0.0005_target_0.07_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002


python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 300 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /workspace/alex/mdp_transformer_results/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_FLOPs_50x200_Global_39000_lat_0.0005_target_0.23_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002

python -m torch.distributed.launch --nproc_per_node=8 --use_env finetune_dense.py --model deit_base_distilled_patch16_224 --epochs 300 --num_workers 10 --batch-size 144 --data-path /raid/ImageNet2012/ImageNet2012 --data-set IMNET --amp --input-size 224 --seed 1 --kl_loss_coeff=100000 --original_loss_coeff=1.0 --dist-eval --pretrained --finetune /workspace/alex/mdp_transformer_results/global_prune_IMNET_deit_base_distilled_patch16_224_lr0.0001_FLOPs_50x200_Global_39000_lat_0.0005_target_0.07_ft_50/ --distillation-type hard --distillation-alpha 0.5 --distillation-tau 20.0 --lr 0.0002