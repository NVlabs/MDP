{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_allow_trim(dict):\n",
    "    res = False\n",
    "    if \"allow_trim\" in dict.keys():\n",
    "        if dict[\"allow_trim\"]:\n",
    "            res = True\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_criteria(list_criteria_per_layer, layers_group, group_size=1):\n",
    "    '''\n",
    "    Function combine criteria per neuron into groups of size group_size.\n",
    "    Output is a list of groups organized by layers. Length of output is a number of layers.\n",
    "    The criterion for the group is computed as an average of member's criteria.\n",
    "    Input:\n",
    "    list_criteria_per_layer - list of criteria per neuron organized per layer\n",
    "    group_size - number of neurons per group\n",
    "    layers_group - layers can form a group, e.g. residual connection, they will be pruned together\n",
    "\n",
    "    Output:\n",
    "    groups - groups organized per layer. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    groups_unique - groups organized per UNIQUE layers only. Each group element is a tuple of 2: (index of neurons, criterion)\n",
    "    '''\n",
    "    assert len(list_criteria_per_layer) == len(layers_group)\n",
    "    groups = list()\n",
    "\n",
    "    for layer_indx, layer_criteria in enumerate(list_criteria_per_layer):\n",
    "        layer = layer_criteria\n",
    "\n",
    "        if layer_indx == 0:\n",
    "            group_size = 16\n",
    "        else:\n",
    "            if layer_indx%4 == 1:\n",
    "                group_size = 1\n",
    "            elif layer_indx%4 == 2:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 3:\n",
    "                group_size = 2\n",
    "            elif layer_indx%4 == 0:\n",
    "                group_size = 32\n",
    "\n",
    "        if layers_group[layer_indx] != -1:\n",
    "\n",
    "            #if layer/parameter is a part of the group\n",
    "            #then we aggregate importance across the group\n",
    "            #this procedure is repeated for each layer/parameter in the group\n",
    "\n",
    "            all_criteria = np.asarray([lc for li, lc in enumerate(list_criteria_per_layer) if layers_group[layer_indx]==layers_group[li]])\n",
    "\n",
    "            all_criteria = all_criteria.sum(0)\n",
    "            layer = all_criteria\n",
    "\n",
    "        groups_in_layer = list()\n",
    "        indeces = np.argsort(layer)\n",
    "        for group_id in range(int(np.ceil(len(layer)/group_size))):\n",
    "            current_group = slice(group_id*group_size, min((group_id+1)*group_size, len(layer)))\n",
    "            values = [layer[ind] for ind in indeces[current_group]]\n",
    "            group = [indeces[current_group], sum(values)]\n",
    "\n",
    "            groups_in_layer.append(group)\n",
    "        groups.append(groups_in_layer)\n",
    "\n",
    "    if all( [l==-1 for l in layers_group]):\n",
    "        #group with index -1 means no group\n",
    "        unique_groups = groups\n",
    "    else:\n",
    "        unique_groups = list()\n",
    "        groups_exists = list()\n",
    "        for gi, g in enumerate(groups):\n",
    "            if (layers_group[gi] == -1) or (layers_group[gi] not in groups_exists):\n",
    "                unique_groups.append(g)\n",
    "                groups_exists.append(layers_group[gi])\n",
    "\n",
    "    # if torch.distributed.get_rank() == 0:\n",
    "    #     import pdb;pdb.set_trace()\n",
    "\n",
    "    return groups, unique_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency():\n",
    "    def write_to_debug(what_write_name, what_write_value):\n",
    "        # Aux function to store information in the text file\n",
    "        with open(prune_engine.log_debug, 'a') as f:\n",
    "            f.write(\"{} {}\\n\".format(what_write_name,what_write_value))\n",
    "\n",
    "    def nothing(what_write_name, what_write_value):\n",
    "        pass\n",
    "\n",
    "    #store the mask for future needs\n",
    "    old_mask = copy.deepcopy(prune_engine.pruning_gates)\n",
    "\n",
    "    \n",
    "    write_to_debug = nothing\n",
    "\n",
    "    # compute loss since the last pruning and decide if to prune:\n",
    "    if prune_engine.util_loss_tracker_num > 0:\n",
    "        # validation_error = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        validation_loss = prune_engine.util_loss_tracker / prune_engine.util_loss_tracker_num\n",
    "        # validation_error_long = validation_error\n",
    "        acc = prune_engine.util_acc_tracker / prune_engine.util_loss_tracker_num\n",
    "    else:\n",
    "        print(\"compute loss and run prune_engine.util_add_loss(loss.item()) before running this\")\n",
    "        validation_error = 0.0\n",
    "        acc = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "    prune_engine.util_training_loss = validation_loss\n",
    "    prune_engine.util_training_acc = acc\n",
    "\n",
    "    # reset training loss tracker\n",
    "    prune_engine.util_loss_tracker = 0.0\n",
    "    prune_engine.util_acc_tracker = 0.0\n",
    "    prune_engine.util_loss_tracker_num = 0\n",
    "\n",
    "\n",
    "    if (validation_loss > prune_engine.pruning_threshold) and (prune_engine.pruning_threshold != -1.0):\n",
    "        ## if error is big then skip pruning\n",
    "        print(\"skipping pruning because current loss is: \", validation_loss, \"while limit is set to\", prune_engine.pruning_threshold)\n",
    "        if prune_engine.method != 4:\n",
    "            prune_engine.res_pruning = -1\n",
    "            return -1\n",
    "\n",
    "    if prune_engine.maximum_pruning_iterations <= prune_engine.pruning_iterations_done:\n",
    "        # if reached max number of pruning iterations -> exit\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "    prune_engine.full_list_of_criteria = list()\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "        if prune_engine.iterations_done > 0:\n",
    "            # momentum turned to be useless and even reduces performance\n",
    "            contribution = prune_engine.prune_network_accomulate[\"by_layer\"][layer] / prune_engine.iterations_done\n",
    "            # import pdb; pdb.set_trace()\n",
    "            if len(prune_engine.prune_network_accomulate[\"averaged\"][layer])==0 or not prune_engine.use_momentum or (prune_engine.method in [4, 40, 50, 25]):\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = contribution\n",
    "            else:\n",
    "                # use momentum to accumulate criteria over several pruning iterations:\n",
    "                prune_engine.prune_network_accomulate[\"averaged\"][layer] = prune_engine.momentum_coeff*prune_engine.prune_network_accomulate[\"averaged\"][layer]+(1.0- prune_engine.momentum_coeff)*contribution\n",
    "\n",
    "            current_layer = prune_engine.prune_network_accomulate[\"averaged\"][layer]\n",
    "            if not (prune_engine.method in [1, 4, 40, 15, 50, 25]):\n",
    "                current_layer = current_layer.cpu().numpy()\n",
    "\n",
    "            if prune_engine.l2_normalization_per_layer:\n",
    "                eps = 1e-8\n",
    "                current_layer = current_layer / (np.linalg.norm(current_layer) + eps)\n",
    "\n",
    "            prune_engine.prune_network_accomulate[\"averaged_cpu\"][layer] = current_layer\n",
    "        else:\n",
    "            print(\"First do some add_criteria iterations\")\n",
    "            return -1\n",
    "\n",
    "        for unit in range(len(prune_engine.parameters[layer])):\n",
    "            criterion_now = current_layer[unit]\n",
    "\n",
    "            # make sure that pruned neurons have 0 criteria\n",
    "            if not prune_engine.push_down:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now * prune_engine.pruning_gates[layer][unit]\n",
    "            else:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "            if prune_engine.method == 50:\n",
    "                prune_engine.prune_network_criteria[layer][unit] =  criterion_now\n",
    "\n",
    "    # count number of neurons\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "    prune_engine.neuron_units = neuron_units\n",
    "    prune_engine.all_neuron_units = all_neuron_units\n",
    "\n",
    "    # store criteria_result into file\n",
    "    if not prune_engine.pruning_silent:\n",
    "\n",
    "        if not torch.distributed.is_initialized() or torch.distributed.get_rank()==0:\n",
    "            import pickle\n",
    "            store_criteria = prune_engine.prune_network_accomulate[\"averaged_cpu\"]\n",
    "            pickle.dump(store_criteria, open(prune_engine.folder_to_write_debug + \"criteria_%04d.pickle\"%prune_engine.pruning_iterations_done, \"wb\"))\n",
    "            if prune_engine.pruning_iterations_done == 0:\n",
    "                pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d.pickle\"%prune_engine.method, \"wb\"))\n",
    "            pickle.dump(store_criteria, open(prune_engine.log_folder + \"criteria_%d_final.pickle\"%prune_engine.method, \"wb\"))\n",
    "\n",
    "\n",
    "\n",
    "    if not prune_engine.fixed_criteria:\n",
    "        prune_engine.iterations_done = 0\n",
    "\n",
    "    prune_network_criteria_updated = prune_engine.prune_network_criteria\n",
    "\n",
    "    # Compute current model statistic\n",
    "    model_dim = list()\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "        if layer == 0:\n",
    "            model_dim.append(np.nonzero(prune_engine.pruning_gates[layer])[0].size)\n",
    "        else:\n",
    "            if layer%4 == 1:\n",
    "                head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 2:\n",
    "                qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 3:\n",
    "                v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            elif layer%4 == 0:\n",
    "                mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                model_dim.append({'head':head,'QK':qk,'V':v,'MLP':mlp})\n",
    "    \n",
    "    # create groups per layer\n",
    "    # comment later\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "#     return groups, unique_groups, prune_network_criteria_updated\n",
    "\n",
    "    # Compute latency and adjust importance\n",
    "    if prune_engine.latency_regularization:\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer==0 and not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                latency_improve = 0.\n",
    "                for blk in range(12):\n",
    "                    qk_head = model_dim[blk+1]['head']\n",
    "                    qk = model_dim[blk+1]['QK']\n",
    "                    v = model_dim[blk+1]['V']\n",
    "                    mlp = model_dim[blk+1]['MLP']\n",
    "                    latency_improve += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb-1,qk_head,qk,v,mlp)\n",
    "                pc = np.array(prune_network_criteria_updated[layer])\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "            elif not prune_engine.pruning_parameters[layer][\"compute_criteria_from\"][0]['fix']:\n",
    "                emb = model_dim[0]\n",
    "                qk_head = model_dim[(layer-1)//4+1]['head']\n",
    "                qk = model_dim[(layer-1)//4+1]['QK']\n",
    "                v = model_dim[(layer-1)//4+1]['V']\n",
    "                mlp = model_dim[(layer-1)//4+1]['MLP']\n",
    "                latency_improve = 0.\n",
    "                if layer%4 == 1 and qk_head>2:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head-1,qk,v,mlp)\n",
    "                elif layer%4 == 2 and qk>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk-1,v,mlp)\n",
    "                elif layer%4 == 3 and v>8:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v-1,mlp)\n",
    "                elif layer%4 == 0 and mlp>16:\n",
    "                    latency_improve = prune_engine.compute_latency(emb,qk_head,qk,v,mlp)-prune_engine.compute_latency(emb,qk_head,qk,v,mlp-1)\n",
    "\n",
    "                pc = np.array(prune_network_criteria_updated[layer])#+1\n",
    "                pc -= prune_engine.latency_regularization*latency_improve\n",
    "                prune_network_criteria_updated[layer] = pc.tolist()\n",
    "\n",
    "\n",
    "    # create groups per layer\n",
    "    groups, unique_groups = prune_engine.group_criteria(prune_network_criteria_updated, layers_group = prune_engine.layers_group, group_size=prune_engine.group_size)\n",
    "\n",
    "    # get an array of all criteria from groups\n",
    "    all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)\n",
    "    # from IPython import embed; embed()\n",
    "\n",
    "    prune_neurons_now = (prune_engine.pruning_iterations_done * prune_engine.prune_per_iteration)//prune_engine.group_size - 1\n",
    "    if prune_engine.push_down:\n",
    "        removed_gates = sum([(a==0.0).sum() for a in prune_engine.pruning_gates])\n",
    "        prune_additionally = prune_engine.prune_per_iteration\n",
    "        prune_neurons_now = (removed_gates + prune_additionally) // prune_engine.group_size - 1\n",
    "\n",
    "    if prune_engine.prune_neurons_max != -1:\n",
    "        prune_neurons_now = max(0,min(len(all_criteria)-1, min(prune_neurons_now, prune_engine.prune_neurons_max//prune_engine.group_size - 1)))\n",
    "\n",
    "    if prune_engine.push_down:\n",
    "        prune_engine.reset_gates_to_1()\n",
    "\n",
    "    # adaptively estimate threshold given a number of neurons to be removed\n",
    "    print(prune_neurons_now)\n",
    "    prune_neurons_now = 100\n",
    "    threshold_now = np.sort(all_criteria)[prune_neurons_now]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if np.isnan(threshold_now):\n",
    "        print(\"skipping\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "    prune_engine.pruning_iterations_done += 1\n",
    "\n",
    "\n",
    "    if prune_engine.pruning_iterations_done < prune_engine.start_pruning_after_n_iterations:\n",
    "        prune_engine.res_pruning = -1\n",
    "        return -1\n",
    "\n",
    "\n",
    "    for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "        if not if_prune:\n",
    "            continue\n",
    "\n",
    "\n",
    "        if prune_engine.prune_per_iteration == 0:\n",
    "            continue\n",
    "\n",
    "        total_groups_in_layer = len(groups[layer])\n",
    "        zeroed_groups = 0\n",
    "\n",
    "        for group in groups[layer]:\n",
    "            if group[1] <= threshold_now:\n",
    "                print(layer, group)\n",
    "                #add skip if all groups are set to zero in the current layer:\n",
    "                if (zeroed_groups >= total_groups_in_layer-1) and prune_engine.leave_at_least_one_group:\n",
    "                    print(\"PRUNING: skipping the group because others are zero\")\n",
    "                    continue\n",
    "\n",
    "                zeroed_groups += 1\n",
    "                for unit in group[0]:\n",
    "                    # do actual pruning\n",
    "                    if prune_engine.leave_at_least_one_group and (prune_engine.pruning_gates[layer].sum()<=1):\n",
    "                        print(\"PRUNING: skipping setting the last neuron to zero\")\n",
    "                        continue\n",
    "\n",
    "                    prune_engine.pruning_gates[layer][unit] *= 0.0\n",
    "\n",
    "\n",
    "                    if not prune_engine.push_down:\n",
    "                        for param in prune_engine.pruning_parameters[layer][\"set_to_zero\"]:\n",
    "\n",
    "                            if check_allow_trim(param):\n",
    "                                in_the_range = unit + param[\"shift\"] < param[\"parameter\"].data.shape[param[\"dim\"]]\n",
    "                                if (not in_the_range) or not(unit + param[\"shift\"] >= 0):\n",
    "                                    continue\n",
    "\n",
    "                            if param[\"dim\"] == 0:\n",
    "                                param[\"parameter\"].data[unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 1:\n",
    "                                param[\"parameter\"].data[:, unit + param[\"shift\"]] *= 0.0\n",
    "                            elif param[\"dim\"] == 2:\n",
    "                                param[\"parameter\"].data[:, :, unit + param[\"shift\"]] *= 0.0\n",
    "\n",
    "        write_to_debug(\"pruned_perc:\", [np.nonzero(1.0-prune_engine.pruning_gates[layer])[0].size, len(prune_engine.pruning_gates[layer])])\n",
    "\n",
    "    # count number of neurons\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        model_dim = np.zeros((1,49))\n",
    "        latency = 0.\n",
    "        for layer, if_prune in enumerate(prune_engine.prune_layers):\n",
    "            if not if_prune:\n",
    "                continue\n",
    "            if layer == 0:\n",
    "                model_dim[0,0] = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "            else:\n",
    "                if layer%4 == 1:\n",
    "                    qk_head = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+1] = qk_head\n",
    "                elif layer%4 == 2:\n",
    "                    qk = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+13] = qk\n",
    "                elif layer%4 == 3:\n",
    "                    v = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+25] = v\n",
    "                elif layer%4 == 0:\n",
    "                    mlp = np.nonzero(prune_engine.pruning_gates[layer])[0].size\n",
    "                    model_dim[0,layer//4+36] = mlp\n",
    "                    latency += prune_engine.compute_latency(emb,qk_head,qk,v,mlp)\n",
    "\n",
    "        prune_engine.current_latency = latency\n",
    "\n",
    "    all_neuron_units, neuron_units = prune_engine._count_number_of_neurons()\n",
    "\n",
    "    prune_engine.pruned_neurons = all_neuron_units-neuron_units\n",
    "\n",
    "    if prune_engine.method == 25:\n",
    "        prune_engine.method_25_first_done = True\n",
    "\n",
    "    prune_engine.threshold_now = threshold_now\n",
    "    try:\n",
    "        prune_engine.min_criteria_value = (all_criteria[all_criteria > 0.0]).min()\n",
    "        prune_engine.max_criteria_value = (all_criteria[all_criteria > 0.0]).max()\n",
    "        prune_engine.median_criteria_value = np.median(all_criteria[all_criteria > 0.0])\n",
    "\n",
    "        prune_engine.min_max_crit_stats =list()\n",
    "        for layer_id, layer in enumerate(unique_groups):\n",
    "            criterias_group = np.asarray([group[1] for group in layer])\n",
    "            min_c = criterias_group[criterias_group>0.0].min()\n",
    "            max_c = criterias_group[criterias_group>0.0].max()\n",
    "            mean_c = criterias_group[criterias_group>0.0].mean()\n",
    "            prune_engine.min_max_crit_stats.append({\"min\": min_c, \"max\": max_c, \"mean_c\": mean_c})\n",
    "\n",
    "    except:\n",
    "        prune_engine.min_criteria_value = 0.0\n",
    "        prune_engine.max_criteria_value = 0.0\n",
    "        prune_engine.median_criteria_value = 0.0\n",
    "\n",
    "    #get overlap\n",
    "    prune_engine.overlap_score = prune_engine.compute_mask_overlap(old_mask, prune_engine.pruning_gates)\n",
    "\n",
    "    # set result to successful\n",
    "    prune_engine.res_pruning = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb, (head, qk, v, mlp) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0 [array([291, 471, 552,  81, 332, 343, 655, 445, 639, 253,  68, 577, 539,\n",
      "       487, 524, 551]), -4.471082928077408e-06]\n",
      "0 [array([637, 277, 517, 212, 617, 131, 233, 631, 404, 556, 502, 615, 174,\n",
      "       114, 419, 451]), -4.313450832256649e-06]\n",
      "0 [array([215, 722, 383, 579, 109, 371, 316,  60,  27, 598,  22, 505, 519,\n",
      "       143,  90, 417]), -4.255369768742412e-06]\n",
      "0 [array([119, 508, 499, 162, 232, 580, 726, 406, 155, 134, 398, 270,  63,\n",
      "       140, 482, 664]), -4.197164747495208e-06]\n",
      "0 [array([501, 547, 112, 738, 695, 274, 113, 348, 128, 154, 701, 293,  51,\n",
      "       136, 731, 659]), -4.129881999233476e-06]\n",
      "0 [array([523, 164, 534, 276, 461,  54, 393, 742, 729, 159, 301,  92, 642,\n",
      "       666, 509, 531]), -4.077673248133351e-06]\n",
      "0 [array([418, 720, 628, 635, 267, 458, 349, 296, 424, 585, 657, 300, 747,\n",
      "         9, 252, 756]), -4.0340965787493134e-06]\n",
      "0 [array([240, 133, 258, 663, 299, 334, 260, 150, 571, 715, 413, 241, 621,\n",
      "        80, 236, 163]), -3.99246691370081e-06]\n",
      "0 [array([572, 120, 680,  61, 466, 228, 275, 245, 124, 202, 152, 725,  83,\n",
      "       171, 739,  84]), -3.947320154480849e-06]\n",
      "0 [array([138, 616, 199, 623, 606, 436, 542, 712, 662, 108, 350, 697, 185,\n",
      "       750, 322, 604]), -3.8977354270457455e-06]\n",
      "0 [array([536, 339, 749,  42, 333,  74, 763, 643, 586,  36, 226, 548, 230,\n",
      "       490, 239, 702]), -3.8429890862516915e-06]\n",
      "0 [array([153,  47, 194, 402, 691, 656, 660, 200, 372, 211, 734, 255, 439,\n",
      "       497, 699,  97]), -3.8058618072227547e-06]\n",
      "0 [array([467, 641, 603, 654, 347, 249, 751, 462,  37, 511, 405, 503, 423,\n",
      "       692, 450, 455]), -3.7715418896482335e-06]\n",
      "0 [array([173, 619, 411, 355, 195, 289,   8, 592, 495,  53, 448, 543, 122,\n",
      "       578, 690, 167]), -3.7285127644963724e-06]\n",
      "0 [array([304, 609, 684, 627, 281, 709, 597, 581, 500, 718,  67, 493, 156,\n",
      "       318, 447, 473]), -3.6898110648735384e-06]\n",
      "0 [array([208,  45, 574, 297,  19, 634, 254,  26, 147,  25, 589,  78, 510,\n",
      "       157, 385, 331]), -3.6392989299542934e-06]\n",
      "0 [array([ 87, 370,  34, 670, 646, 629, 209, 758, 594, 188,  93, 694, 175,\n",
      "       146, 151, 429]), -3.591784631680639e-06]\n",
      "0 [array([689, 588, 719, 569, 104, 246, 360, 427, 361, 464, 395, 311,  65,\n",
      "        96, 149, 178]), -3.540969244227199e-06]\n",
      "0 [array([421, 106, 472,  55, 516, 187, 123, 518, 324, 484, 687, 329, 645,\n",
      "       541, 647, 582]), -3.4994420572047604e-06]\n",
      "0 [array([477, 313, 137, 166, 285, 191, 261,   0, 352, 533, 735, 177, 431,\n",
      "       363,  98, 231]), -3.467936862307397e-06]\n",
      "0 [array([314, 456, 563, 346,  40, 512,  48, 529, 374, 387,  79,  18, 319,\n",
      "       380, 459, 358]), -3.4290671264329832e-06]\n",
      "0 [array([764, 650, 205, 238,  15, 600, 494, 515, 443, 728, 148, 452, 686,\n",
      "       129, 345, 416]), -3.4009306928055595e-06]\n",
      "0 [array([676, 190, 307, 470, 207, 622, 698, 658, 721,  12, 115,  20,   3,\n",
      "       601, 292,  57]), -3.346273152089907e-06]\n",
      "0 [array([760, 632, 724, 182, 620, 457, 446, 244, 683, 479, 492, 575,  86,\n",
      "       608, 422, 336]), -3.3029121750587366e-06]\n",
      "0 [array([568, 362, 630, 271, 409, 555, 282,  69, 403, 682, 243, 468, 216,\n",
      "       325, 644, 696]), -3.2513255005710563e-06]\n",
      "0 [array([221, 506, 229, 481, 265, 573, 399, 288, 176, 716, 280, 356, 557,\n",
      "       491,  72, 103]), -3.208494886848712e-06]\n",
      "0 [array([651, 435, 375, 391, 704,  35, 344,  99, 287, 223,   7, 354,  39,\n",
      "       736, 105, 107]), -3.1484955711107433e-06]\n",
      "0 [array([537, 135, 327,  23, 607, 488,  10, 583, 700, 741, 706, 514, 626,\n",
      "       192, 591, 550]), -3.086582174034902e-06]\n",
      "0 [array([303, 611, 730,  21, 525, 125, 217, 283, 381, 553, 340, 668, 210,\n",
      "       394, 117, 220]), -3.0359521956029313e-06]\n",
      "0 [array([320, 130, 198, 610, 489,  41, 183, 388, 353, 225, 440, 483, 520,\n",
      "       745, 373, 640]), -2.9784693877559222e-06]\n",
      "0 [array([179, 566, 746, 305, 545, 365,  43, 203, 757, 101, 111, 593, 486,\n",
      "       560, 213, 612]), -2.8901356030246463e-06]\n",
      "0 [array([425,  46, 227, 624, 441, 306, 449, 677,  95,  31, 544, 145, 341,\n",
      "       180, 234, 386]), -2.806014647489974e-06]\n",
      "0 [array([160, 268, 605,  89, 561, 102, 337, 535,  85, 596, 369, 256, 294,\n",
      "       672, 197, 351]), -2.7438593674560254e-06]\n",
      "0 [array([504,  94, 279, 189, 754, 272, 142, 478, 317, 116, 132, 242, 193,\n",
      "       717,  32, 100]), -2.635411588698844e-06]\n",
      "0 [array([613, 707, 453, 158, 602,  28, 476, 118, 463, 384, 141, 432, 521,\n",
      "       618, 214,  62]), -2.5511793337500423e-06]\n",
      "0 [array([ 52,  76, 315, 711, 169, 218, 204, 396, 559, 554, 678, 250, 330,\n",
      "       652, 748, 237]), -2.4831462736756295e-06]\n",
      "0 [array([567, 614, 723,  58, 219, 357, 284, 703,  24, 752, 638, 165, 480,\n",
      "       475,  33, 507]), -2.3989341412971045e-06]\n",
      "0 [array([ 73, 526, 743, 161, 442, 530, 633,  13, 121, 172, 235, 321, 753,\n",
      "       653, 673, 290]), -2.33085800812205e-06]\n",
      "0 [array([469, 433, 407,  66, 599, 765, 766, 648, 222,   2, 528, 420,   1,\n",
      "       266, 127, 693]), -2.1683557049527737e-06]\n",
      "0 [array([310, 181, 576, 428,   4, 732, 390, 309,  14, 430, 273, 367, 144,\n",
      "       759, 587, 392]), -1.988612451782501e-06]\n",
      "0 [array([264, 584, 224, 286, 669,  38,  11, 308,  70, 295, 298, 595, 761,\n",
      "       434, 465, 426]), -1.868003819367914e-06]\n",
      "0 [array([  6, 438, 762, 342, 460, 636,  59, 649, 767, 527,  50, 485, 454,\n",
      "       674, 247, 681]), -1.619863878772776e-06]\n",
      "0 [array([259, 414, 667, 379, 733, 338, 522, 408, 251, 269,  71, 410,  82,\n",
      "       257, 671, 538]), -1.2696936677230043e-06]\n",
      "0 [array([376, 364, 665, 513, 737, 570, 685, 335, 625, 248, 170, 139, 184,\n",
      "       382, 444, 397]), -9.215716245307705e-07]\n",
      "1 [array([3]), -1.2048242813212582e-06]\n",
      "1 [array([7]), -1.0049578432547932e-06]\n",
      "1 [array([4]), -8.423420948323255e-07]\n",
      "5 [array([0]), -1.2264144563024146e-06]\n",
      "5 [array([3]), -1.210085836068707e-06]\n",
      "5 [array([9]), -1.1649007359627248e-06]\n",
      "5 [array([10]), -1.1562341254223938e-06]\n",
      "5 [array([1]), -1.1540719438774758e-06]\n",
      "5 [array([8]), -1.1147229682440738e-06]\n",
      "9 [array([0]), -1.158782280886804e-06]\n",
      "9 [array([11]), -1.0593414910620922e-06]\n",
      "9 [array([3]), -1.0553796752977505e-06]\n",
      "9 [array([5]), -9.906412313588007e-07]\n",
      "9 [array([7]), -9.834987420921024e-07]\n",
      "9 [array([10]), -9.674969786222718e-07]\n",
      "9 [array([1]), -7.11981206526012e-07]\n",
      "9 [array([9]), -7.04313029321689e-07]\n",
      "9 [array([8]), -6.603847762865698e-07]\n",
      "13 [array([9]), -1.1348119156385437e-06]\n",
      "13 [array([3]), -9.110936130885747e-07]\n",
      "13 [array([5]), -8.411378101603404e-07]\n",
      "13 [array([6]), -7.812707477417789e-07]\n",
      "17 [array([1]), -9.063040150370725e-07]\n",
      "17 [array([0]), -7.961552822356097e-07]\n",
      "17 [array([10]), -7.813008463320656e-07]\n",
      "17 [array([6]), -7.490971737145955e-07]\n",
      "17 [array([3]), -6.966214894722143e-07]\n",
      "21 [array([4]), -9.768881082739753e-07]\n",
      "21 [array([5]), -9.306759719341571e-07]\n",
      "21 [array([9]), -7.966040041840969e-07]\n",
      "25 [array([0]), -9.031930599879487e-07]\n",
      "29 [array([11]), -1.06353951965351e-06]\n",
      "29 [array([0]), -8.654153217169878e-07]\n",
      "29 [array([6]), -8.406134580430589e-07]\n",
      "29 [array([4]), -8.008144246408264e-07]\n",
      "29 [array([8]), -7.520833290376103e-07]\n",
      "29 [array([3]), -6.885139126400972e-07]\n",
      "33 [array([11]), -9.90920446232245e-07]\n",
      "33 [array([6]), -9.36493185311825e-07]\n",
      "33 [array([5]), -8.552536226792071e-07]\n",
      "33 [array([3]), -8.510968624098823e-07]\n",
      "33 [array([1]), -7.248721570552661e-07]\n",
      "33 [array([2]), -6.547282876757308e-07]\n",
      "37 [array([4]), -1.0643599265070733e-06]\n",
      "37 [array([10]), -9.616993478024014e-07]\n",
      "37 [array([8]), -9.398954628826104e-07]\n",
      "37 [array([11]), -8.061533006487807e-07]\n",
      "37 [array([3]), -7.903581914282185e-07]\n",
      "37 [array([0]), -7.579455908247255e-07]\n",
      "37 [array([7]), -7.150928152744527e-07]\n",
      "37 [array([1]), -6.771695010077892e-07]\n",
      "41 [array([10]), -9.622025825895761e-07]\n",
      "41 [array([0]), -8.957017510814111e-07]\n",
      "41 [array([4]), -7.968462424135723e-07]\n",
      "41 [array([6]), -6.976249463453641e-07]\n",
      "41 [array([9]), -6.555288135435475e-07]\n",
      "45 [array([3]), -8.470148512363593e-07]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/workspace/alex/NViT/nvit/pre_compute_saliency.pkl\", 'rb') as f:\n",
    "    prune_engine = pickle.load(f)\n",
    "    prune_engine.group_criteria = group_criteria\n",
    "# model_dim = compute_saliency()\n",
    "# groups, unique_groups, prune_network_criteria_updated = compute_saliency()\n",
    "compute_saliency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([3]), 5.1065452311149784e-08],\n",
       " [array([7]), 2.5093189037761476e-07],\n",
       " [array([4]), 4.135476388000825e-07],\n",
       " [array([0]), 8.859171316544234e-07],\n",
       " [array([1]), 9.903394584398484e-07],\n",
       " [array([6]), 1.0704957276175264e-06],\n",
       " [array([5]), 1.2170478385087335e-06],\n",
       " [array([8]), 1.4203769751475193e-06],\n",
       " [array([9]), 1.5064233593875542e-06],\n",
       " [array([10]), 1.6574052779105841e-06],\n",
       " [array([2]), 2.4562550606788136e-06],\n",
       " [array([11]), 4.141616955166683e-06]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_criteria = np.asarray([group[1] for layer in unique_groups for group in layer]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_importance = np.min(all_criteria)\n",
    "offset = np.abs(min_importance) + 1e-8\n",
    "IMPORTANCE_SCALE = 1e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 256\n",
    "NUM_TOKENS = 198\n",
    "WARMUP = 20\n",
    "TOTAL = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f\"mlp_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    mlp_lut = pickle.load(f)\n",
    "    mlp_lut = mlp_lut[1:, 1:]\n",
    "save_name = f\"qk_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    qk_lut = pickle.load(f)\n",
    "    qk_lut = qk_lut[1:, 1:, 1:]\n",
    "save_name = f\"vandproj_lut_BS{BS}_NUM_TOKENS{NUM_TOKENS}_v100.pkl\"\n",
    "with open(save_name, 'rb') as f:\n",
    "    vandproj_lut = pickle.load(f)\n",
    "    vandproj_lut = vandproj_lut[1:, 1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pyomo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pyomo.environ import *\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "# all_variable_specs = {\n",
    "#     \"EMB\": [768, 16, 768//16+1],\n",
    "#     \"HEAD\": [12, 1, 12//1+1],\n",
    "#     \"QK\": [64, 2, 64//2+1],\n",
    "#     \"V\": [64, 2, 64//2+1],\n",
    "#     \"MLP\": [3072, 32, 3072//32+1],\n",
    "# }\n",
    "\n",
    "EMB, HEAD, QK, V, MLP = 768,12,64,64,3072\n",
    "all_variable_specs = {\n",
    "    \"EMB\": [768, 16, 768//16],\n",
    "    \"HEAD\": [12, 1, 12//1],\n",
    "    \"QK\": [64, 2, 64//2],\n",
    "    \"V\": [64, 2, 64//2],\n",
    "    \"MLP\": [3072, 32, 3072//32],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "importance_dict = {}\n",
    "NAMES = [\"HEAD\", \"QK\", \"V\", \"MLP\"]\n",
    "for layer_idx, layer_importance in enumerate(unique_groups):\n",
    "    block_idx = (layer_idx - 1) // 4\n",
    "    layer_name = \"EMB\" if layer_idx == 0 else f\"{block_idx}_{NAMES[(layer_idx-1) % 4]}\"\n",
    "    running_total_importance = 0\n",
    "    running_total_indices = []\n",
    "    importance_dict[layer_name] = {\"importance\":[], \"indices\":[]}\n",
    "    for x in reversed(layer_importance):\n",
    "        group_indices, group_importance = x\n",
    "#         transformed_group_importance = (group_importance + offset) * IMPORTANCE_SCALE\n",
    "        transformed_group_importance = int((group_importance + offset) * IMPORTANCE_SCALE)\n",
    "        \n",
    "        running_total_importance += transformed_group_importance\n",
    "        running_total_indices += list(group_indices)\n",
    "        \n",
    "        importance_dict[layer_name][\"importance\"].append(running_total_importance)\n",
    "        importance_dict[layer_name][\"indices\"].append(deepcopy(running_total_indices))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14766,\n",
       " 16387,\n",
       " 17135,\n",
       " 17618,\n",
       " 18014,\n",
       " 18375,\n",
       " 18701,\n",
       " 19002,\n",
       " 19291,\n",
       " 19562,\n",
       " 19817,\n",
       " 20065,\n",
       " 20304,\n",
       " 20537,\n",
       " 20761,\n",
       " 20974,\n",
       " 21181,\n",
       " 21380,\n",
       " 21570,\n",
       " 21754,\n",
       " 21933,\n",
       " 22106,\n",
       " 22273,\n",
       " 22436,\n",
       " 22593,\n",
       " 22746,\n",
       " 22894,\n",
       " 23039,\n",
       " 23180,\n",
       " 23318,\n",
       " 23452,\n",
       " 23581,\n",
       " 23705,\n",
       " 23824,\n",
       " 23939,\n",
       " 24050,\n",
       " 24157,\n",
       " 24260,\n",
       " 24358,\n",
       " 24451,\n",
       " 24539,\n",
       " 24623,\n",
       " 24703,\n",
       " 24778,\n",
       " 24846,\n",
       " 24908,\n",
       " 24964,\n",
       " 25005]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict[\"EMB\"][\"importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23496,\n",
       " 33647,\n",
       " 41757,\n",
       " 48711,\n",
       " 54934,\n",
       " 60541,\n",
       " 65629,\n",
       " 70174,\n",
       " 74246,\n",
       " 77942,\n",
       " 81303,\n",
       " 84298,\n",
       " 86731,\n",
       " 88684,\n",
       " 90310,\n",
       " 91722,\n",
       " 92903,\n",
       " 93939,\n",
       " 94877,\n",
       " 95731,\n",
       " 96497,\n",
       " 97193,\n",
       " 97839,\n",
       " 98439,\n",
       " 98989,\n",
       " 99505,\n",
       " 99985,\n",
       " 100420,\n",
       " 100818,\n",
       " 101190,\n",
       " 101539,\n",
       " 101865,\n",
       " 102161,\n",
       " 102435,\n",
       " 102687,\n",
       " 102917,\n",
       " 103132,\n",
       " 103332,\n",
       " 103518,\n",
       " 103690,\n",
       " 103850,\n",
       " 103998,\n",
       " 104135,\n",
       " 104259,\n",
       " 104374,\n",
       " 104478,\n",
       " 104574,\n",
       " 104663,\n",
       " 104747,\n",
       " 104827,\n",
       " 104902,\n",
       " 104973,\n",
       " 105040,\n",
       " 105104,\n",
       " 105164,\n",
       " 105222,\n",
       " 105277,\n",
       " 105330,\n",
       " 105381,\n",
       " 105430,\n",
       " 105477,\n",
       " 105522,\n",
       " 105565,\n",
       " 105606,\n",
       " 105645,\n",
       " 105683,\n",
       " 105720,\n",
       " 105756,\n",
       " 105790,\n",
       " 105823,\n",
       " 105856,\n",
       " 105888,\n",
       " 105919,\n",
       " 105949,\n",
       " 105978,\n",
       " 106005,\n",
       " 106032,\n",
       " 106058,\n",
       " 106083,\n",
       " 106107,\n",
       " 106130,\n",
       " 106152,\n",
       " 106174,\n",
       " 106195,\n",
       " 106215,\n",
       " 106234,\n",
       " 106252,\n",
       " 106269,\n",
       " 106285,\n",
       " 106300,\n",
       " 106314,\n",
       " 106327,\n",
       " 106339,\n",
       " 106350,\n",
       " 106360,\n",
       " 106367]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict[\"0_MLP\"][\"importance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EMB', '0_HEAD', '0_QK', '0_V', '0_MLP', '1_HEAD', '1_QK', '1_V', '1_MLP', '2_HEAD', '2_QK', '2_V', '2_MLP', '3_HEAD', '3_QK', '3_V', '3_MLP', '4_HEAD', '4_QK', '4_V', '4_MLP', '5_HEAD', '5_QK', '5_V', '5_MLP', '6_HEAD', '6_QK', '6_V', '6_MLP', '7_HEAD', '7_QK', '7_V', '7_MLP', '8_HEAD', '8_QK', '8_V', '8_MLP', '9_HEAD', '9_QK', '9_V', '9_MLP', '10_HEAD', '10_QK', '10_V', '10_MLP', '11_HEAD', '11_QK', '11_V', '11_MLP'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_NUMBER = 12\n",
    "# BLOCK_NUMBER = 1\n",
    "total_latency = (mlp_lut[-1, -1] + vandproj_lut[-1, -1, -1] + qk_lut[-1, -1, -1]) * BLOCK_NUMBER\n",
    "target_latency = total_latency * 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConcreteModel()\n",
    "# Define variables\n",
    "variable_slices_by_type = {}\n",
    "counter = 0\n",
    "variable_slices_by_type[\"EMB\"] = (counter, counter+all_variable_specs[\"EMB\"][2])\n",
    "counter += all_variable_specs[\"EMB\"][2]\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        variable_slices_by_type[f\"{block_idx}_{var_type}\"] = (counter, counter+all_variable_specs[var_type][2])\n",
    "        counter += all_variable_specs[var_type][2]\n",
    "\n",
    "all_items = list(range(counter))\n",
    "model.decision_vars = Var(all_items, domain=Binary)\n",
    "\n",
    "# Define importance and uniqueness constraints\n",
    "importance = 0\n",
    "model.group_unique_constraint = ConstraintList()\n",
    "\n",
    "def add_uniqueness_constraint_and_importance_expr(layer_name):\n",
    "    # uniqueness constraint\n",
    "    # only selecting one configuration\n",
    "    cur_slices = variable_slices_by_type[layer_name]\n",
    "    cur_decision_vars = [model.decision_vars[k] for k in range(cur_slices[0], cur_slices[1])]\n",
    "    model.group_unique_constraint.add(sum(cur_decision_vars) == 1)\n",
    "    # get importance expr\n",
    "    cur_importance = importance_dict[layer_name][\"importance\"]\n",
    "    cur_importance_expr = sum(cur_decision_vars[i] * cur_importance[i] for i in range(len(cur_decision_vars)))\n",
    "    return cur_importance_expr\n",
    "\n",
    "cur_importance_expr = add_uniqueness_constraint_and_importance_expr(\"EMB\")\n",
    "importance += cur_importance_expr\n",
    "\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        cur_importance_expr = add_uniqueness_constraint_and_importance_expr(f\"{block_idx}_{var_type}\")\n",
    "        importance += cur_importance_expr\n",
    "\n",
    "model.obj = Objective(expr=importance, sense=maximize)\n",
    "\n",
    "# Define latency constraint\n",
    "# Add latency constraint\n",
    "latency_expr = 0\n",
    "emb_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1])])\n",
    "for block_idx in range(BLOCK_NUMBER):\n",
    "    head_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_HEAD\"][0], variable_slices_by_type[f\"{block_idx}_HEAD\"][1])])\n",
    "    qk_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_QK\"][0], variable_slices_by_type[f\"{block_idx}_QK\"][1])])\n",
    "    v_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_V\"][0], variable_slices_by_type[f\"{block_idx}_V\"][1])])\n",
    "    mlp_vectors = np.array([model.decision_vars[k] for k in range(variable_slices_by_type[f\"{block_idx}_MLP\"][0], variable_slices_by_type[f\"{block_idx}_MLP\"][1])])\n",
    "    \n",
    "    T1 = np.tensordot(emb_vectors, mlp_vectors, axes=0)\n",
    "    latency_expr_mlp = np.sum(T1 * mlp_lut)\n",
    "    T2 = np.tensordot(head_vectors, np.tensordot(emb_vectors, v_vectors, axes=0), axes=0)\n",
    "    latency_expr_vandproj = np.sum(T2 * vandproj_lut)\n",
    "    T3 = np.tensordot(head_vectors, np.tensordot(emb_vectors, qk_vectors, axes=0), axes=0)\n",
    "    latency_expr_qk = np.sum(T3 * qk_lut)\n",
    "    latency_expr += latency_expr_mlp + latency_expr_vandproj + latency_expr_qk\n",
    "\n",
    "model.latency_constraint = Constraint(expr=latency_expr <= target_latency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "              Mixed-Integer Nonlinear Decomposition Toolbox in Pyomo (MindtPy)               \n",
      "---------------------------------------------------------------------------------------------\n",
      "For more information, please visit https://pyomo.readthedocs.io/en/stable/contributed_packages/mindtpy.html\n",
      "Original model has 50 constraints (1 nonlinear) and 0 disjunctions, with 2112 variables, of which 2112 are binary, 0 are integer, and 0 are continuous.\n",
      "Moving objective to constraint set.\n",
      "FP is the initial strategy being used.\n",
      "\n",
      " ===============================================================================================\n",
      " Iteration | Subproblem Type | Objective Value | Primal Bound |   Dual Bound |   Gap   | Time(s)\n",
      "\n",
      "         -       Relaxed NLP            722853           -inf         722853      nan%    197.08\n",
      "         1            FP-MIP           1.48487           -inf         722853      nan%    251.86\n",
      "         1            FP-NLP       5.40237e-06           -inf         722853      nan%    328.32\n",
      "*        1         Fixed NLP            721629         721629         722853     0.17%    432.51\n",
      "FP-MIP infeasible\n",
      "         1              MILP            722764         721629         722764     0.16%    447.44\n",
      "*        2         Fixed NLP            722764         722764         722764     0.00%    557.18\n",
      "MindtPy exiting on bound convergence. |Primal Bound: 722764.0072276375 - Dual Bound: 722764.0| / (1e-10 + |Primal Bound|:722764.0072276375) <= relative tolerance: 0.001\n",
      "Solve the main problem without the last no_good cut to fix the bound.zero_tolerance is set to 1E-4\n",
      "Fixed bound values: Primal Bound: 722764.0072276375  Dual Bound: 722764.0\n",
      " ===============================================================================================\n",
      " Primal integral          :   427086.1505 \n",
      " Dual integral            :   17469.4778 \n",
      " Primal-dual gap integral :   444555.6282 \n"
     ]
    }
   ],
   "source": [
    "solver = SolverFactory('mindtpy')\n",
    "# solver = SolverFactorypyomopyo('glpk')\n",
    "# solver.solve(model)\n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, solver_tee=True, time_limit=1800) \n",
    "results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt', tee=True, time_limit=1800) \n",
    "# results = solver.solve(model, strategy='OA', init_strategy='FP', mip_solver='glpk', nlp_solver='ipopt') \n",
    "# results = solver.solve(model) \n",
    "# results = solver.solve(model, mip_solver='glpk', nlp_solver='ipopt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EMB': [768, 16, 48],\n",
       " 'HEAD': [12, 1, 12],\n",
       " 'QK': [64, 2, 32],\n",
       " 'V': [64, 2, 32],\n",
       " 'MLP': [3072, 32, 96]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variable_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 29 95\n",
      "HEAD 9 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 5 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 7 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 8 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 10 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n",
      "HEAD 11 11\n",
      "QK 31 31\n",
      "V 31 31\n",
      "MLP 95 95\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMB 29 47\n",
      "HEAD 10 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 6 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 8 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 12 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 9 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 11 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n",
      "HEAD 12 12\n",
      "QK 32 32\n",
      "V 32 32\n",
      "MLP 96 96\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = []\n",
    "full_values = []\n",
    "pruned_values = []\n",
    "\n",
    "indices = list(range(variable_slices_by_type[\"EMB\"][0], variable_slices_by_type[\"EMB\"][1]))\n",
    "cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "var_type = \"EMB\"\n",
    "print(\"EMB\", np.argmax(cur_decision_vars_value), all_variable_specs[var_type][2]-1)\n",
    "names.append(\"EMB\")\n",
    "full_values.append(all_variable_specs[var_type][2])\n",
    "pruned_values.append(np.argmax(cur_decision_vars_value)+1)\n",
    "\n",
    "for block_id in range(BLOCK_NUMBER):\n",
    "    for var_type, var_spec in all_variable_specs.items():\n",
    "        if var_type == \"EMB\":\n",
    "            continue\n",
    "        indices = list(range(variable_slices_by_type[f\"{block_id}_{var_type}\"][0], variable_slices_by_type[f\"{block_id}_{var_type}\"][1]))\n",
    "        cur_decision_vars = [model.decision_vars[k] for k in indices]\n",
    "        cur_decision_vars_value = [x.value for x in cur_decision_vars]\n",
    "        print(var_type, np.argmax(cur_decision_vars_value)+1, all_variable_specs[var_type][2])\n",
    "        names.append(f\"{block_id}_{var_type}\")\n",
    "        full_values.append(all_variable_specs[var_type][2])\n",
    "        pruned_values.append(np.argmax(cur_decision_vars_value)+1)\n",
    "        \n",
    "plt.figure(figsize=(10, 8))  # Adjust the size as needed\n",
    "\n",
    "# Plot full values\n",
    "plt.barh(names, full_values, color='lightblue', label='Full Values')\n",
    "\n",
    "# Plot pruned values\n",
    "plt.barh(names, pruned_values, color='orange', alpha=0.7, label='Kept Values after Pruning')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Module Names')\n",
    "plt.yticks(fontsize=8)\n",
    "plt.title('Deit pruned structure')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_dims = {\"EMB\": 304, \"HEAD\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(48-29) * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
